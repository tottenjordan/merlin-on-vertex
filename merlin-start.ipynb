{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7483ac1f-bb86-4b51-a296-92f545905cd4",
   "metadata": {},
   "source": [
    "# Train Merlin Two-Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d13046-5cb5-43c7-a6ee-b42a3ee9a1e9",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867b302d-ff4e-4ae4-ba6a-18b717f6d6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 15:29:51.220437: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-10-27 15:29:53.234448: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 15:29:54.599279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20480 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9140a-8529-489e-9885-8431960e129a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495ea222-2f9e-4dc5-8cfa-23d1470a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project definitions\n",
    "PROJECT_ID = 'hybrid-vertex' # Change to your project ID.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "\n",
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' # Change to your service account with Vertex AI Admin permitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc441d2e-b156-4d03-9951-ff943b129fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "BUCKET = 'jt-merlin-scaling' # 'spotify-merlin-v1'\n",
    "\n",
    "VERSION = 'jtv1'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "MODEL_DISPLAY_NAME = f'vertex-{FRAMEWORK}-{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions for training\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# # DOCKERNAME = 'hugectr'\n",
    "# DOCKERNAME = 'merlintf'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49f034-6107-426c-9ca5-78bc8e4bed22",
   "metadata": {},
   "source": [
    "# Training Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dcb695-6e50-4efd-9715-0eb02e1dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAIN_SUB_DIR = 'trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cfb1e6-fef6-42b7-ba21-b730ade84cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47e4461-5b2d-4082-bfd3-88072163d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db637aa-c331-4db0-a7da-5fcceed19fd6",
   "metadata": {},
   "source": [
    "## Interactive Train Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a376724-9ed3-4a8b-9b6f-cc139ccc0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141daa01-b5dc-4757-a92c-aad284dea45b",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d7ad57-89f5-4c68-9fa4-26644c2541d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/two_tower_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/two_tower_model.py\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_two_tower(\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    layer_sizes: List[Any] = [512, 256, 128],\n",
    "):\n",
    "    \n",
    "    #=========================================\n",
    "    # get workflow details\n",
    "    #=========================================\n",
    "    workflow = nvt.Workflow.load(workflow_dir) # gs://spotify-merlin-v1/nvt-preprocessing-spotify-v24/nvt-analyzed\n",
    "    \n",
    "    schema = workflow.output_schema\n",
    "    embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    user_schema = schema.select_by_tag(Tags.USER)\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    \n",
    "    #=========================================\n",
    "    # build towers\n",
    "    #=========================================\n",
    "    query = mm.Encoder(user_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    item_schema = schema.select_by_tag(Tags.ITEM)\n",
    "    item_inputs = mm.InputBlockV2(\n",
    "        item_schema,\n",
    "    )\n",
    "    candidate = mm.Encoder(item_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    model = mm.RetrievalModelV2(\n",
    "        query=query,\n",
    "        candidate=candidate,\n",
    "        output=mm.ContrastiveOutput(\n",
    "            to_call=DotProduct(),\n",
    "            negative_samplers=\"in-batch\",\n",
    "            schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "            candidate_name=\"item\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac09-65a6-494c-9b42-7b12a22d650f",
   "metadata": {},
   "source": [
    "## Train task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a431f3c9-7e9c-4399-a541-9b6eb514c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# merlin\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# gcp\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "# import hypertune\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "# repo\n",
    "from two_tower_model import create_two_tower\n",
    "# import utils\n",
    "\n",
    "# local\n",
    "HYPERTUNE_METRIC_NAME = 'AUC'\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions - TODO: move to utils?\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "    # return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "    # return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "\n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "def get_upload_logs_to_manged_tb_command(tb_resource_name, logs_dir, experiment_name, ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(\n",
    "        f\"\"\"tb-gcp-uploader --tensorboard_resource_name={tb_resource_name} \\\n",
    "        --logdir={logs_dir} \\\n",
    "        --experiment_name={experiment_name} \\\n",
    "        --one_shot={oneshot} \\\n",
    "        --event_file_inactive_secs={60*60*ttl_hrs}\"\"\"\n",
    "    )\n",
    "\n",
    "def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# TRAINING SCRIPT\n",
    "# ====================================================\n",
    "    \n",
    "def main(args):\n",
    "    \"\"\"Runs a training loop.\"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    # os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "    # os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "    \n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    vertex_ai.init(project=f'{args.project}', location=f'{args.location}')\n",
    "    storage_client = storage.Client(project=args.project)\n",
    "    logging.info(\"vertex_ai initialized...\")\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}-{TIMESTAMP}\" # f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    WORKING_DIR_GCS_URI = f'gs://{args.train_output_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}' \n",
    "    \n",
    "    TB_RESOURCE_NAME = f'{args.tb_name}'\n",
    "    LOGS_DIR = f'gs://{args.train_output_bucket}/tb_logs/{EXPERIMENT_NAME}'\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU Strategy\n",
    "    # ====================================================    \n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.test.is_gpu_available(): # TODO: replace with - tf.config.list_physical_devices('GPU')\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "        \n",
    "    \n",
    "    # set related vars...\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = NUM_WORKERS * args.per_gpu_batch_size\n",
    "    # num_gpus = sum([len(gpus) for gpus in args.gpus])\n",
    "    # GLOBAL_BATCH_SIZE = num_gpus * args.per_gpu_batch_size\n",
    "\n",
    "    logging.info(f'NUM_WORKERS = {NUM_WORKERS}')\n",
    "    # logging.info(f'num_gpus: {num_gpus}')\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE: {GLOBAL_BATCH_SIZE}')\n",
    "    \n",
    "    # set worker vars...\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "        \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'Loading workflow & schema from : {args.workflow_dir}')\n",
    "    \n",
    "    workflow = nvt.Workflow.load(args.workflow_dir) # gs://{BUCKET}/..../nvt-analyzed\n",
    "    schema = workflow.output_schema\n",
    "    embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    train_data = MerlinDataset(os.path.join(args.train_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    valid_data = MerlinDataset(os.path.join(args.valid_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    \n",
    "    # train_data = MerlinDataset(args.train_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    # valid_data = MerlinDataset(args.valid_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Callbacks\n",
    "    # ====================================================\n",
    "    class UploadTBLogsBatchEnd(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            os.system(\n",
    "                get_upload_logs_to_manged_tb_command(\n",
    "                    tb_resource_name=TB_RESOURCE_NAME, \n",
    "                    logs_dir=LOGS_DIR, \n",
    "                    experiment_name=EXPERIMENT_NAME,\n",
    "                    ttl_hrs = 5, \n",
    "                    oneshot=\"true\",\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOGS_DIR,\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        # profile_batch=(20,50)\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Train\n",
    "    # ==================================================== \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = create_two_tower(\n",
    "            train_dir=args.train_dir,\n",
    "            valid_dir=args.valid_dir,\n",
    "            workflow_dir=args.workflow_dir,\n",
    "            layer_sizes=LAYER_SIZES # args.layer_sizes,\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "            run_eagerly=False,\n",
    "            metrics=[mm.RecallAt(1), mm.RecallAt(10), mm.NDCGAt(10)],\n",
    "        )\n",
    "        \n",
    "    # cloud_profiler.init() # managed TB profiler\n",
    "        \n",
    "    logging.info('Starting training loop...')\n",
    "    \n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=valid_data, \n",
    "        batch_size=GLOBAL_BATCH_SIZE, \n",
    "        epochs=args.num_epochs,\n",
    "        # steps_per_epoch=20, \n",
    "        callbacks=[\n",
    "            tensorboard_callback, \n",
    "            UploadTBLogsBatchEnd()\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    elapsed_model_fit = end_model_fit - start_model_fit\n",
    "    elapsed_model_fit = round(elapsed_model_fit, 2)\n",
    "    logging.info(f'Elapsed model_fit: {elapsed_model_fit} seconds')\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams & metrics for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging params & metrics for Vertex Experiments')\n",
    "    \n",
    "    # get the metrics for the experiment run\n",
    "    history_keys = model.history.history.keys()\n",
    "    metrics_dict = {}\n",
    "    _ = [metrics_dict.update({key: model.history.history[key][-1]}) for key in history_keys]\n",
    "    metrics_dict[\"elapsed_model_fit\"] = elapsed_model_fit\n",
    "    \n",
    "    logging.info(f'metrics_dict: {metrics_dict}')\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    \n",
    "    logging.info(f'metaparams: {metaparams}')\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"num_gpus\"] = num_gpus\n",
    "    hyperparams[\"per_gpu_batch_size\"] = args.per_gpu_batch_size\n",
    "    hyperparams[\"global_batch_size\"] = GLOBAL_BATCH_SIZE\n",
    "    hyperparams[\"learning_rate\"] = args.learning_rate\n",
    "    hyperparams['layers'] = f'{args.layer_sizes}'\n",
    "    \n",
    "    logging.info(f'hyperparams: {hyperparams}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Experiments\n",
    "    # ====================================================\n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "    \n",
    "        # Create experiment\n",
    "        vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "        with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "            logging.info(f\"logging metrics_dict\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams\")\n",
    "            my_run.log_params(metaparams)\n",
    "\n",
    "            logging.info(f\"logging hyperparams\")\n",
    "            my_run.log_params(hyperparams)\n",
    "        \n",
    "    # =============================================\n",
    "    # save retrieval (query) tower\n",
    "    # =============================================\n",
    "    # set vars...\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model-dir\"\n",
    "    logging.info(f'Saving towers to {MODEL_DIR}')\n",
    "    \n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query-tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate-tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate-embeddings\"\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        # save query tower\n",
    "        query_tower = model.query_encoder\n",
    "        query_tower.save(QUERY_TOWER_PATH)\n",
    "        logging.info(f'Saved query tower to {QUERY_TOWER_PATH}')\n",
    "    \n",
    "    # =============================================\n",
    "    # save embeddings for ME index\n",
    "    # =============================================\n",
    "    EMBEDDINGS_FILE_NAME = \"candidate_embeddings.json\"\n",
    "    logging.info(f\"Saving {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    def format_for_matching_engine(data) -> None:\n",
    "        emb = [data[i] for i in range(LAYER_SIZES[-1])] # get the embeddings\n",
    "        formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "        with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "            f.write(formatted_emb)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # !rm candidate_embeddings.json > /dev/null \n",
    "    # !touch candidate_embeddings.json\n",
    "    item_data = pd.read_parquet(f'{args.workflow_dir}/categories/unique.track_uri_can.parquet')\n",
    "    lookup_dict = dict(item_data['track_uri_can'])\n",
    "\n",
    "    # item embeds from TRAIN\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features = (unique_rows_by_features(train_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs = model.candidate_embeddings(item_features, index=item_features.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd = item_embs.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd['track_uri_can'] = item_emb_pd['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting TRAIN embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    # item embeds from VALID\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features_val = (unique_rows_by_features(valid_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs_val = model.candidate_embeddings(item_features_val, index=item_features_val.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd_val = item_embs_val.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd_val['track_uri_can'] = item_emb_pd_val['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd_val.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting VALID embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        _upload_blob_gcs(\n",
    "            EMBEDDINGS_PATH, \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\", \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\",\n",
    "        )\n",
    "    \n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "# ====================================================\n",
    "# arg parser\n",
    "# ====================================================\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--experiment_name',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed-experiment',\n",
    "        help='name of vertex ai experiment'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--experiment_run',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed_run',\n",
    "        help='name of vertex ai experiment run'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--tb_name',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='projects/XXXXXX/locations/us-central1/tensorboards/XXXXXXXX'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--distribute',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='single',\n",
    "        help='training strategy'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_output_bucket',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        # default='single',\n",
    "        help='gcs bucket name'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to saved workflow.pkl e.g., nvt-analyzed'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to training data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--valid_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to validation data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='num_epochs'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--per_gpu_batch_size',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='Per GPU Batch size'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--layer_sizes',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='[512, 256, 128]',\n",
    "        help='layer_sizes'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=.001,\n",
    "        help='learning_rate'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp project'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--location',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp location'\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     '--gpus',\n",
    "    #     type=str,\n",
    "    #     required=False,\n",
    "    #     default='[[0]]',\n",
    "    #     help='GPU devices to use for Preprocessing'\n",
    "    # )\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    # parsed_args.gpus = json.loads(parsed_args.gpus)\n",
    "\n",
    "    # parsed_args.slot_size_array = [\n",
    "    #     int(i) for i in parsed_args.slot_size_array.split(sep=' ')\n",
    "    # ]\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting training')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158e674-6b0b-47ac-ac12-cce0b002bf93",
   "metadata": {},
   "source": [
    "## Training Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa81f3-0912-4f00-ba09-9bb9c40991eb",
   "metadata": {},
   "source": [
    "### versioned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad9482d-5713-43bc-81db-d269ac4283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker definitions for training\n",
    "MERLIN_VERSION = '22_09'\n",
    "IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}-{MERLIN_VERSION}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = f'merlintf-{MERLIN_VERSION}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13146051-f765-4bdf-9e87-d2eb8f05e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.merlintf-22_09\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.07\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:nightly\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "RUN pip install -U pip\n",
    "RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
    "RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular==1.5\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "COPY trainer/* ./\n",
    "\n",
    "ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e455c73-6ace-40fe-a9a1-308830a80029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-merlin/src/trainer\u001b[00m\n",
      "├── __init__.py\n",
      "├── interactive_train.py\n",
      "├── train_task.py\n",
      "└── two_tower_model.py\n",
      "\n",
      "0 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-merlin/{REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740819d7-2a79-46a2-9c79-a1f718da76ba",
   "metadata": {},
   "source": [
    "### nightly image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8476061-b8b1-4fbe-8e56-76d94cf34bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Docker definitions for training\n",
    "# MERLIN_VERSION = 'nightly'\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}-{MERLIN_VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "# DOCKERNAME = f'merlintf-{MERLIN_VERSION}'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b1d9c9-e31e-4de4-9ea5-f51aeec5c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# # FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:nightly\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
    "# RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
    "# RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "# RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "# COPY trainer/* ./\n",
    "\n",
    "# ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39a5146-1880-44ba-ab66-95045c05cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree /home/jupyter/spotify-merlin/{REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513735ef-7713-4a59-b5bc-5e82f298ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URI='gcr.io/hybrid-vertex/merlin-tf-twotower-training-jtv1-nightly'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5134-d7aa-4f6c-882c-5f11d35cbfc2",
   "metadata": {},
   "source": [
    "# Build Train Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42e901-e60c-47ab-ba34-c5add53845c0",
   "metadata": {},
   "source": [
    "### test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75e03e5f-c0b3-4654-b40f-b33d5aa0f1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7fb19c0-4201-48e9-b0c8-470424be95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# _LOCATION='us-central1'\n",
    "# _TB_NAME='projects/934903580331/locations/us-central1/tensorboards/5925030667573264384'\n",
    "# _DATA_DIR='gs://spotify-beam-v3/merlin-processed'\n",
    "# _TRAIN_DATA=f'{_DATA_DIR}/valid/'\n",
    "# _VALID_DATA=f'{_DATA_DIR}/valid/'\n",
    "# _WORKFLOW_DIR=f'{_DATA_DIR}/workflow/2t-spotify-workflow'\n",
    "# _OUTPUT_BUCKET='jt-merlin-scaling'\n",
    "# _EXPERIMENT_NAME='local-experiment'\n",
    "# _EXPERIMENT_RUN=f'run-v1-{TIMESTAMP}'\n",
    "# _DISTRIBUTE='single'\n",
    "# _PER_GPU_BATCH_SIZE=4096\n",
    "# _LAYER_SIZES='[256, 128]'\n",
    "# _LEARNING_RATE=0.001\n",
    "# _NUM_EPOCHS=1\n",
    "\n",
    "# # !cd src/trainer; python3 -m trainer.train_task \\\n",
    "# !cd src/trainer; python3 train_task.py \\\n",
    "#     --project=PROJECT_ID --location=$_LOCATION \\\n",
    "#     --train_output_bucket=$_OUTPUT_BUCKET --tb_name=$_TB_NAME \\\n",
    "#     --workflow_dir=$_WORKFLOW_DIR --train_dir=$_TRAIN_DATA --valid_dir=$_VALID_DATA \\\n",
    "#     --experiment_name=$_EXPERIMENT_NAME --experiment_run=$_EXPERIMENT_RUN \\\n",
    "#     --distribute=$_DISTRIBUTE \\\n",
    "#     --num_epochs=$_NUM_EPOCHS \\\n",
    "#     --per_gpu_batch_size=$_PER_GPU_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54394f2-7c65-40ea-9102-ccf0cd3178a5",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817dc303-b3e6-4427-bfd0-5cd949183a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e469cf-13ae-47e6-93bb-30aef60ebe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e640822-e85b-4ba2-a192-9c8d790ff2cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 79 file(s) totalling 2.0 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1666884813.252391-cf62d9b4af56478ba12c071ed1dec99e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/4708cfce-ac01-40bd-8085-d6570d7b809d].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/4708cfce-ac01-40bd-8085-d6570d7b809d?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"4708cfce-ac01-40bd-8085-d6570d7b809d\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1666884813.252391-cf62d9b4af56478ba12c071ed1dec99e.tgz#1666884813861089\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1666884813.252391-cf62d9b4af56478ba12c071ed1dec99e.tgz#1666884813861089...\n",
      "/ [1 files][331.9 KiB/331.9 KiB]                                                \n",
      "Operation completed over 1 objects/331.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  485.9kB\n",
      "Step 1/9 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      "22.09: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "3b65ec22a9e9: Pulling fs layer\n",
      "fd80d866e8b2: Pulling fs layer\n",
      "a364ca75fd6d: Pulling fs layer\n",
      "3d4731d03623: Pulling fs layer\n",
      "53a5c2e0251f: Pulling fs layer\n",
      "b00ff40d02d9: Pulling fs layer\n",
      "3036e9b94123: Pulling fs layer\n",
      "453fdcdda788: Pulling fs layer\n",
      "35e12ec5e515: Pulling fs layer\n",
      "11f61a475a23: Pulling fs layer\n",
      "24280cf31c9a: Pulling fs layer\n",
      "79007799e2ed: Pulling fs layer\n",
      "03eb76abf1e5: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "5e9434e8ae41: Pulling fs layer\n",
      "88a3e778b5bf: Pulling fs layer\n",
      "729af2b35d14: Pulling fs layer\n",
      "30e0a3a7e9e5: Pulling fs layer\n",
      "0852b4bd65a1: Pulling fs layer\n",
      "81cb421c2c25: Pulling fs layer\n",
      "3d6b664afa23: Pulling fs layer\n",
      "3e8f37aba8a2: Pulling fs layer\n",
      "53a5c2e0251f: Waiting\n",
      "91bba9bd0f1f: Pulling fs layer\n",
      "a422be4dcb08: Pulling fs layer\n",
      "b00ff40d02d9: Waiting\n",
      "ca10bb6dc143: Pulling fs layer\n",
      "e48bbfc7d00e: Pulling fs layer\n",
      "860a23551ac0: Pulling fs layer\n",
      "3036e9b94123: Waiting\n",
      "cc78be876588: Pulling fs layer\n",
      "453fdcdda788: Waiting\n",
      "35e12ec5e515: Waiting\n",
      "ad6568ad37e5: Pulling fs layer\n",
      "258a31babfce: Pulling fs layer\n",
      "116ca0069c88: Pulling fs layer\n",
      "ede4a5022f61: Pulling fs layer\n",
      "154d6414dd17: Pulling fs layer\n",
      "e1f68d1c5137: Pulling fs layer\n",
      "0d4b5cd36c43: Pulling fs layer\n",
      "fc9b6547dc7c: Pulling fs layer\n",
      "de51b5b1b318: Pulling fs layer\n",
      "d684c579871f: Pulling fs layer\n",
      "4a39f6623824: Pulling fs layer\n",
      "30bba30584e3: Pulling fs layer\n",
      "c104fa3a7626: Pulling fs layer\n",
      "22f09e497c63: Pulling fs layer\n",
      "5f66cd739f31: Pulling fs layer\n",
      "c9c9a9e1cad6: Pulling fs layer\n",
      "630fcaa7a158: Pulling fs layer\n",
      "1b8090778700: Pulling fs layer\n",
      "4ca3cacea924: Pulling fs layer\n",
      "4aa043daa566: Pulling fs layer\n",
      "c1d86dc35ba1: Pulling fs layer\n",
      "55f4be7d7adc: Pulling fs layer\n",
      "f8e1ddeececb: Pulling fs layer\n",
      "11f61a475a23: Waiting\n",
      "bf1f5a5d15b8: Pulling fs layer\n",
      "68d63385def6: Pulling fs layer\n",
      "24280cf31c9a: Waiting\n",
      "865f53eecc40: Pulling fs layer\n",
      "70ac11bbf381: Pulling fs layer\n",
      "3d4731d03623: Waiting\n",
      "91a761249212: Pulling fs layer\n",
      "79007799e2ed: Waiting\n",
      "2cb953eac694: Pulling fs layer\n",
      "2e6386dccac0: Pulling fs layer\n",
      "860a23551ac0: Waiting\n",
      "67793606273a: Pulling fs layer\n",
      "03eb76abf1e5: Waiting\n",
      "bfdaa5a6754f: Pulling fs layer\n",
      "3e0d0b3b5c1c: Pulling fs layer\n",
      "8d6d2c98840c: Pulling fs layer\n",
      "2559630e37d2: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "d0f13d58f7fa: Pulling fs layer\n",
      "929ce3ad5dc9: Pulling fs layer\n",
      "ad6568ad37e5: Waiting\n",
      "0d4b5cd36c43: Waiting\n",
      "7a42c1978b2e: Pulling fs layer\n",
      "5e9434e8ae41: Waiting\n",
      "258a31babfce: Waiting\n",
      "c104fa3a7626: Waiting\n",
      "116ca0069c88: Waiting\n",
      "fc9b6547dc7c: Waiting\n",
      "88a3e778b5bf: Waiting\n",
      "ede4a5022f61: Waiting\n",
      "de51b5b1b318: Waiting\n",
      "154d6414dd17: Waiting\n",
      "729af2b35d14: Waiting\n",
      "22f09e497c63: Waiting\n",
      "e1f68d1c5137: Waiting\n",
      "d684c579871f: Waiting\n",
      "30e0a3a7e9e5: Waiting\n",
      "5f66cd739f31: Waiting\n",
      "4a39f6623824: Waiting\n",
      "91a761249212: Waiting\n",
      "30bba30584e3: Waiting\n",
      "a422be4dcb08: Waiting\n",
      "0852b4bd65a1: Waiting\n",
      "2cb953eac694: Waiting\n",
      "c9c9a9e1cad6: Waiting\n",
      "ca10bb6dc143: Waiting\n",
      "2e6386dccac0: Waiting\n",
      "7a42c1978b2e: Waiting\n",
      "81cb421c2c25: Waiting\n",
      "e48bbfc7d00e: Waiting\n",
      "630fcaa7a158: Waiting\n",
      "67793606273a: Waiting\n",
      "2559630e37d2: Waiting\n",
      "1b8090778700: Waiting\n",
      "f8e1ddeececb: Waiting\n",
      "91bba9bd0f1f: Waiting\n",
      "3d6b664afa23: Waiting\n",
      "bfdaa5a6754f: Waiting\n",
      "4ca3cacea924: Waiting\n",
      "3e0d0b3b5c1c: Waiting\n",
      "d0f13d58f7fa: Waiting\n",
      "bf1f5a5d15b8: Waiting\n",
      "3e8f37aba8a2: Waiting\n",
      "55f4be7d7adc: Waiting\n",
      "929ce3ad5dc9: Waiting\n",
      "8d6d2c98840c: Waiting\n",
      "4aa043daa566: Waiting\n",
      "c1d86dc35ba1: Waiting\n",
      "68d63385def6: Waiting\n",
      "865f53eecc40: Waiting\n",
      "3b65ec22a9e9: Download complete\n",
      "3d4731d03623: Download complete\n",
      "fd80d866e8b2: Verifying Checksum\n",
      "fd80d866e8b2: Download complete\n",
      "a364ca75fd6d: Verifying Checksum\n",
      "a364ca75fd6d: Download complete\n",
      "b00ff40d02d9: Verifying Checksum\n",
      "b00ff40d02d9: Download complete\n",
      "3036e9b94123: Verifying Checksum\n",
      "3036e9b94123: Download complete\n",
      "453fdcdda788: Verifying Checksum\n",
      "453fdcdda788: Download complete\n",
      "3b65ec22a9e9: Pull complete\n",
      "35e12ec5e515: Download complete\n",
      "11f61a475a23: Verifying Checksum\n",
      "11f61a475a23: Download complete\n",
      "79007799e2ed: Verifying Checksum\n",
      "79007799e2ed: Download complete\n",
      "24280cf31c9a: Verifying Checksum\n",
      "24280cf31c9a: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "5e9434e8ae41: Verifying Checksum\n",
      "5e9434e8ae41: Download complete\n",
      "88a3e778b5bf: Verifying Checksum\n",
      "88a3e778b5bf: Download complete\n",
      "729af2b35d14: Verifying Checksum\n",
      "729af2b35d14: Download complete\n",
      "03eb76abf1e5: Verifying Checksum\n",
      "03eb76abf1e5: Download complete\n",
      "0852b4bd65a1: Download complete\n",
      "fd80d866e8b2: Pull complete\n",
      "81cb421c2c25: Verifying Checksum\n",
      "81cb421c2c25: Download complete\n",
      "3d6b664afa23: Verifying Checksum\n",
      "3d6b664afa23: Download complete\n",
      "30e0a3a7e9e5: Verifying Checksum\n",
      "30e0a3a7e9e5: Download complete\n",
      "a364ca75fd6d: Pull complete\n",
      "3d4731d03623: Pull complete\n",
      "91bba9bd0f1f: Verifying Checksum\n",
      "91bba9bd0f1f: Download complete\n",
      "a422be4dcb08: Verifying Checksum\n",
      "a422be4dcb08: Download complete\n",
      "ca10bb6dc143: Download complete\n",
      "e48bbfc7d00e: Verifying Checksum\n",
      "e48bbfc7d00e: Download complete\n",
      "860a23551ac0: Verifying Checksum\n",
      "860a23551ac0: Download complete\n",
      "cc78be876588: Verifying Checksum\n",
      "cc78be876588: Download complete\n",
      "ad6568ad37e5: Verifying Checksum\n",
      "ad6568ad37e5: Download complete\n",
      "258a31babfce: Verifying Checksum\n",
      "258a31babfce: Download complete\n",
      "116ca0069c88: Verifying Checksum\n",
      "116ca0069c88: Download complete\n",
      "ede4a5022f61: Verifying Checksum\n",
      "ede4a5022f61: Download complete\n",
      "154d6414dd17: Verifying Checksum\n",
      "154d6414dd17: Download complete\n",
      "e1f68d1c5137: Verifying Checksum\n",
      "e1f68d1c5137: Download complete\n",
      "0d4b5cd36c43: Verifying Checksum\n",
      "0d4b5cd36c43: Download complete\n",
      "fc9b6547dc7c: Verifying Checksum\n",
      "fc9b6547dc7c: Download complete\n",
      "53a5c2e0251f: Verifying Checksum\n",
      "53a5c2e0251f: Download complete\n",
      "3e8f37aba8a2: Verifying Checksum\n",
      "3e8f37aba8a2: Download complete\n",
      "4a39f6623824: Verifying Checksum\n",
      "4a39f6623824: Download complete\n",
      "d684c579871f: Verifying Checksum\n",
      "d684c579871f: Download complete\n",
      "c104fa3a7626: Verifying Checksum\n",
      "c104fa3a7626: Download complete\n",
      "30bba30584e3: Verifying Checksum\n",
      "30bba30584e3: Download complete\n",
      "22f09e497c63: Download complete\n",
      "5f66cd739f31: Verifying Checksum\n",
      "5f66cd739f31: Download complete\n",
      "c9c9a9e1cad6: Verifying Checksum\n",
      "c9c9a9e1cad6: Download complete\n",
      "630fcaa7a158: Verifying Checksum\n",
      "630fcaa7a158: Download complete\n",
      "1b8090778700: Download complete\n",
      "4ca3cacea924: Verifying Checksum\n",
      "4ca3cacea924: Download complete\n",
      "c1d86dc35ba1: Verifying Checksum\n",
      "c1d86dc35ba1: Download complete\n",
      "4aa043daa566: Verifying Checksum\n",
      "4aa043daa566: Download complete\n",
      "55f4be7d7adc: Verifying Checksum\n",
      "55f4be7d7adc: Download complete\n",
      "de51b5b1b318: Verifying Checksum\n",
      "bf1f5a5d15b8: Verifying Checksum\n",
      "bf1f5a5d15b8: Download complete\n",
      "f8e1ddeececb: Verifying Checksum\n",
      "f8e1ddeececb: Download complete\n",
      "68d63385def6: Verifying Checksum\n",
      "68d63385def6: Download complete\n",
      "70ac11bbf381: Verifying Checksum\n",
      "70ac11bbf381: Download complete\n",
      "865f53eecc40: Verifying Checksum\n",
      "865f53eecc40: Download complete\n",
      "2e6386dccac0: Download complete\n",
      "67793606273a: Verifying Checksum\n",
      "67793606273a: Download complete\n",
      "bfdaa5a6754f: Verifying Checksum\n",
      "bfdaa5a6754f: Download complete\n",
      "3e0d0b3b5c1c: Download complete\n",
      "8d6d2c98840c: Verifying Checksum\n",
      "8d6d2c98840c: Download complete\n",
      "2559630e37d2: Verifying Checksum\n",
      "2559630e37d2: Download complete\n",
      "d0f13d58f7fa: Verifying Checksum\n",
      "d0f13d58f7fa: Download complete\n",
      "91a761249212: Verifying Checksum\n",
      "91a761249212: Download complete\n",
      "7a42c1978b2e: Verifying Checksum\n",
      "7a42c1978b2e: Download complete\n",
      "2cb953eac694: Verifying Checksum\n",
      "2cb953eac694: Download complete\n",
      "929ce3ad5dc9: Verifying Checksum\n",
      "929ce3ad5dc9: Download complete\n",
      "53a5c2e0251f: Pull complete\n",
      "b00ff40d02d9: Pull complete\n",
      "3036e9b94123: Pull complete\n",
      "453fdcdda788: Pull complete\n",
      "35e12ec5e515: Pull complete\n",
      "11f61a475a23: Pull complete\n",
      "24280cf31c9a: Pull complete\n",
      "79007799e2ed: Pull complete\n",
      "03eb76abf1e5: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "5e9434e8ae41: Pull complete\n",
      "88a3e778b5bf: Pull complete\n",
      "729af2b35d14: Pull complete\n",
      "30e0a3a7e9e5: Pull complete\n",
      "0852b4bd65a1: Pull complete\n",
      "81cb421c2c25: Pull complete\n",
      "3d6b664afa23: Pull complete\n",
      "3e8f37aba8a2: Pull complete\n",
      "91bba9bd0f1f: Pull complete\n",
      "a422be4dcb08: Pull complete\n",
      "ca10bb6dc143: Pull complete\n",
      "e48bbfc7d00e: Pull complete\n",
      "860a23551ac0: Pull complete\n",
      "cc78be876588: Pull complete\n",
      "ad6568ad37e5: Pull complete\n",
      "258a31babfce: Pull complete\n",
      "116ca0069c88: Pull complete\n",
      "ede4a5022f61: Pull complete\n",
      "154d6414dd17: Pull complete\n",
      "e1f68d1c5137: Pull complete\n",
      "0d4b5cd36c43: Pull complete\n",
      "fc9b6547dc7c: Pull complete\n",
      "de51b5b1b318: Pull complete\n",
      "d684c579871f: Pull complete\n",
      "4a39f6623824: Pull complete\n",
      "30bba30584e3: Pull complete\n",
      "c104fa3a7626: Pull complete\n",
      "22f09e497c63: Pull complete\n",
      "5f66cd739f31: Pull complete\n",
      "c9c9a9e1cad6: Pull complete\n",
      "630fcaa7a158: Pull complete\n",
      "1b8090778700: Pull complete\n",
      "4ca3cacea924: Pull complete\n",
      "4aa043daa566: Pull complete\n",
      "c1d86dc35ba1: Pull complete\n",
      "55f4be7d7adc: Pull complete\n",
      "f8e1ddeececb: Pull complete\n",
      "bf1f5a5d15b8: Pull complete\n",
      "68d63385def6: Pull complete\n",
      "865f53eecc40: Pull complete\n",
      "70ac11bbf381: Pull complete\n",
      "91a761249212: Pull complete\n",
      "2cb953eac694: Pull complete\n",
      "2e6386dccac0: Pull complete\n",
      "67793606273a: Pull complete\n",
      "bfdaa5a6754f: Pull complete\n",
      "3e0d0b3b5c1c: Pull complete\n",
      "8d6d2c98840c: Pull complete\n",
      "2559630e37d2: Pull complete\n",
      "d0f13d58f7fa: Pull complete\n",
      "929ce3ad5dc9: Pull complete\n",
      "7a42c1978b2e: Pull complete\n",
      "Digest: sha256:2475b7062a16cd7ba0e5eda0ff58f206400714aafd061d4d8a1a1e8aacd59668\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      " ---> ec90adb8185e\n",
      "Step 2/9 : WORKDIR /src\n",
      " ---> Running in 11e1aad10bfd\n",
      "Removing intermediate container 11e1aad10bfd\n",
      " ---> 01fca90061fb\n",
      "Step 3/9 : RUN pip install -U pip\n",
      " ---> Running in 706ef7480aa3\n",
      "Collecting pip\n",
      "  Downloading pip-22.3-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'pip'. No files were found to uninstall.\n",
      "Successfully installed pip-22.3\n",
      "Removing intermediate container 706ef7480aa3\n",
      " ---> b43277c4c4c0\n",
      "Step 4/9 : RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
      " ---> Running in 683646f5561d\n",
      "Collecting git+https://github.com/NVIDIA-Merlin/models.git\n",
      "  Cloning https://github.com/NVIDIA-Merlin/models.git to /tmp/pip-req-build-0r_4kvrw\n",
      "\u001b[91m  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA-Merlin/models.git /tmp/pip-req-build-0r_4kvrw\n",
      "\u001b[0m  Resolved https://github.com/NVIDIA-Merlin/models.git to commit 637c5691820312d6f82d891c742ae3a81a7b6db8\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-models==0.9.0+11.g637c5691) (0.7.0)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (7.0.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (0.56.2)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2022.5.1)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.10.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.3.5)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.2.5)\n",
      "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (3.19.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (4.64.1)\n",
      "Requirement already satisfied: fsspec==2022.5.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2022.5.0)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2022.5.1)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (0.4.3)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (0.12.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (3.1.2)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (8.1.3)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (6.2)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (5.9.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.26.12)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.7.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2.2.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2.4.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.0.4)\n",
      "Requirement already satisfied: setuptools<60 in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (0.39.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.22.4)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (4.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2022.2.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.56.4)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.14.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (1.0.1)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (6.0.2)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (4.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (3.8.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (2.1.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (4.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+11.g637c5691) (6.0.1)\n",
      "Building wheels for collected packages: merlin-models\n",
      "  Building wheel for merlin-models (pyproject.toml): started\n",
      "  Building wheel for merlin-models (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for merlin-models: filename=merlin_models-0.9.0+11.g637c5691-py3-none-any.whl size=349997 sha256=ccdd1b2a1bd3f7872895e47e2efb56dc66d6d8da63e9fed34cf8649cb5002f3b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ii6lgn08/wheels/5a/43/99/d50fe2c33b4f4686db73207ce3865e0d6be6609ffb03abade5\n",
      "Successfully built merlin-models\n",
      "Installing collected packages: merlin-models\n",
      "  Attempting uninstall: merlin-models\n",
      "    Found existing installation: merlin-models 0.8.0\n",
      "    Uninstalling merlin-models-0.8.0:\n",
      "      Successfully uninstalled merlin-models-0.8.0\n",
      "Successfully installed merlin-models-0.9.0+11.g637c5691\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 683646f5561d\n",
      " ---> d2bf9df174b7\n",
      "Step 5/9 : RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
      " ---> Running in 25af4211e631\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.3.5-py2.py3-none-any.whl (211 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.9/211.9 kB 10.0 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.10.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (3.19.5)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-cloud-bigquery) (2.22.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (21.3)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.6/115.6 kB 20.0 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.9/47.9 kB 8.2 MB/s eta 0:00:00\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 185.4/185.4 kB 26.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyarrow<10.0dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (7.0.0)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.4/77.4 kB 13.3 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 79.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 kB 18.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.12.0)\n",
      "Collecting fsspec==2022.10.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 22.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.56.4)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-bigquery) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow<10.0dev,>=3.0.0->google-cloud-bigquery) (1.22.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.9-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.4/408.4 kB 43.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.8)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=ddd96d98bf80b63f68857b58ac2ec89f615210318c4c809bb2565fbcce6472d4\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/f7/20/63b767d529308c530275a9738e4a46a848757cc86040dafe19\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, protobuf, grpcio, google-crc32c, fsspec, proto-plus, google-resumable-media, grpcio-status, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-bigquery-storage, google-cloud-bigquery, gcsfs\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.41.0\n",
      "    Uninstalling grpcio-1.41.0:\n",
      "      Successfully uninstalled grpcio-1.41.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "wandb 0.13.3 requires protobuf<4.0dev,>=3.12.0, but you have protobuf 4.21.9 which is incompatible.\n",
      "tensorflow-metadata 1.10.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.9 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.9 which is incompatible.\n",
      "merlin-core 0.7.0 requires fsspec==2022.5.0, but you have fsspec 2022.10.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fsspec-2022.10.0 gcsfs-2022.10.0 google-api-core-2.10.2 google-cloud-bigquery-3.3.5 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 grpcio-1.50.0 grpcio-status-1.50.0 proto-plus-1.22.1 protobuf-4.21.9\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 25af4211e631\n",
      " ---> 9ed19577d38a\n",
      "Step 6/9 : RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular==1.5\n",
      " ---> Running in 9873af93840b\n",
      "Collecting google-cloud-aiplatform[cloud_profiler]\n",
      "  Downloading google_cloud_aiplatform-1.18.2-py2.py3-none-any.whl (2.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 38.0 MB/s eta 0:00:00\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.14.tar.gz (304 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 304.3/304.3 kB 37.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nvtabular==1.5 in /usr/local/lib/python3.8/dist-packages (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nvtabular==1.5) (1.9.1)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nvtabular==1.5) (0.7.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (21.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (1.22.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (4.21.9)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.8/233.8 kB 31.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.5.0)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.6/206.6 kB 30.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.10.2)\n",
      "Collecting tensorflow<3.0.0dev,>=2.4.0\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 578.1/578.1 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting werkzeug<2.1.0dev,>=2.0.0\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 289.2/289.2 kB 36.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-profile<3.0.0dev,>=2.4.0\n",
      "  Downloading tensorboard_plugin_profile-2.8.0-py3-none-any.whl (5.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 83.2 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.4/662.4 kB 54.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.8/dist-packages (from kfp) (1.2.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.2.0)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 kB 15.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 11.3 MB/s eta 0:00:00\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 kB 22.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 9.6 MB/s eta 0:00:00\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.1/58.1 kB 9.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 67.4 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 65.4 MB/s eta 0:00:00\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.6/13.6 MB 89.8 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 9.8 MB/s eta 0:00:00\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.6.1-py3-none-any.whl (38 kB)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire<1,>=0.3.1->kfp) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire<1,>=0.3.1->kfp) (2.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.56.4)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (2.22.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.50.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.50.0)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 16.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.9)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.1->kfp) (45.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.4.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.8.2)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.12)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.4.1)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (2022.5.1)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (2022.5.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (7.0.0)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (1.2.5)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 21.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (0.56.2)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (1.3.5)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (4.64.1)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.5) (1.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform[cloud_profiler]) (3.0.9)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.34.2)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.7.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 92.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.1.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.22.4)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 438.7/438.7 kB 45.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.3.0)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 60.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (14.0.6)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 78.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.27.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (0.4.3)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (0.12.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (1.3.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (5.9.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (6.2)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (3.1.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (1.0.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (1.7.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (2.4.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (1.0.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (1.5.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.5) (4.12.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.5) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular==1.5) (2022.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.54->merlin-core>=0.2.0->nvtabular==1.5) (3.8.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (1.0.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.5) (2.1.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.5) (4.0.0)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.14-py3-none-any.whl size=426465 sha256=e4e5f9fd49255c866e60db13f97ad74ba8c2242f81c6a307e61a3391eae135a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/c5/4d/d3fe2a6c76b64ebb4ae7a2a7a35b00d01bb7dcfa741e460152\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115925 sha256=f150ebff92128195112dc49ab48a51a9c2baf84477ea7990e25d6019357ac721\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/05/8d/5951e074fe660f634ca0a3402fa9903d9f772014fdb0e593dd\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=5fb6336febde800f19ab874c9ec458f831062b087522061c4ed8b0072d6e13c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/3a/82/1f0a3c4193b6b4420910b6c3a6ff206358533e968e86aa4ecc\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=f791911544b9e643b4067b806f42b6869b51a0dbe1ebefaf24e58eeae0be3d6f\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/55/09/dc55b59d831b95b88ad0f65d4026aa36e9974601abe17f8659\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: keras, flatbuffers, werkzeug, uritemplate, typer, tensorflow-estimator, tabulate, strip-hints, requests-toolbelt, PyYAML, pydantic, protobuf, jsonschema, httplib2, gviz-api, fsspec, fire, docstring-parser, Deprecated, cachetools, tensorboard-plugin-profile, kfp-server-api, kfp-pipeline-spec, google-auth, kubernetes, grpcio-status, google-auth-httplib2, tensorboard, grpc-google-iam-v1, google-api-python-client, tensorflow, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.2.2\n",
      "    Uninstalling Werkzeug-2.2.2:\n",
      "      Successfully uninstalled Werkzeug-2.2.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.9\n",
      "    Uninstalling protobuf-4.21.9:\n",
      "      Successfully uninstalled protobuf-4.21.9\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.16.0\n",
      "    Uninstalling jsonschema-4.16.0:\n",
      "      Successfully uninstalled jsonschema-4.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.10.0\n",
      "    Uninstalling fsspec-2022.10.0:\n",
      "      Successfully uninstalled fsspec-2022.10.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.12.0\n",
      "    Uninstalling google-auth-2.12.0:\n",
      "      Successfully uninstalled google-auth-2.12.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.50.0\n",
      "    Uninstalling grpcio-status-1.50.0:\n",
      "      Successfully uninstalled grpcio-status-1.50.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.5.0\n",
      "    Uninstalling google-cloud-storage-2.5.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.5.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.3.5\n",
      "    Uninstalling google-cloud-bigquery-3.3.5:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-channelz 1.49.1 requires protobuf>=4.21.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "gcsfs 2022.10.0 requires fsspec==2022.10.0, but you have fsspec 2022.5.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 docstring-parser-0.15 fire-0.4.0 flatbuffers-22.10.26 fsspec-2022.5.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.18.2 google-cloud-bigquery-2.34.4 google-cloud-resource-manager-1.6.3 google-cloud-storage-1.44.0 grpc-google-iam-v1-0.12.4 grpcio-status-1.48.2 gviz-api-1.10.0 httplib2-0.20.4 jsonschema-3.2.0 keras-2.10.0 kfp-1.8.14 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-18.20.0 protobuf-3.19.6 pydantic-1.10.2 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0 tensorboard-2.10.1 tensorboard-plugin-profile-2.8.0 tensorflow-2.10.0 tensorflow-estimator-2.10.0 typer-0.6.1 uritemplate-3.0.1 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 9873af93840b\n",
      " ---> 5c94e72afc9a\n",
      "Step 7/9 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      " ---> Running in 1d78f0f302b5\n",
      "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed\u001b[0m\u001b[91m   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2537  100  2537    0     0   117k      0 --:--:-- --:--:-- --:--:--  117k\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:3 http://packages.cloud.google.com/apt cloud-sdk InRelease [6751 B]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [740 kB]\n",
      "Get:5 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [346 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2267 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1655 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [926 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [37.7 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1225 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2737 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1772 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 25.3 MB in 4s (7063 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "Suggested packages:\n",
      "  google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-python\n",
      "  google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator\n",
      "  google-cloud-sdk-datastore-emulator kubectl\n",
      "The following NEW packages will be installed:\n",
      "  google-cloud-sdk\n",
      "0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\n",
      "Need to get 129 MB of archives.\n",
      "After this operation, 714 MB of additional disk space will be used.\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk/main amd64 google-cloud-sdk all 407.0.0-0 [129 MB]\n",
      "Fetched 129 MB in 2s (77.8 MB/s)\n",
      "Selecting previously unselected package google-cloud-sdk.\n",
      "(Reading database ... 41552 files and directories currently installed.)\n",
      "Preparing to unpack .../google-cloud-sdk_407.0.0-0_all.deb ...\n",
      "Unpacking google-cloud-sdk (407.0.0-0) ...\n",
      "Setting up google-cloud-sdk (407.0.0-0) ...\n",
      "Removing intermediate container 1d78f0f302b5\n",
      " ---> 8a692972d8c3\n",
      "Step 8/9 : COPY trainer/* ./\n",
      " ---> 294f04559f1e\n",
      "Step 9/9 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      " ---> Running in 3d773dd85ae5\n",
      "Removing intermediate container 3d773dd85ae5\n",
      " ---> 18f9c3bbaef7\n",
      "Successfully built 18f9c3bbaef7\n",
      "Successfully tagged gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09\n",
      "The push refers to repository [gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09]\n",
      "6878b0b87f59: Preparing\n",
      "6ac0a4f05d25: Preparing\n",
      "5d2b6daad3c3: Preparing\n",
      "1c35515cb342: Preparing\n",
      "54381cfe78bc: Preparing\n",
      "8d7ba65ad916: Preparing\n",
      "940828c4fba7: Preparing\n",
      "bd113aa55fbd: Preparing\n",
      "e58f9c500afd: Preparing\n",
      "c3c9ca29c9f2: Preparing\n",
      "e844c8057c95: Preparing\n",
      "0b07555d3a5b: Preparing\n",
      "a1537cf26842: Preparing\n",
      "00679b7c9426: Preparing\n",
      "add0b850bdf8: Preparing\n",
      "c2e412b87e2d: Preparing\n",
      "7730d4c011cd: Preparing\n",
      "f83c32bae622: Preparing\n",
      "86147dce553c: Preparing\n",
      "a86afd489635: Preparing\n",
      "725647e88671: Preparing\n",
      "78e008bc66d2: Preparing\n",
      "e7d002ddb49b: Preparing\n",
      "f2b540bc31be: Preparing\n",
      "442b3d22fc2d: Preparing\n",
      "72f4d03b40d8: Preparing\n",
      "54244453f24a: Preparing\n",
      "c9dfb1d8d420: Preparing\n",
      "c1af80eb8994: Preparing\n",
      "5c0e49e0fefd: Preparing\n",
      "64579a0c8694: Preparing\n",
      "82432f6543d2: Preparing\n",
      "efbb58199899: Preparing\n",
      "f390faf5524c: Preparing\n",
      "daee9dea71d9: Preparing\n",
      "dc74b4fa6312: Preparing\n",
      "cf7ec5236059: Preparing\n",
      "2784fc353e53: Preparing\n",
      "4748e1954466: Preparing\n",
      "17dd0d3a32ca: Preparing\n",
      "1a4d9b216faa: Preparing\n",
      "29ec3f10d323: Preparing\n",
      "b103452845b7: Preparing\n",
      "5de10b8eda77: Preparing\n",
      "93a1a17119ba: Preparing\n",
      "0b949eaca829: Preparing\n",
      "8d7ba65ad916: Waiting\n",
      "76dffad7db12: Preparing\n",
      "940828c4fba7: Waiting\n",
      "103f14ded07d: Preparing\n",
      "05a0d2d578a6: Preparing\n",
      "bd113aa55fbd: Waiting\n",
      "1f7bd087086a: Preparing\n",
      "a03ce844e2ad: Preparing\n",
      "e58f9c500afd: Waiting\n",
      "b5583e44add1: Preparing\n",
      "3ff439c0455c: Preparing\n",
      "c3c9ca29c9f2: Waiting\n",
      "2ee8c052052a: Preparing\n",
      "e844c8057c95: Waiting\n",
      "f3154f787b0f: Preparing\n",
      "944a1106424f: Preparing\n",
      "0b07555d3a5b: Waiting\n",
      "01386fafb257: Preparing\n",
      "8a9d499564b0: Preparing\n",
      "d882bfae03e4: Preparing\n",
      "a1537cf26842: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "913f47d5362d: Preparing\n",
      "00679b7c9426: Waiting\n",
      "06f02804b89d: Preparing\n",
      "54beb86c2dbe: Preparing\n",
      "78e008bc66d2: Waiting\n",
      "add0b850bdf8: Waiting\n",
      "aa57b43dc9e0: Preparing\n",
      "ae5c80704277: Preparing\n",
      "e7d002ddb49b: Waiting\n",
      "d1cc4baf7a93: Preparing\n",
      "c2e412b87e2d: Waiting\n",
      "8fd21a588646: Preparing\n",
      "f2b540bc31be: Waiting\n",
      "b470f3b3096a: Preparing\n",
      "7730d4c011cd: Waiting\n",
      "9af2b05f2c3b: Preparing\n",
      "4cf9aed48cda: Preparing\n",
      "f83c32bae622: Waiting\n",
      "442b3d22fc2d: Waiting\n",
      "57f574ab1503: Preparing\n",
      "c5b9544e7743: Preparing\n",
      "86147dce553c: Waiting\n",
      "72f4d03b40d8: Waiting\n",
      "a86afd489635: Waiting\n",
      "54244453f24a: Waiting\n",
      "c3f11d77a5de: Preparing\n",
      "725647e88671: Waiting\n",
      "c9dfb1d8d420: Waiting\n",
      "c1af80eb8994: Waiting\n",
      "1a4d9b216faa: Waiting\n",
      "29ec3f10d323: Waiting\n",
      "5c0e49e0fefd: Waiting\n",
      "b103452845b7: Waiting\n",
      "5de10b8eda77: Waiting\n",
      "64579a0c8694: Waiting\n",
      "93a1a17119ba: Waiting\n",
      "82432f6543d2: Waiting\n",
      "0b949eaca829: Waiting\n",
      "76dffad7db12: Waiting\n",
      "efbb58199899: Waiting\n",
      "103f14ded07d: Waiting\n",
      "05a0d2d578a6: Waiting\n",
      "54beb86c2dbe: Waiting\n",
      "1f7bd087086a: Waiting\n",
      "f390faf5524c: Waiting\n",
      "aa57b43dc9e0: Waiting\n",
      "a03ce844e2ad: Waiting\n",
      "daee9dea71d9: Waiting\n",
      "ae5c80704277: Waiting\n",
      "b5583e44add1: Waiting\n",
      "3ff439c0455c: Waiting\n",
      "d1cc4baf7a93: Waiting\n",
      "dc74b4fa6312: Waiting\n",
      "2ee8c052052a: Waiting\n",
      "8fd21a588646: Waiting\n",
      "f3154f787b0f: Waiting\n",
      "cf7ec5236059: Waiting\n",
      "b470f3b3096a: Waiting\n",
      "2784fc353e53: Waiting\n",
      "944a1106424f: Waiting\n",
      "9af2b05f2c3b: Waiting\n",
      "4748e1954466: Waiting\n",
      "4cf9aed48cda: Waiting\n",
      "17dd0d3a32ca: Waiting\n",
      "01386fafb257: Waiting\n",
      "57f574ab1503: Waiting\n",
      "8a9d499564b0: Waiting\n",
      "c5b9544e7743: Waiting\n",
      "d882bfae03e4: Waiting\n",
      "c3f11d77a5de: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "913f47d5362d: Waiting\n",
      "06f02804b89d: Waiting\n",
      "6878b0b87f59: Pushed\n",
      "54381cfe78bc: Pushed\n",
      "940828c4fba7: Pushed\n",
      "bd113aa55fbd: Layer already exists\n",
      "e58f9c500afd: Layer already exists\n",
      "c3c9ca29c9f2: Layer already exists\n",
      "e844c8057c95: Layer already exists\n",
      "1c35515cb342: Pushed\n",
      "0b07555d3a5b: Layer already exists\n",
      "8d7ba65ad916: Pushed\n",
      "a1537cf26842: Layer already exists\n",
      "00679b7c9426: Layer already exists\n",
      "add0b850bdf8: Layer already exists\n",
      "c2e412b87e2d: Layer already exists\n",
      "7730d4c011cd: Layer already exists\n",
      "f83c32bae622: Layer already exists\n",
      "a86afd489635: Layer already exists\n",
      "86147dce553c: Layer already exists\n",
      "725647e88671: Layer already exists\n",
      "78e008bc66d2: Layer already exists\n",
      "e7d002ddb49b: Layer already exists\n",
      "f2b540bc31be: Layer already exists\n",
      "442b3d22fc2d: Layer already exists\n",
      "72f4d03b40d8: Layer already exists\n",
      "54244453f24a: Layer already exists\n",
      "c9dfb1d8d420: Layer already exists\n",
      "c1af80eb8994: Layer already exists\n",
      "5c0e49e0fefd: Layer already exists\n",
      "64579a0c8694: Layer already exists\n",
      "82432f6543d2: Layer already exists\n",
      "efbb58199899: Layer already exists\n",
      "f390faf5524c: Layer already exists\n",
      "daee9dea71d9: Layer already exists\n",
      "dc74b4fa6312: Layer already exists\n",
      "cf7ec5236059: Layer already exists\n",
      "2784fc353e53: Layer already exists\n",
      "4748e1954466: Layer already exists\n",
      "17dd0d3a32ca: Layer already exists\n",
      "1a4d9b216faa: Layer already exists\n",
      "29ec3f10d323: Layer already exists\n",
      "b103452845b7: Layer already exists\n",
      "5de10b8eda77: Layer already exists\n",
      "93a1a17119ba: Layer already exists\n",
      "0b949eaca829: Layer already exists\n",
      "76dffad7db12: Layer already exists\n",
      "103f14ded07d: Layer already exists\n",
      "05a0d2d578a6: Layer already exists\n",
      "1f7bd087086a: Layer already exists\n",
      "a03ce844e2ad: Layer already exists\n",
      "b5583e44add1: Layer already exists\n",
      "3ff439c0455c: Layer already exists\n",
      "2ee8c052052a: Layer already exists\n",
      "f3154f787b0f: Layer already exists\n",
      "944a1106424f: Layer already exists\n",
      "01386fafb257: Layer already exists\n",
      "8a9d499564b0: Layer already exists\n",
      "d882bfae03e4: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "913f47d5362d: Layer already exists\n",
      "06f02804b89d: Layer already exists\n",
      "54beb86c2dbe: Layer already exists\n",
      "aa57b43dc9e0: Layer already exists\n",
      "ae5c80704277: Layer already exists\n",
      "d1cc4baf7a93: Layer already exists\n",
      "8fd21a588646: Layer already exists\n",
      "b470f3b3096a: Layer already exists\n",
      "9af2b05f2c3b: Layer already exists\n",
      "4cf9aed48cda: Layer already exists\n",
      "57f574ab1503: Layer already exists\n",
      "c5b9544e7743: Layer already exists\n",
      "c3f11d77a5de: Layer already exists\n",
      "6ac0a4f05d25: Pushed\n",
      "5d2b6daad3c3: Pushed\n",
      "latest: digest: sha256:4800bc0d34dd2ed96a1ca4a9379a864464c36a8afe794d103eab30ba632b5825 size: 15696\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                               STATUS\n",
      "4708cfce-ac01-40bd-8085-d6570d7b809d  2022-10-27T15:33:34+00:00  8M24S     gs://hybrid-vertex_cloudbuild/source/1666884813.252391-cf62d9b4af56478ba12c071ed1dec99e.tgz  gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1473cd1-af0d-4c63-aff0-d1286f096170",
   "metadata": {},
   "source": [
    "# Vertex Train Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cdca6-d053-401d-8f95-5f24d3192d7a",
   "metadata": {},
   "source": [
    "### Prepare `worker_pool_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59df486c-8698-4528-9b08-dbc19de7c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    # args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        # \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6d7e-959a-4354-85d5-36f7a9a662ed",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d7050b-e6a8-4314-b3df-27e7abf0d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ====================================================\n",
    "# Single | Single machine, single GPU\n",
    "# ====================================================\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# ====================================================\n",
    "# Mirrored | Single Machine; multiple GPU\n",
    "# ====================================================\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-2g'           # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 2\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# ====================================================\n",
    "# Multi-Worker | Multiple Machines, 1 GPU per Machine\n",
    "# ====================================================\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 10\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 2\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 4\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158e910-f435-40fe-8150-6529191cab5d",
   "metadata": {},
   "source": [
    "## Train Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0383-d688-4fb9-b252-152a4639572e",
   "metadata": {},
   "source": [
    "### Previously defined Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b9fcc0-f78a-4c43-9f71-aade96b55d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "VERSION: jtv1\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09\n",
      "MODEL_NAME: 2tower\n",
      "FRAMEWORK: merlin-tf\n",
      "MODEL_DISPLAY_NAME: vertex-merlin-tf-2tower-jtv1\n",
      "WORKSPACE: gs://jt-merlin-scaling/vertex-merlin-tf-2tower-jtv1\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT_ID}\")\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "487df262-9e3d-4dd9-b7c5-94ca8da2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: t-2tower-merlin-tf-jtv1\n",
      "RUN_NAME_PREFIX: run-v1\n",
      "TRAIN_DATA: /gcs/jt-merlin-scaling/nvt-last5-v3sub/nvt-processed/train\n",
      "VALID_DATA: /gcs/jt-merlin-scaling/nvt-last5-v3sub/nvt-processed/valid\n",
      "WORKFLOW_DIR: gs://jt-merlin-scaling/nvt-last5-v3sub/nvt-analyzed\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 't'\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-{MODEL_NAME}-{FRAMEWORK}-{VERSION}'\n",
    "RUN_NAME_PREFIX = f'run-v1' # timestamp assigned during job\n",
    "\n",
    "DATA_DIR = '/gcs/jt-merlin-scaling/nvt-last5-v3sub/nvt-processed'\n",
    "TRAIN_DATA = f'{DATA_DIR}/train' #/_gcs_file_list.txt'\n",
    "VALID_DATA = f'{DATA_DIR}/valid' #/_gcs_file_list.txt'\n",
    "WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-v3sub/nvt-analyzed'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME_PREFIX: {RUN_NAME_PREFIX}\")\n",
    "print(f\"TRAIN_DATA: {TRAIN_DATA}\")\n",
    "print(f\"VALID_DATA: {VALID_DATA}\")\n",
    "print(f\"WORKFLOW_DIR: {WORKFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded705c-b752-4459-a8a3-4cab97f94360",
   "metadata": {},
   "source": [
    "### Managed TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a76672e0-ca2f-4c2c-9028-9d388bfa2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard resource name: projects/934903580331/locations/us-central1/tensorboards/70659015247396864\n",
      "TENSORBOARD_DISPLAY_NAME: tb-test-twotower-merlin-tf-jtv1\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Managed Tensorboard\n",
    "# ====================================================\n",
    "\n",
    "# create new\n",
    "# TENSORBOARD_DISPLAY_NAME = f\"tb-{EXPERIMENT_NAME}\"\n",
    "# tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "# EXPERIMENT_TB = f'{tensorboard.gca_resource.name}'\n",
    "\n",
    "# use existing\n",
    "EXPERIMENT_TB = 'projects/934903580331/locations/us-central1/tensorboards/70659015247396864'\n",
    "TENSORBOARD_DISPLAY_NAME = 'tb-test-twotower-merlin-tf-jtv1'\n",
    "\n",
    "\n",
    "print(\"TensorBoard resource name:\", EXPERIMENT_TB)\n",
    "print(\"TENSORBOARD_DISPLAY_NAME:\", TENSORBOARD_DISPLAY_NAME)\n",
    "# TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/7439955380509081600\"\n",
    "# tensorboard = vertex_ai.Tensorboard(f'{TENSORBOARD}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58659-7506-4275-b26d-f37bfd3c9c79",
   "metadata": {},
   "source": [
    "### Worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "811a40ec-4829-4013-8820-2ffbfb5de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'command': ['sh',\n",
      "                                 '-euc',\n",
      "                                 'python -m train_task '\n",
      "                                 '--tb_name=projects/934903580331/locations/us-central1/tensorboards/70659015247396864 '\n",
      "                                 '--per_gpu_batch_size=16384     '\n",
      "                                 '--train_output_bucket=jt-merlin-scaling '\n",
      "                                 '--train_dir=/gcs/jt-merlin-scaling/nvt-last5-v3sub/nvt-processed/train '\n",
      "                                 '--valid_dir=/gcs/jt-merlin-scaling/nvt-last5-v3sub/nvt-processed/valid '\n",
      "                                 '--workflow_dir=gs://jt-merlin-scaling/nvt-last5-v3sub/nvt-analyzed     '\n",
      "                                 '--num_epochs=1 --learning_rate=0.001 '\n",
      "                                 '--distribute=single     '\n",
      "                                 '--experiment_name=t-2tower-merlin-tf-jtv1 '\n",
      "                                 '--experiment_run=run-v1 '\n",
      "                                 '--project=hybrid-vertex '\n",
      "                                 '--location=us-central1'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4096*4      # TODO: `batch_size * 4 ? jw\n",
    "LEARNING_RATE = 0.001\n",
    "LAYERS = \"[512, 256, 128]\"\n",
    "\n",
    "# ACCELERATOR_NUM = REPLICA_COUNT * PER_MACHINE_ACCELERATOR_COUNT\n",
    "# gpus = json.dumps([list(range(ACCELERATOR_NUM))]).replace(' ','')\n",
    "\n",
    "# WORKER_CMD = [\"python\", \"trainer/train_task.py\"]\n",
    "# WORKER_CMD = [\"python\", \"-m\", \"train_task\"]\n",
    "\n",
    "# WORKER_ARGS = [\n",
    "#     f'--project={PROJECT_ID}',\n",
    "#     f'--location={REGION}',\n",
    "#     f'--tb_name={EXPERIMENT_TB}',\n",
    "#     f'--workflow_dir={WORKFLOW_DIR}',\n",
    "#     f'--train_dir={TRAIN_DATA}',\n",
    "#     f'--valid_dir={VALID_DATA}',\n",
    "#     f'--train_output_bucket={OUTPUT_BUCKET}',\n",
    "#     f'--experiment_name={EXPERIMENT_NAME}',\n",
    "#     f'--experiment_run={RUN_NAME_PREFIX}',\n",
    "#     f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "#     f'--per_gpu_batch_size={BATCH_SIZE}',\n",
    "#     f'--layer_sizes={LAYERS}',\n",
    "#     f'--learning_rate={LEARNING_RATE}',\n",
    "#     f'--num_epochs={NUM_EPOCHS}',\n",
    "# ]\n",
    "\n",
    "WORKER_CMD = [\n",
    "    'sh',\n",
    "    '-euc',\n",
    "    f'''python -m train_task --tb_name={EXPERIMENT_TB} --per_gpu_batch_size={BATCH_SIZE} \\\n",
    "    --train_output_bucket={OUTPUT_BUCKET} --train_dir={TRAIN_DATA} --valid_dir={VALID_DATA} --workflow_dir={WORKFLOW_DIR} \\\n",
    "    --num_epochs={NUM_EPOCHS} --learning_rate={LEARNING_RATE} --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "    --experiment_name={EXPERIMENT_NAME} --experiment_run={RUN_NAME_PREFIX} --project={PROJECT_ID} --location={REGION}'''\n",
    "]\n",
    "    # --layer_sizes={LAYERS} \\\n",
    "\n",
    "# ====================================================\n",
    "# Worker pool specs\n",
    "# ====================================================\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    # args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f5b0-22ab-451d-ba49-09cc445d053f",
   "metadata": {},
   "source": [
    "## Submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bddd12da-d6ef-4ffe-917f-408b7bade0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGING_BUCKET : gs://jt-merlin-scaling/t-2tower-merlin-tf-jtv1\n",
      "JOB_NAME : 2209-train-vertex-merlin-tf-2tower-jtv1\n",
      "\n",
      "gpu_type : nvidia_tesla_a100\n",
      "gpu_per_replica : 1\n",
      "replica_cnt : 1\n"
     ]
    }
   ],
   "source": [
    "STAGING_BUCKET = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "job_prefix = '2209'\n",
    "JOB_NAME = f'{job_prefix}-train-{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# labels for train job\n",
    "gpu_type = ACCELERATOR_TYPE.lower()\n",
    "gpu_per_replica = PER_MACHINE_ACCELERATOR_COUNT\n",
    "replica_cnt = REPLICA_COUNT\n",
    "\n",
    "print(f'STAGING_BUCKET : {STAGING_BUCKET}')\n",
    "print(f'JOB_NAME : {JOB_NAME}\\n')\n",
    "print(f'gpu_type : {gpu_type}')\n",
    "print(f'gpu_per_replica : {gpu_per_replica}')\n",
    "print(f'replica_cnt : {replica_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "041217fa-0cf5-477a-b1a8-e5cfb58712bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=934903580331&resource=ml_job%2Fjob_id%2F8309069569364852736&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%228309069569364852736%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m job \u001b[38;5;241m=\u001b[39m vertex_ai\u001b[38;5;241m.\u001b[39mCustomJob(\n\u001b[1;32m      2\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mJOB_NAME,\n\u001b[1;32m      3\u001b[0m     worker_pool_specs\u001b[38;5;241m=\u001b[39mWORKER_POOL_SPECS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     }\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERTEX_SA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# tensorboard=EXPERIMENT_TB,\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_web_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/base.py:807\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    806\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    810\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/jobs.py:1615\u001b[0m, in \u001b[0;36mCustomJob.run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, tensorboard, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensorboard:\n\u001b[1;32m   1608\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1609\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Tensorboard:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1610\u001b[0m         \u001b[38;5;241m%\u001b[39m console_utils\u001b[38;5;241m.\u001b[39mcustom_job_tensorboard_console_uri(\n\u001b[1;32m   1611\u001b[0m             tensorboard, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name\n\u001b[1;32m   1612\u001b[0m         )\n\u001b[1;32m   1613\u001b[0m     )\n\u001b[0;32m-> 1615\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/cloud/aiplatform/jobs.py:1046\u001b[0m, in \u001b[0;36m_RunnableJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _JOB_ERROR_STATES:\n\u001b[0;32m-> 1046\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1048\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=934903580331&resource=ml_job%2Fjob_id%2F8309069569364852736&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%228309069569364852736%22\"\n"
     ]
    }
   ],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    labels={\n",
    "        # 'mm_image' : 'nightly',\n",
    "        'gpu' : f'{gpu_type}',\n",
    "        'gpu_per_replica' : f'{gpu_per_replica}',\n",
    "        'replica_cnt' : f'{replica_cnt}',\n",
    "    }\n",
    ")\n",
    "job.run(sync=True, \n",
    "        service_account=VERTEX_SA,\n",
    "        # tensorboard=EXPERIMENT_TB,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b978-1a64-40e8-b8c4-149a7a40bca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
