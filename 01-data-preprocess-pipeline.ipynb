{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592fa423-5ce2-4e2b-a7ab-97174c387d47",
   "metadata": {},
   "source": [
    "# Preprocessing with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17770281-7bed-4495-a70f-1397441b60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-pipeline-components \n",
    "# !pip install google-cloud-bigquery-storage \n",
    "# !pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb3673e-3b2a-436c-a002-b09ab92aa9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.19\n",
      "google_cloud_pipeline_components version: 1.0.40\n",
      "aiplatform SDK version: 1.23.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbd89bc-5668-4e36-a0b8-c8a4ad835f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# import nvtabular as nvt\n",
    "# from nvtabular.ops import (\n",
    "#     Categorify,\n",
    "#     TagAsUserID,\n",
    "#     TagAsItemID,\n",
    "#     TagAsItemFeatures,\n",
    "#     TagAsUserFeatures,\n",
    "#     AddMetadata,\n",
    "#     ListSlice\n",
    "# )\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.schema.tags import Tags\n",
    "\n",
    "# import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "# import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e737b-e271-445b-a0c3-3b2dca620352",
   "metadata": {},
   "source": [
    "### Load config from setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the 00_environment_setup.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59e733b8-6417-431a-9ca5-51b6e4924642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baea87eb-a282-4ce5-ada0-bc1791b0da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1278545-3674-4552-832f-8bedc19d5791",
   "metadata": {},
   "source": [
    "# Define preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0466709-cfb6-42ae-9d91-f02a2e8fb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket definitions\n",
    "# VERSION = 'v1-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions\n",
    "# IMAGE_NAME = 'nvt-preprocessing'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# DOCKERNAME = f'nvtabular-160' # 150\n",
    "\n",
    "# # Pipeline definitions\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "# print(f\"VERSION: {VERSION}\")\n",
    "# print(f\"APP: {APP}\")\n",
    "# print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "# print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "# print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "# print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "# print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c78b87f1-429e-4fe6-821f-5fcf2b4818bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0816f671-55c0-40a8-a0a9-3f280c16b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the current work dir\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23ce7975-79fc-4280-9452-5162244cfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PREPROC_SUB_DIR = 'preprocessor'\n",
    "PIPELINE_SUB_DIR = 'process_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa05f47a-e39a-4f81-b96c-3da03005520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/__init__.py\n",
    "\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933d24f-8c8e-46e8-a72b-a821f2be7fe3",
   "metadata": {},
   "source": [
    "## preprocessing task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def34240-3173-42e8-b00d-fea08a951517",
   "metadata": {},
   "source": [
    "* see `LocalCudaCluster` [src](https://github.com/rapidsai/dask-cuda/blob/branch-23.04/dask_cuda/local_cuda_cluster.py) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a5fe2c1-e2b9-4162-be43-91be39b7f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocessor/preprocess_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/preprocess_task.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "# logging.disable(logging.WARNING)\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import fsspec\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.io.shuffle import Shuffle\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "# import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "# # =============================================\n",
    "# # featutres\n",
    "# # =============================================\n",
    "# item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "# playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "# item_features_cat = [\n",
    "#     'artist_name_can',\n",
    "#     'track_name_can',\n",
    "#     'artist_genres_can',\n",
    "# ]\n",
    "\n",
    "# item_features_cont = [\n",
    "#     'duration_ms_can',\n",
    "#     'track_pop_can',\n",
    "#     'artist_pop_can',\n",
    "#     'artist_followers_can',\n",
    "# ]\n",
    "\n",
    "# playlist_features_cat = [\n",
    "#     'description_pl',\n",
    "#     'name',\n",
    "#     'collaborative',\n",
    "# ]\n",
    "\n",
    "# playlist_features_cont = [\n",
    "#     'duration_ms_seed_pl',\n",
    "#     'n_songs_pl',\n",
    "#     'num_artists_pl',\n",
    "#     'num_albums_pl',\n",
    "# ]\n",
    "\n",
    "# seq_feats_cat = [\n",
    "#     'artist_name_pl',\n",
    "#     'track_uri_pl',\n",
    "#     'track_name_pl',\n",
    "#     'album_name_pl',\n",
    "#     'artist_genres_pl',\n",
    "# ]\n",
    "\n",
    "# CAT = playlist_features_cat + item_features_cat\n",
    "# CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "# =============================================\n",
    "# featutres\n",
    "# =============================================\n",
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = [\n",
    "    'artist_name_can',\n",
    "    'track_name_can',\n",
    "    'album_name_can',\n",
    "    'artist_genres_can',\n",
    "    'track_key_can',\n",
    "    'track_mode_can',\n",
    "    'track_time_signature_can'\n",
    "]\n",
    "\n",
    "item_features_cont = [\n",
    "    'duration_ms_can',\n",
    "    'track_pop_can',\n",
    "    'artist_pop_can',\n",
    "    'artist_followers_can',\n",
    "]\n",
    "\n",
    "playlist_features_cat = [\n",
    "    # 'description_pl',\n",
    "    'pl_name_src',\n",
    "    'pl_collaborative_src',\n",
    "]\n",
    "\n",
    "playlist_features_cont = [\n",
    "    'pl_duration_ms_new',\n",
    "    'num_pl_songs_new',\n",
    "    'num_pl_artists_new',\n",
    "    'num_pl_albums_new',\n",
    "]\n",
    "\n",
    "seq_feats_cat = [\n",
    "    'artist_name_pl',\n",
    "    'track_uri_pl',\n",
    "    'track_name_pl',\n",
    "    'album_name_pl',\n",
    "    'artist_genres_pl',\n",
    "    'track_key_pl',\n",
    "    'track_mode_pl',\n",
    "    'track_time_signature_pl'\n",
    "]\n",
    "\n",
    "# seq_feats_cont = [\n",
    "#     'duration_ms_songs_pl',\n",
    "#     'artist_pop_pl',\n",
    "#     'artists_followers_pl',\n",
    "#     'track_pop_pl',\n",
    "#     'track_danceability_pl',\n",
    "#     'track_energy_pl',\n",
    "#     'track_loudness_pl',\n",
    "#     'track_acousticness_pl',\n",
    "#     'track_instrumentalness_pl',\n",
    "#     'track_liveness',\n",
    "#     'track_valence_pl',\n",
    "#     'track_tempo_pl',\n",
    "#     'track_speechiness_pl',\n",
    "# ]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "# =============================================\n",
    "# create cluster\n",
    "# =============================================\n",
    "def create_cluster(\n",
    "    n_workers,\n",
    "    device_limit_frac,\n",
    "    device_pool_frac,\n",
    "    memory_limit\n",
    "):\n",
    "    \"\"\"Create a Dask cluster to apply the transformations steps to the Dataset.\"\"\"\n",
    "    device_size = device_mem_size()\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    rmm_pool_size = (device_pool_size // 256) * 256\n",
    "\n",
    "    cluster = LocalCUDACluster(\n",
    "        n_workers=n_workers,\n",
    "        device_memory_limit=device_limit,\n",
    "        rmm_pool_size=rmm_pool_size,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "    return Client(cluster)\n",
    "\n",
    "# =============================================\n",
    "#            Create & Save dataset\n",
    "# =============================================\n",
    "\n",
    "def create_parquet_nvt_dataset(\n",
    "    # data_path,\n",
    "    frac_size,\n",
    "    data_prefix,\n",
    "    bucket_name,\n",
    "    file_pattern,\n",
    "):\n",
    "    \"\"\"Create a nvt.Dataset definition for the parquet files.\"\"\"\n",
    "    \n",
    "    # BUCKET = 'gs://spotify-builtin-2t'\n",
    "    # DATA_PATH = f\"{BUCKET}/{data_prefix}/0000000000**.snappy.parquet\"\n",
    "    DATA_PATH = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\" #0000000000**.snappy.parquet\"\n",
    "    logging.info(f\"DATA_PATH: {DATA_PATH}\")\n",
    "    \n",
    "    fs = fsspec.filesystem('gs')\n",
    "    \n",
    "    file_list = fs.glob(DATA_PATH)\n",
    "        # os.path.join(data_path, '*.parquet')\n",
    "    # )\n",
    "\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError('Parquet file(s) not found')\n",
    "\n",
    "    file_list = [os.path.join('gs://', i) for i in file_list]\n",
    "    \n",
    "    logging.info(f\"Number of files: {len(file_list)}\")\n",
    "\n",
    "    # return nvt.Dataset(f\"{bucket_name}/{data_prefix}/0000000000**.snappy.parquet\", part_mem_fraction=frac_size)\n",
    "    return nvt.Dataset(\n",
    "        file_list,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size\n",
    "  )\n",
    "\n",
    "def save_dataset(\n",
    "    dataset,\n",
    "    output_path,\n",
    "    output_files,\n",
    "    # categorical_cols,\n",
    "    # continuous_cols,\n",
    "    shuffle=None,\n",
    "):\n",
    "    \"\"\"Save dataset to parquet files to path.\"\"\"\n",
    "    categorical_cols=CAT\n",
    "    continuous_cols=CONT\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in categorical_cols:\n",
    "        dict_dtypes[col] = np.int32\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        dict_dtypes[col] = np.float64\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path=output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files,\n",
    "        dtypes=dict_dtypes,\n",
    "        cats=categorical_cols,\n",
    "        conts=continuous_cols,\n",
    "    )\n",
    "\n",
    "# # =============================================\n",
    "# #            Workflow\n",
    "# # =============================================\n",
    "# def create_nvt_workflow():\n",
    "#     '''\n",
    "#     Create a nvt.Workflow definition with transformation all the steps\n",
    "#     '''\n",
    "#     item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "#     playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "#     item_features_cat = ['artist_name_can',\n",
    "#             'track_name_can',\n",
    "#             'artist_genres_can',\n",
    "#         ]\n",
    "\n",
    "#     item_features_cont = [\n",
    "#             'duration_ms_can',\n",
    "#             'track_pop_can',\n",
    "#             'artist_pop_can',\n",
    "#             'artist_followers_can',\n",
    "#         ]\n",
    "\n",
    "#     playlist_features_cat = [\n",
    "#             'description_pl',\n",
    "#             'name',\n",
    "#             'collaborative',\n",
    "#         ]\n",
    "\n",
    "#     playlist_features_cont = [\n",
    "#             'duration_ms_seed_pl',\n",
    "#             'n_songs_pl',\n",
    "#             'num_artists_pl',\n",
    "#             'num_albums_pl',\n",
    "#         ]\n",
    "\n",
    "#     seq_feats_cat = [\n",
    "#             'artist_name_pl',\n",
    "#             'track_uri_pl',\n",
    "#             'track_name_pl',\n",
    "#             'album_name_pl',\n",
    "#             'artist_genres_pl',\n",
    "#         ]\n",
    "\n",
    "#     CAT = playlist_features_cat + item_features_cat\n",
    "#     CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "#     item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "#     item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "#     playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "#     playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "#     playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    \n",
    "#     # define a workflow\n",
    "#     output = playlist_id + item_id \\\n",
    "#     + item_feature_cat_node \\\n",
    "#     + item_feature_cont_node \\\n",
    "#     + playlist_feature_cat_node \\\n",
    "#     + playlist_feature_cont_node \\\n",
    "#     + playlist_feature_cat_seq_node \n",
    "\n",
    "#     workflow = nvt.Workflow(output)\n",
    "    \n",
    "#     return workflow\n",
    "\n",
    "# =============================================\n",
    "#            Workflow\n",
    "# =============================================\n",
    "def create_nvt_workflow():\n",
    "    '''\n",
    "    Create a nvt.Workflow definition with transformation all the steps\n",
    "    '''\n",
    "    item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "    playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "    item_features_cat = [\n",
    "        'artist_name_can',\n",
    "        'track_name_can',\n",
    "        'album_name_can',\n",
    "        'artist_genres_can',\n",
    "        'track_key_can',\n",
    "        'track_mode_can',\n",
    "        'track_time_signature_can'\n",
    "    ]\n",
    "\n",
    "    item_features_cont = [\n",
    "        'duration_ms_can',\n",
    "        'track_pop_can',\n",
    "        'artist_pop_can',\n",
    "        'artist_followers_can',\n",
    "    ]\n",
    "\n",
    "    playlist_features_cat = [\n",
    "        # 'description_pl',\n",
    "        'pl_name_src',\n",
    "        'pl_collaborative_src',\n",
    "    ]\n",
    "\n",
    "    playlist_features_cont = [\n",
    "        'pl_duration_ms_new',\n",
    "        'num_pl_songs_new',\n",
    "        'num_pl_artists_new',\n",
    "        'num_pl_albums_new',\n",
    "    ]\n",
    "\n",
    "    seq_feats_cat = [\n",
    "        'artist_name_pl',\n",
    "        'track_uri_pl',\n",
    "        'track_name_pl',\n",
    "        'album_name_pl',\n",
    "        'artist_genres_pl',\n",
    "        'track_key_pl',\n",
    "        'track_mode_pl',\n",
    "        'track_time_signature_pl'\n",
    "    ]\n",
    "    \n",
    "    # seq_feats_cont = [\n",
    "    #     'duration_ms_songs_pl',\n",
    "    #     'artist_pop_pl',\n",
    "    #     'artists_followers_pl',\n",
    "    #     'track_pop_pl',\n",
    "    #     'track_danceability_pl',\n",
    "    #     'track_energy_pl',\n",
    "    #     'track_loudness_pl',\n",
    "    #     'track_acousticness_pl',\n",
    "    #     'track_instrumentalness_pl',\n",
    "    #     'track_liveness',\n",
    "    #     'track_valence_pl',\n",
    "    #     'track_tempo_pl',\n",
    "    #     'track_speechiness_pl',\n",
    "    # ]\n",
    "\n",
    "    CAT = playlist_features_cat + item_features_cat\n",
    "    CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "    item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "    item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "    playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "    playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "    playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    \n",
    "    # define a workflow\n",
    "    output = playlist_id + item_id \\\n",
    "    + item_feature_cat_node \\\n",
    "    + item_feature_cont_node \\\n",
    "    + playlist_feature_cat_node \\\n",
    "    + playlist_feature_cont_node \\\n",
    "    + playlist_feature_cat_seq_node \n",
    "\n",
    "    workflow = nvt.Workflow(output)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# =============================================\n",
    "#            Create Parquet Dataset \n",
    "# =============================================\n",
    "\n",
    "def create_parquet_dataset_definition(\n",
    "    # data_paths,\n",
    "    # recursive,\n",
    "    # col_dtypes,\n",
    "    frac_size,\n",
    "    bucket_name,\n",
    "    data_prefix,\n",
    "    file_pattern,\n",
    "    # sep='\\t'\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    DATASET_DEFINITION = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\"  # 0000000000**.snappy.parquet\"\n",
    "    \n",
    "    logging.info(f'DATASET_DEFINITION: {DATASET_DEFINITION}')\n",
    "    \n",
    "    fs = fsspec.filesystem('gs')\n",
    "    file_list = fs.glob(DATASET_DEFINITION)\n",
    "\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError('Parquet file(s) not found')\n",
    "\n",
    "    file_list = [os.path.join('gs://', i) for i in file_list]\n",
    "    logging.info(f\"Number of files: {len(file_list)}\")\n",
    "    \n",
    "    return nvt.Dataset(f\"{DATASET_DEFINITION}\", engine='parquet', part_mem_fraction=frac_size)\n",
    "\n",
    "\n",
    "def convert_definition_to_parquet(\n",
    "    output_path,\n",
    "    dataset,\n",
    "    output_files,\n",
    "    shuffle=None\n",
    "):\n",
    "    \"\"\"Convert Parquet files to parquet and write to GCS.\"\"\"\n",
    "    if shuffle == 'None':\n",
    "        shuffle = None\n",
    "    else:\n",
    "        try:\n",
    "            shuffle = getattr(Shuffle, shuffle)\n",
    "        except:\n",
    "            print('Shuffle method not available. Using default.')\n",
    "            shuffle = None\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Create nv-tabular definition\n",
    "# =============================================\n",
    "def main_convert(args):\n",
    "    \n",
    "    logging.info('Beginning main-convert from preprocess_task.py...')\n",
    "    logging.info(f'args.output_path: {args.output_path}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating parquet dataset definition')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        # data_paths=args.parq_data_path,\n",
    "        # recursive=False,\n",
    "        bucket_name=args.bucket_name,     # 'spotify-builtin-2t', # TODO: parameterize\n",
    "        data_prefix=args.data_prefix,     # 'train', # TODO: JT check\n",
    "        frac_size=args.frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "\n",
    "    logging.info('Converting definition to Parquet')\n",
    "    convert_definition_to_parquet(\n",
    "        args.output_path,\n",
    "        dataset,\n",
    "        args.output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Analyse Dataset \n",
    "# =============================================\n",
    "def main_analyze(args):\n",
    "    \n",
    "    logging.info('Beginning main-analyze from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating Parquet dataset')\n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=file_pattern #\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "  \n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "  \n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(args.output_path)\n",
    "    \n",
    "# =============================================\n",
    "#            Transform Dataset \n",
    "# =============================================\n",
    "def main_transform(args):\n",
    "    \n",
    "    logging.info('Beginning main-transform from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit,\n",
    "    )\n",
    "\n",
    "    # nvt_workflow = create_nvt_workflow()\n",
    "    nvt_workflow = nvt.Workflow.load(args.workflow_path, client)\n",
    "\n",
    "    # dataset = create_parquet_nvt_dataset(\n",
    "    #     args.parquet_data_path, \n",
    "    #     frac_size=args.frac_size)\n",
    "    \n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=file_pattern #\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    transformed_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info('Saving transformed dataset')\n",
    "    save_dataset(\n",
    "        transformed_dataset,\n",
    "        output_path=args.output_path,\n",
    "        output_files=args.output_files,\n",
    "        # categorical_cols=CAT,\n",
    "        # continuous_cols=CONT,\n",
    "        shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            args\n",
    "# =============================================\n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "  \n",
    "    parser.add_argument(\n",
    "        '--task',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bucket_name',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parquet_data_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parq_data_path',\n",
    "        required=False,\n",
    "        nargs='+'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_files',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_workers',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--frac_size',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--memory_limit',\n",
    "        type=int,\n",
    "        required=False,\n",
    "        default=200_000_000_000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_limit_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_pool_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.90\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    start_time = time.time()\n",
    "    logging.info('Timing task')\n",
    "\n",
    "    if parsed_args.task == 'transform':\n",
    "        main_transform(parsed_args)\n",
    "    elif parsed_args.task == 'analyze':\n",
    "        main_analyze(parsed_args)\n",
    "    elif parsed_args.task == 'convert':\n",
    "        main_convert(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Task completed. Elapsed time: %s', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df76071-ea5a-479b-bd27-0135cd3c2fdd",
   "metadata": {},
   "source": [
    "## pipe components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5eaef21-5eaa-4f4a-b1f4-d2d0392cefa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/pipe_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/pipe_components.py\n",
    "\"\"\"KFP components.\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "from . import config\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Dataset\n",
    "from kfp.v2.dsl import Input\n",
    "from kfp.v2.dsl import Model\n",
    "from kfp.v2.dsl import Output\n",
    "\n",
    "# =============================================\n",
    "#            convert_to_parquet_op\n",
    "# =============================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def convert_parquet_op(\n",
    "    output_dataset: Output[Dataset],\n",
    "    bucket_name: str,\n",
    "    data_prefix: str,\n",
    "    file_pattern: str,\n",
    "    output_path_defined_dir: str,\n",
    "    # data_dir_pattern: str,\n",
    "    # data_paths: list,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: Optional[str] = None,\n",
    "    recursive: Optional[bool] = False,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 200_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to create NVTabular definition.\n",
    "    \n",
    "    Args:\n",
    "    output_dataset: Output[Dataset]\n",
    "      Output metadata with references to the converted CSV files in GCS\n",
    "      and the split name.The path to the files are in GCS fuse format:\n",
    "      /gcs/<bucket name>/path/to/file\n",
    "    bucket: gcs bucket holding train & valid data\n",
    "    data_path_prefix: file path to GCS blobl object (e.g., gs://...data/path/prefix.../blob.xxx)\n",
    "    data_paths: list\n",
    "    split: str\n",
    "      Split name of the dataset. Example: train or valid\n",
    "    shuffle: str\n",
    "      How to shuffle the converted CSV, default to None. Options:\n",
    "        PER_PARTITION\n",
    "        PER_WORKER\n",
    "        FULL\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    memory_limit: Optional[int] = 200_000_000_000\n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    #            import packages\n",
    "    # =========================================================\n",
    "    import os\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_parquet_dataset_definition,\n",
    "        convert_definition_to_parquet,\n",
    "        # get_criteo_col_dtypes,\n",
    "    )\n",
    "    \n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "    logging.info('Base path in %s', output_dataset.path)\n",
    "    # =========================================================\n",
    "    #            Define data paths\n",
    "    # =========================================================\n",
    "    logging.info(f'bucket_name: {bucket_name}')\n",
    "    logging.info(f'data_prefix: {data_prefix}')\n",
    "    \n",
    "    # Write metadata\n",
    "    output_dataset.metadata['split'] = split\n",
    "\n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating dataset definition from: {data_path_prefix}')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        bucket_name=bucket_name,\n",
    "        data_prefix=data_prefix,\n",
    "        frac_size=frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Converting Definition to Parquet; {output_dataset.uri}')\n",
    "    logging.info(f'Parquet Definition Output Path: ; {output_path_defined_dir}/{split}')\n",
    "    convert_definition_to_parquet(\n",
    "        output_path=f'{output_path_defined_dir}/{split}', # output_dataset.uri,\n",
    "        dataset=dataset,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "# =========================================================\n",
    "#            analyze_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def analyze_dataset_op(\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    workflow: Output[Artifact],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    n_workers: int,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 200_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to generate statistics from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    parquet_dataset: List of strings\n",
    "      Input metadata with references to the train and valid converted\n",
    "      datasets in GCS and the split name.\n",
    "    workflow: Output[Artifact]\n",
    "      Output metadata with the path to the fitted workflow artifacts\n",
    "      (statistics).\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    '''\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "  \n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_nvt_workflow,\n",
    "    )\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    create_cluster(\n",
    "      n_workers=n_workers,\n",
    "      device_limit_frac=device_limit_frac,\n",
    "      device_pool_frac=device_pool_frac,\n",
    "      memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating Parquet dataset:{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset output_path_defined_dir: {output_path_defined_dir}/train')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/train', # TODO: JT Check \"train\"    # parquet_dataset.uri,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "\n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "\n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "    \n",
    "# =========================================================\n",
    "#            transform_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def transform_dataset_op(\n",
    "    workflow: Input[Artifact],\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    transformed_dataset: Output[Dataset],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    version: str,\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    app: str,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: str = None,\n",
    "    device_limit_frac: float = 0.6,\n",
    "    device_pool_frac: float = 0.9,\n",
    "    frac_size: float = 0.10,\n",
    "    memory_limit: int = 200_000_000_000\n",
    "):\n",
    "    \"\"\"Component to transform a dataset according to the workflow definitions.\n",
    "    Args:\n",
    "        workflow: Input[Artifact]\n",
    "        Input metadata with the path to the fitted_workflow\n",
    "        parquet_dataset: Input[Dataset]\n",
    "              Location of the converted dataset in GCS and split name\n",
    "        transformed_dataset: Output[Dataset]\n",
    "        Split name of the transformed dataset.\n",
    "        shuffle: str\n",
    "            How to shuffle the converted CSV, default to None. Options:\n",
    "                PER_PARTITION\n",
    "                PER_WORKER\n",
    "                FULL\n",
    "    device_limit_frac: float = 0.6\n",
    "    device_pool_frac: float = 0.9\n",
    "    frac_size: float = 0.10\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "    from merlin.schema import Tags\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        save_dataset,\n",
    "    )\n",
    "    def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "        client = storage.Client()\n",
    "        blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "        blob.bucket._client = client\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "    def _read_blob_gcs(bucket_name, source_blob_name, destination_filename):\n",
    "        \"\"\"Downloads a file from GCS to local directory\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_filename)\n",
    "        \n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    transformed_dataset.metadata['split'] = split\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "   # logging.info(f'Creating Parquet dataset:gs://{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset:{output_path_defined_dir}/{split}')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/{split}', #f'gs://{parquet_dataset.uri}',\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "    \n",
    "    logging.info('Loading Workflow')\n",
    "    nvt_workflow = nvt.Workflow.load(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    trans_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info(f'transformed_dataset.uri: {transformed_dataset.uri}')\n",
    "    logging.info(f'Saving transformed dataset: {output_path_transformed_dir}/{split}')\n",
    "    save_dataset(\n",
    "        dataset=trans_dataset,\n",
    "        output_path=f'{output_path_transformed_dir}/{split}', # transformed_dataset.uri,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    logging.info(f'transformed_dataset saved!')\n",
    "    logging.info(f'transformed_dataset.path: {transformed_dataset.path}')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        read and upload files\n",
    "    # =========================================================\n",
    "    '''\n",
    "    nv-tabular creates a txt file with all `gs://` paths\n",
    "    create a copy that replaces `gs://` with `/gcs/`\n",
    "    '''\n",
    "    logging.info('Generating file list for training...')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        Saving cardinalities\n",
    "    # =========================================================\n",
    "    logging.info('Saving cardinalities')\n",
    "    \n",
    "    cols_schemas = nvt_workflow.output_schema.select_by_tag(Tags.CATEGORICAL)\n",
    "    cols_names = cols_schemas.column_names\n",
    "\n",
    "    cards = []\n",
    "    for c in cols_names:\n",
    "        col = cols_schemas.get(c)\n",
    "        cards.append(col.properties['embedding_sizes']['cardinality'])\n",
    "\n",
    "    transformed_dataset.metadata['cardinalities'] = cards\n",
    "    # transformed_dataset.metadata['dataset_gcs_uri'] = gcs_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729926e-c1f0-4389-995b-bc6db237c734",
   "metadata": {},
   "source": [
    "## preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88103c26-3f43-4835-b2ff-8a26245d6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/preproc_pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/preproc_pipelines.py\n",
    "\"\"\"Preprocessing pipelines.\"\"\"\n",
    "\n",
    "from . import pipe_components\n",
    "from . import config\n",
    "from kfp.v2 import dsl\n",
    "import os\n",
    "\n",
    "GKE_ACCELERATOR_KEY = 'cloud.google.com/gke-accelerator'\n",
    "\n",
    "# TODO: parametrize and fix config file \n",
    "# BUCKET_parquet = 'spotify-builtin-2t'\n",
    "# BUCKET = 'spotify-merlin-v1'\n",
    "# VERSION = 'v32-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-preprocessing-{APP}-{VERSION}'\n",
    "# WORKSPACE = f'gs://{config.BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvtabular-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'{config.PREPROCESS_PARQUET_PIPELINE_NAME}', \n",
    "    pipeline_root=f'{config.PREPROCESS_PARQUET_PIPELINE_ROOT}'\n",
    ")\n",
    "def preprocessing_parquet(\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    # train_pattern: str,\n",
    "    # valid_pattern: str,\n",
    "    train_prefix: str,\n",
    "    valid_prefix: str,\n",
    "    file_pattern: str,\n",
    "    num_output_files_train: int,\n",
    "    num_output_files_valid: int,\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    shuffle: str,\n",
    "    version: str,\n",
    "    app: str,\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Pipeline to preprocess parquet files in GCS.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    # TODO: extract from BQ to parquet \n",
    "    # =========================================================\n",
    "    \n",
    "    \n",
    "    # =========================================================\n",
    "    #             Convert from parquet to def \n",
    "    # =========================================================\n",
    "    # config.BUCKET_NAME = 'spotify-builtin-2t' # 'spotify-merlin-v1' # TODO: parameterize\n",
    "    \n",
    "    parquet_to_def_train = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=train_prefix,\n",
    "            # data_dir_pattern=train_pattern,\n",
    "            split='train',\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_train.set_display_name('Convert training split')\n",
    "    parquet_to_def_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_train.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # === Convert eval dataset from CSV to Parquet\n",
    "    parquet_to_def_valid = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=valid_prefix,\n",
    "            # data_dir_pattern=valid_pattern,\n",
    "            split='valid',\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_valid.set_display_name('Convert validation split')\n",
    "    parquet_to_def_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_valid.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Analyse train dataset \n",
    "    # =========================================================\n",
    "    \n",
    "    # === Analyze train data split\n",
    "    analyze_dataset = (\n",
    "        pipe_components.analyze_dataset_op(\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir\n",
    "        )\n",
    "    )\n",
    "    analyze_dataset.set_display_name('Analyze Dataset')\n",
    "    analyze_dataset.set_cpu_limit(config.CPU_LIMIT)\n",
    "    # analyze_dataset.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    analyze_dataset.set_gpu_limit(config.GPU_LIMIT)\n",
    "    analyze_dataset.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    analyze_dataset.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Transform train split \n",
    "    # =========================================================\n",
    "\n",
    "    # === Transform train data split\n",
    "    transform_train = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='train',\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_train.set_display_name('Transform train split')\n",
    "    transform_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    # transform_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_train.set_caching_options(enable_caching=True)\n",
    "\n",
    "    # =========================================================\n",
    "    #     Transform valid split\n",
    "    # =========================================================\n",
    "    \n",
    "    transform_valid = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='valid',\n",
    "            parquet_dataset=parquet_to_def_valid.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_valid.set_display_name('Transform valid split')\n",
    "    transform_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    transform_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_valid.set_caching_options(enable_caching=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16211a8f-23a4-4bf5-a8d3-ccc99b9ea669",
   "metadata": {},
   "source": [
    "### Set Pipeline Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534f199-11b3-4e6e-be16-44fc6a7c7f38",
   "metadata": {},
   "source": [
    "* see GPU config types [here](https://cloud.google.com/compute/docs/gpus#a100-gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa92776-dc2d-4a86-8078-0119140082a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_source: spotify-million-playlist-dataset\n",
      "BUCKET_destin: jt-merlin-scaling\n",
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "\n",
      "GPU_LIMIT: 2\n",
      "GPU_TYPE: NVIDIA_TESLA_A100\n",
      "CPU_LIMIT: 24\n",
      "MEMORY_LIMIT: 170G\n",
      "INSTANCE_TYPE: a2-highgpu-2g\n",
      "\n",
      "VERSION: latest-16\n",
      "APP: spotify\n",
      "MODEL_DISPLAY_NAME: nvt-last5-latest-16\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-latest-16\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-latest-16\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-latest-16/nvt-parquet-latest-16\n",
      "\n",
      "IMAGE_NAME: nvt-preprocessing\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "DOCKERNAME: nvtabular\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "#           storage\n",
    "# =============================================\n",
    "# BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_source ='spotify-million-playlist-dataset'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "print(f\"BUCKET_source: {BUCKET_source}\")\n",
    "print(f\"BUCKET_destin: {BUCKET_destin}\")\n",
    "print(f\"TRAIN_SRC_DIR: {TRAIN_SRC_DIR}\")\n",
    "print(f\"VALID_SRC_DIR: {VALID_SRC_DIR}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           accelerators\n",
    "# =============================================\n",
    "# Instance configuration\n",
    "# GPU_LIMIT = '4'\n",
    "# GPU_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# CPU_LIMIT = '64'\n",
    "# MEMORY_LIMIT = '624G'\n",
    "# INSTANCE_TYPE = \"n1-highmem-64\"\n",
    "\n",
    "# Instance configuration\n",
    "# GPU_LIMIT = '2'                   # 1\n",
    "# GPU_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# CPU_LIMIT = '24'                  # '64' '96'\n",
    "# MEMORY_LIMIT = '170G'              #'624G' | 680\n",
    "# INSTANCE_TYPE = \"a2-highgpu-2g\"\n",
    "\n",
    "GPU_LIMIT = '4'                   # 1\n",
    "GPU_TYPE = 'NVIDIA_TESLA_A100'\n",
    "CPU_LIMIT = '48'                  # '64' '96'\n",
    "MEMORY_LIMIT = '340G'              #'624G' | 680\n",
    "INSTANCE_TYPE = \"a2-highgpu-4g\"\n",
    "\n",
    "\n",
    "https://cloud.google.com/compute/docs/gpus#a100-gpus\n",
    "\n",
    "print(f\"GPU_LIMIT: {GPU_LIMIT}\")\n",
    "print(f\"GPU_TYPE: {GPU_TYPE}\")\n",
    "print(f\"CPU_LIMIT: {CPU_LIMIT}\")\n",
    "print(f\"MEMORY_LIMIT: {MEMORY_LIMIT}\")\n",
    "print(f\"INSTANCE_TYPE: {INSTANCE_TYPE}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           pipelines\n",
    "# =============================================\n",
    "VERSION = 'latest-16'\n",
    "APP = 'spotify'\n",
    "MODEL_DISPLAY_NAME = f'nvt-last5-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "# Pipeline definitions\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-{VERSION}'\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           custom image\n",
    "# =============================================\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt-preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = f'nvtabular' # 150\n",
    "\n",
    "print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"DOCKERNAME: {DOCKERNAME}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381dcfc7-6f7c-4427-8859-8375d92be415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/home/jupyter/spotify-merlin')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1776608-a23c-4aa5-acc3-2b965be1b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/config.py\n",
    "\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "#           Cloud Storage Directorires\n",
    "# =============================================\n",
    "# BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_source ='spotify-million-playlist-dataset'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "# =============================================\n",
    "#           Setup\n",
    "# =============================================\n",
    "VERSION = 'latest-16'\n",
    "APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "PROJECT_ID = \"hybrid-vertex\"\n",
    "REGION = \"us-central1\"\n",
    "VERTEX_SA = f\"vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# =============================================\n",
    "#           Artifacts\n",
    "# =============================================\n",
    "# MODEL_DISPLAY_NAME = f\"nvt-last5-{VERSION}\"\n",
    "# WORKSPACE = f\"gs://jt-merlin-scaling/nvt-last5-{VERSION}\"\n",
    "MODEL_DISPLAY_NAME = f'nvt-last5-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "NVT_IMAGE_URI = \"gcr.io/hybrid-vertex/nvt-preprocessing\"\n",
    "\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt-preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "# =============================================\n",
    "#           Pipeline Configs\n",
    "# =============================================\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f\"nvt-parquet-{VERSION}\"\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = f\"gs://jt-merlin-scaling/{MODEL_DISPLAY_NAME}/{PREPROCESS_PARQUET_PIPELINE_NAME}\"\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-{VERSION}'\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "# 4 tesla 4s\n",
    "# INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"n1-highmem-64\")\n",
    "# CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"64\")\n",
    "# MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"624G\")\n",
    "# GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"4\")\n",
    "# GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_T4\")\n",
    "\n",
    "# 2 A100s\n",
    "# INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"a2-highgpu-2g\")\n",
    "# CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"24\")\n",
    "# MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"170G\")\n",
    "# GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"2\")\n",
    "# GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_A100\")\n",
    "\n",
    "# 4 A100s\n",
    "INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"a2-highgpu-4g\")\n",
    "CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"48\")\n",
    "MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"340G\")\n",
    "GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"4\")\n",
    "GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_A100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f0b96-b8df-4893-8bfc-83a508db9f4f",
   "metadata": {},
   "source": [
    "### check config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40adb058-a744-47fd-b68e-c224014d32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "VERSION: latest-16\n",
      "APP: spotify\n",
      "PROJECT_ID: hybrid-vertex\n",
      "REGION: us-central1\n",
      "VERTEX_SA: vertex-sa@hybrid-vertex.iam.gserviceaccount.com\n",
      "MODEL_DISPLAY_NAME: nvt-last5-latest-16\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-latest-16\n",
      "NVT_IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "IMAGE_NAME: nvt-preprocessing\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-latest-16\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-latest-16/nvt-parquet-latest-16\n",
      "INSTANCE_TYPE: a2-highgpu-4g\n",
      "CPU_LIMIT: 48\n",
      "MEMORY_LIMIT: 340G\n",
      "GPU_LIMIT: 4\n",
      "GPU_TYPE: NVIDIA_TESLA_A100\n"
     ]
    }
   ],
   "source": [
    "from src.process_pipes import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9e4a9-4d2b-46e7-a51b-65b04ee6bcd6",
   "metadata": {},
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31d0e345-d566-4162-8852-2f06e2a74d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed247c0c-94a1-491c-b7df-c778e49096d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/requirements.txt\n",
    "# google-cloud-bigquery\n",
    "# gcsfs\n",
    "# google-cloud-aiplatform\n",
    "# kfp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "981f2ba9-4e39-4873-96f9-81815d67784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.nvtabular\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# RUN pip install -U pip\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform kfp \n",
    "\n",
    "COPY preprocessor/* ./\n",
    "\n",
    "ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c7a71a5-e4d6-43b5-8c5e-32bbc3ffc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2febc882-7982-4956-9700-aa3e9d2d9138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41411e43-b5af-4148-ae51-835d17278cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/archive/*\n",
    "/imgs/*\n",
    "/mm_src/*\n",
    "/src/serving/*\n",
    "/src/train_pipes/*\n",
    "src/trainer/*\n",
    "/test_app/*\n",
    "/local_workflow/\n",
    "README.md\n",
    "*.pkl\n",
    "*.png\n",
    "*.ipynb\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "*cpython-37.pyc\n",
    "pip_freeze.txt\n",
    "custom_container_pipeline_spec.json\n",
    "# *.json\n",
    "src/Dockerfile.triton-cpr\n",
    "src/Dockerfile.merlin-retriever\n",
    "src/Dockerfile.merlintf-22_12_v4\n",
    "src/Dockerfile.nvt-133\n",
    "Dockerfile\n",
    "src/Dockerfile.mm-query-serve\n",
    "src/Dockerfile.train\n",
    "nvt-parquet-full-1a100.json\n",
    "nvt-parquet-latest-12.json\n",
    "nvt-parquet-full-4t4.json\n",
    "nvt-parquet-full-2a100.json\n",
    "custom_pipeline_spec.json\n",
    "spotipy_secret_creds.py\n",
    "sp_utils.py\n",
    ".gitignore\n",
    ".cache\n",
    "utils/train_utils.py\n",
    "src/process_pipes/*\n",
    "nvt-parquet-latest-13.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "965bbf34-5c56-445c-bf32-87744e94e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvt-parquet-latest-15.json\n",
      "nvt-parquet-latest-14.json\n",
      "src/cloudbuild.yaml\n",
      "src/Dockerfile.nvtabular\n",
      "src/preprocessor/preprocess_task.py\n",
      "src/preprocessor/__init__.py\n"
     ]
    }
   ],
   "source": [
    "!gcloud meta list-files-for-upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074c1600-7874-44a8-b1db-850882129e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_NAME: nvt-preprocessing\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "DOCKERNAME: nvtabular\n",
      "FILE_LOCATION: ./src\n",
      "MACHINE_TYPE: e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'\n",
    "\n",
    "print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "print(f\"FILE_LOCATION: {FILE_LOCATION}\")\n",
    "print(f\"MACHINE_TYPE: {MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd86503b-0c58-4b6c-ab69-e4f088fa038b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 6 file(s) totalling 108.8 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1679459207.072481-3fdaeee4e0f64f8a97b3df4a808f75ed.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/185a0bb6-cde0-43df-b597-e6326f462215].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/185a0bb6-cde0-43df-b597-e6326f462215?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"185a0bb6-cde0-43df-b597-e6326f462215\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1679459207.072481-3fdaeee4e0f64f8a97b3df4a808f75ed.tgz#1679459207787683\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1679459207.072481-3fdaeee4e0f64f8a97b3df4a808f75ed.tgz#1679459207787683...\n",
      "/ [1 files][ 12.1 KiB/ 12.1 KiB]                                                \n",
      "Operation completed over 1 objects/12.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  25.09kB\n",
      "Step 1/6 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      "22.12: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "eaead16dc43b: Pulling fs layer\n",
      "d86e6ecee9ab: Pulling fs layer\n",
      "6b08e0981273: Pulling fs layer\n",
      "54de03fa67ca: Pulling fs layer\n",
      "d0067f8e4744: Pulling fs layer\n",
      "9e8473502f86: Pulling fs layer\n",
      "62cbcf4cbe13: Pulling fs layer\n",
      "9054d495b912: Pulling fs layer\n",
      "363a6cd12433: Pulling fs layer\n",
      "69633d945bde: Pulling fs layer\n",
      "030b5d54675a: Pulling fs layer\n",
      "282a25844090: Pulling fs layer\n",
      "51caa25c21ff: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "77bc21e94f24: Pulling fs layer\n",
      "784e1b6fd612: Pulling fs layer\n",
      "af1ee39433f4: Pulling fs layer\n",
      "52321cba559e: Pulling fs layer\n",
      "af00054fc370: Pulling fs layer\n",
      "0b6fc99b1680: Pulling fs layer\n",
      "6cee73498bf6: Pulling fs layer\n",
      "8b4cfd996372: Pulling fs layer\n",
      "b090c9ed50ff: Pulling fs layer\n",
      "0a8adb3dcf22: Pulling fs layer\n",
      "6e88a7f6f15d: Pulling fs layer\n",
      "00f5ed83fb5b: Pulling fs layer\n",
      "e518f4c1fb4d: Pulling fs layer\n",
      "8b6afde98e21: Pulling fs layer\n",
      "dcdde75d70d4: Pulling fs layer\n",
      "54de03fa67ca: Waiting\n",
      "c131c32a822c: Pulling fs layer\n",
      "7ef98a0bb031: Pulling fs layer\n",
      "7b2bf50cb482: Pulling fs layer\n",
      "7dd53e86e25e: Pulling fs layer\n",
      "d0067f8e4744: Waiting\n",
      "228e876a170d: Pulling fs layer\n",
      "2b8ce232b0f5: Pulling fs layer\n",
      "9e8473502f86: Waiting\n",
      "6acab9f9b6c9: Pulling fs layer\n",
      "62cbcf4cbe13: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "49738298fc0b: Pulling fs layer\n",
      "030b5d54675a: Waiting\n",
      "450e691b8934: Pulling fs layer\n",
      "77bc21e94f24: Waiting\n",
      "c11b1ed1ee46: Pulling fs layer\n",
      "282a25844090: Waiting\n",
      "67e193f6923c: Pulling fs layer\n",
      "784e1b6fd612: Waiting\n",
      "9054d495b912: Waiting\n",
      "51caa25c21ff: Waiting\n",
      "f3a3e4335302: Pulling fs layer\n",
      "af1ee39433f4: Waiting\n",
      "31514946c388: Pulling fs layer\n",
      "52321cba559e: Waiting\n",
      "4045938dbcf0: Pulling fs layer\n",
      "363a6cd12433: Waiting\n",
      "6cee73498bf6: Waiting\n",
      "ec3ab3614609: Pulling fs layer\n",
      "69633d945bde: Waiting\n",
      "af00054fc370: Waiting\n",
      "8b4cfd996372: Waiting\n",
      "d59ff5de4fe3: Pulling fs layer\n",
      "0b6fc99b1680: Waiting\n",
      "c5e6195fcb5a: Pulling fs layer\n",
      "b090c9ed50ff: Waiting\n",
      "b64eb74ea0f6: Pulling fs layer\n",
      "fde2f9173aa1: Pulling fs layer\n",
      "0a8adb3dcf22: Waiting\n",
      "8b6afde98e21: Waiting\n",
      "05c48a632d89: Pulling fs layer\n",
      "7ef98a0bb031: Waiting\n",
      "45643bc35dd6: Pulling fs layer\n",
      "6e88a7f6f15d: Waiting\n",
      "dac08d81643b: Pulling fs layer\n",
      "7b2bf50cb482: Waiting\n",
      "dcdde75d70d4: Waiting\n",
      "753e545517ec: Pulling fs layer\n",
      "00f5ed83fb5b: Waiting\n",
      "c131c32a822c: Waiting\n",
      "e518f4c1fb4d: Waiting\n",
      "7dd53e86e25e: Waiting\n",
      "59fa5f6703c7: Pulling fs layer\n",
      "228e876a170d: Waiting\n",
      "5251fc0e145f: Pulling fs layer\n",
      "c5e6195fcb5a: Waiting\n",
      "c11b1ed1ee46: Waiting\n",
      "bdd807f81520: Pulling fs layer\n",
      "2b8ce232b0f5: Waiting\n",
      "b64eb74ea0f6: Waiting\n",
      "cda9dd232241: Pulling fs layer\n",
      "67e193f6923c: Waiting\n",
      "6acab9f9b6c9: Waiting\n",
      "c1c0778801ed: Pulling fs layer\n",
      "fde2f9173aa1: Waiting\n",
      "f3a3e4335302: Waiting\n",
      "d2ac586dac26: Pulling fs layer\n",
      "49738298fc0b: Waiting\n",
      "31514946c388: Waiting\n",
      "c82a34ea3155: Pulling fs layer\n",
      "05c48a632d89: Waiting\n",
      "450e691b8934: Waiting\n",
      "07391a0f51be: Pulling fs layer\n",
      "4045938dbcf0: Waiting\n",
      "45643bc35dd6: Waiting\n",
      "7e85ce497f3c: Pulling fs layer\n",
      "d59ff5de4fe3: Waiting\n",
      "732c433b96b6: Pulling fs layer\n",
      "ec3ab3614609: Waiting\n",
      "dac08d81643b: Waiting\n",
      "de409ef6b855: Pulling fs layer\n",
      "36a7c7813429: Pulling fs layer\n",
      "59fa5f6703c7: Waiting\n",
      "cc688c082c64: Pulling fs layer\n",
      "c82a34ea3155: Waiting\n",
      "d2ac586dac26: Waiting\n",
      "92681808abed: Pulling fs layer\n",
      "5251fc0e145f: Waiting\n",
      "8b7d3b755746: Pulling fs layer\n",
      "07391a0f51be: Waiting\n",
      "bdd807f81520: Waiting\n",
      "07973a04f293: Pulling fs layer\n",
      "732c433b96b6: Waiting\n",
      "7e85ce497f3c: Waiting\n",
      "84b159fc1d60: Pulling fs layer\n",
      "cda9dd232241: Waiting\n",
      "161c50cb1ab8: Pulling fs layer\n",
      "de409ef6b855: Waiting\n",
      "c1c0778801ed: Waiting\n",
      "3adfd786ac79: Pulling fs layer\n",
      "36a7c7813429: Waiting\n",
      "b687b9abe298: Pulling fs layer\n",
      "92681808abed: Waiting\n",
      "8b7d3b755746: Waiting\n",
      "4d55d43d5c97: Pulling fs layer\n",
      "cc688c082c64: Waiting\n",
      "2bda520db73d: Pulling fs layer\n",
      "f006e76f83a3: Pulling fs layer\n",
      "84b159fc1d60: Waiting\n",
      "07973a04f293: Waiting\n",
      "7b8995abb34f: Pulling fs layer\n",
      "70651e98f007: Pulling fs layer\n",
      "161c50cb1ab8: Waiting\n",
      "7e50d9a71c79: Pulling fs layer\n",
      "b687b9abe298: Waiting\n",
      "3adfd786ac79: Waiting\n",
      "fe9e4cebdbb5: Pulling fs layer\n",
      "2bda520db73d: Waiting\n",
      "4d55d43d5c97: Waiting\n",
      "0147547644c1: Pulling fs layer\n",
      "daffe4a11a5d: Pulling fs layer\n",
      "f006e76f83a3: Waiting\n",
      "7b8995abb34f: Waiting\n",
      "70651e98f007: Waiting\n",
      "7e50d9a71c79: Waiting\n",
      "fe9e4cebdbb5: Waiting\n",
      "0147547644c1: Waiting\n",
      "daffe4a11a5d: Waiting\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "54de03fa67ca: Verifying Checksum\n",
      "54de03fa67ca: Download complete\n",
      "d86e6ecee9ab: Verifying Checksum\n",
      "d86e6ecee9ab: Download complete\n",
      "9e8473502f86: Verifying Checksum\n",
      "9e8473502f86: Download complete\n",
      "6b08e0981273: Verifying Checksum\n",
      "6b08e0981273: Download complete\n",
      "62cbcf4cbe13: Download complete\n",
      "9054d495b912: Download complete\n",
      "363a6cd12433: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "030b5d54675a: Verifying Checksum\n",
      "030b5d54675a: Download complete\n",
      "69633d945bde: Verifying Checksum\n",
      "69633d945bde: Download complete\n",
      "282a25844090: Verifying Checksum\n",
      "282a25844090: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "77bc21e94f24: Verifying Checksum\n",
      "77bc21e94f24: Download complete\n",
      "784e1b6fd612: Download complete\n",
      "af1ee39433f4: Verifying Checksum\n",
      "af1ee39433f4: Download complete\n",
      "d86e6ecee9ab: Pull complete\n",
      "51caa25c21ff: Verifying Checksum\n",
      "51caa25c21ff: Download complete\n",
      "af00054fc370: Verifying Checksum\n",
      "af00054fc370: Download complete\n",
      "0b6fc99b1680: Verifying Checksum\n",
      "0b6fc99b1680: Download complete\n",
      "6cee73498bf6: Verifying Checksum\n",
      "6cee73498bf6: Download complete\n",
      "6b08e0981273: Pull complete\n",
      "54de03fa67ca: Pull complete\n",
      "52321cba559e: Verifying Checksum\n",
      "52321cba559e: Download complete\n",
      "b090c9ed50ff: Verifying Checksum\n",
      "b090c9ed50ff: Download complete\n",
      "0a8adb3dcf22: Verifying Checksum\n",
      "0a8adb3dcf22: Download complete\n",
      "6e88a7f6f15d: Verifying Checksum\n",
      "6e88a7f6f15d: Download complete\n",
      "00f5ed83fb5b: Verifying Checksum\n",
      "00f5ed83fb5b: Download complete\n",
      "e518f4c1fb4d: Verifying Checksum\n",
      "e518f4c1fb4d: Download complete\n",
      "8b6afde98e21: Verifying Checksum\n",
      "8b6afde98e21: Download complete\n",
      "dcdde75d70d4: Verifying Checksum\n",
      "dcdde75d70d4: Download complete\n",
      "c131c32a822c: Download complete\n",
      "7ef98a0bb031: Verifying Checksum\n",
      "7ef98a0bb031: Download complete\n",
      "7b2bf50cb482: Verifying Checksum\n",
      "7b2bf50cb482: Download complete\n",
      "7dd53e86e25e: Verifying Checksum\n",
      "7dd53e86e25e: Download complete\n",
      "228e876a170d: Download complete\n",
      "2b8ce232b0f5: Verifying Checksum\n",
      "2b8ce232b0f5: Download complete\n",
      "6acab9f9b6c9: Verifying Checksum\n",
      "6acab9f9b6c9: Download complete\n",
      "49738298fc0b: Verifying Checksum\n",
      "49738298fc0b: Download complete\n",
      "450e691b8934: Download complete\n",
      "c11b1ed1ee46: Download complete\n",
      "d0067f8e4744: Verifying Checksum\n",
      "d0067f8e4744: Download complete\n",
      "f3a3e4335302: Verifying Checksum\n",
      "f3a3e4335302: Download complete\n",
      "31514946c388: Verifying Checksum\n",
      "31514946c388: Download complete\n",
      "4045938dbcf0: Download complete\n",
      "ec3ab3614609: Verifying Checksum\n",
      "ec3ab3614609: Download complete\n",
      "d59ff5de4fe3: Download complete\n",
      "8b4cfd996372: Verifying Checksum\n",
      "8b4cfd996372: Download complete\n",
      "c5e6195fcb5a: Verifying Checksum\n",
      "c5e6195fcb5a: Download complete\n",
      "b64eb74ea0f6: Verifying Checksum\n",
      "b64eb74ea0f6: Download complete\n",
      "fde2f9173aa1: Verifying Checksum\n",
      "fde2f9173aa1: Download complete\n",
      "67e193f6923c: Verifying Checksum\n",
      "67e193f6923c: Download complete\n",
      "05c48a632d89: Verifying Checksum\n",
      "05c48a632d89: Download complete\n",
      "dac08d81643b: Verifying Checksum\n",
      "dac08d81643b: Download complete\n",
      "753e545517ec: Download complete\n",
      "45643bc35dd6: Verifying Checksum\n",
      "45643bc35dd6: Download complete\n",
      "59fa5f6703c7: Verifying Checksum\n",
      "59fa5f6703c7: Download complete\n",
      "5251fc0e145f: Verifying Checksum\n",
      "5251fc0e145f: Download complete\n",
      "bdd807f81520: Verifying Checksum\n",
      "bdd807f81520: Download complete\n",
      "cda9dd232241: Download complete\n",
      "c1c0778801ed: Download complete\n",
      "c82a34ea3155: Verifying Checksum\n",
      "c82a34ea3155: Download complete\n",
      "d2ac586dac26: Download complete\n",
      "7e85ce497f3c: Download complete\n",
      "732c433b96b6: Verifying Checksum\n",
      "732c433b96b6: Download complete\n",
      "de409ef6b855: Verifying Checksum\n",
      "de409ef6b855: Download complete\n",
      "36a7c7813429: Verifying Checksum\n",
      "36a7c7813429: Download complete\n",
      "07391a0f51be: Verifying Checksum\n",
      "07391a0f51be: Download complete\n",
      "cc688c082c64: Verifying Checksum\n",
      "cc688c082c64: Download complete\n",
      "92681808abed: Verifying Checksum\n",
      "92681808abed: Download complete\n",
      "84b159fc1d60: Download complete\n",
      "161c50cb1ab8: Verifying Checksum\n",
      "161c50cb1ab8: Download complete\n",
      "3adfd786ac79: Verifying Checksum\n",
      "3adfd786ac79: Download complete\n",
      "b687b9abe298: Verifying Checksum\n",
      "b687b9abe298: Download complete\n",
      "4d55d43d5c97: Verifying Checksum\n",
      "4d55d43d5c97: Download complete\n",
      "2bda520db73d: Download complete\n",
      "f006e76f83a3: Verifying Checksum\n",
      "f006e76f83a3: Download complete\n",
      "7b8995abb34f: Verifying Checksum\n",
      "7b8995abb34f: Download complete\n",
      "70651e98f007: Verifying Checksum\n",
      "70651e98f007: Download complete\n",
      "7e50d9a71c79: Download complete\n",
      "8b7d3b755746: Verifying Checksum\n",
      "8b7d3b755746: Download complete\n",
      "fe9e4cebdbb5: Verifying Checksum\n",
      "fe9e4cebdbb5: Download complete\n",
      "daffe4a11a5d: Verifying Checksum\n",
      "daffe4a11a5d: Download complete\n",
      "07973a04f293: Verifying Checksum\n",
      "07973a04f293: Download complete\n",
      "0147547644c1: Verifying Checksum\n",
      "0147547644c1: Download complete\n",
      "d0067f8e4744: Pull complete\n",
      "9e8473502f86: Pull complete\n",
      "62cbcf4cbe13: Pull complete\n",
      "9054d495b912: Pull complete\n",
      "363a6cd12433: Pull complete\n",
      "69633d945bde: Pull complete\n",
      "030b5d54675a: Pull complete\n",
      "282a25844090: Pull complete\n",
      "51caa25c21ff: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "77bc21e94f24: Pull complete\n",
      "784e1b6fd612: Pull complete\n",
      "af1ee39433f4: Pull complete\n",
      "52321cba559e: Pull complete\n",
      "af00054fc370: Pull complete\n",
      "0b6fc99b1680: Pull complete\n",
      "6cee73498bf6: Pull complete\n",
      "8b4cfd996372: Pull complete\n",
      "b090c9ed50ff: Pull complete\n",
      "0a8adb3dcf22: Pull complete\n",
      "6e88a7f6f15d: Pull complete\n",
      "00f5ed83fb5b: Pull complete\n",
      "e518f4c1fb4d: Pull complete\n",
      "8b6afde98e21: Pull complete\n",
      "dcdde75d70d4: Pull complete\n",
      "c131c32a822c: Pull complete\n",
      "7ef98a0bb031: Pull complete\n",
      "7b2bf50cb482: Pull complete\n",
      "7dd53e86e25e: Pull complete\n",
      "228e876a170d: Pull complete\n",
      "2b8ce232b0f5: Pull complete\n",
      "6acab9f9b6c9: Pull complete\n",
      "49738298fc0b: Pull complete\n",
      "450e691b8934: Pull complete\n",
      "c11b1ed1ee46: Pull complete\n",
      "67e193f6923c: Pull complete\n",
      "f3a3e4335302: Pull complete\n",
      "31514946c388: Pull complete\n",
      "4045938dbcf0: Pull complete\n",
      "ec3ab3614609: Pull complete\n",
      "d59ff5de4fe3: Pull complete\n",
      "c5e6195fcb5a: Pull complete\n",
      "b64eb74ea0f6: Pull complete\n",
      "fde2f9173aa1: Pull complete\n",
      "05c48a632d89: Pull complete\n",
      "45643bc35dd6: Pull complete\n",
      "dac08d81643b: Pull complete\n",
      "753e545517ec: Pull complete\n",
      "59fa5f6703c7: Pull complete\n",
      "5251fc0e145f: Pull complete\n",
      "bdd807f81520: Pull complete\n",
      "cda9dd232241: Pull complete\n",
      "c1c0778801ed: Pull complete\n",
      "d2ac586dac26: Pull complete\n",
      "c82a34ea3155: Pull complete\n",
      "07391a0f51be: Pull complete\n",
      "7e85ce497f3c: Pull complete\n",
      "732c433b96b6: Pull complete\n",
      "de409ef6b855: Pull complete\n",
      "36a7c7813429: Pull complete\n",
      "cc688c082c64: Pull complete\n",
      "92681808abed: Pull complete\n",
      "8b7d3b755746: Pull complete\n",
      "07973a04f293: Pull complete\n",
      "84b159fc1d60: Pull complete\n",
      "161c50cb1ab8: Pull complete\n",
      "3adfd786ac79: Pull complete\n",
      "b687b9abe298: Pull complete\n",
      "4d55d43d5c97: Pull complete\n",
      "2bda520db73d: Pull complete\n",
      "f006e76f83a3: Pull complete\n",
      "7b8995abb34f: Pull complete\n",
      "70651e98f007: Pull complete\n",
      "7e50d9a71c79: Pull complete\n",
      "fe9e4cebdbb5: Pull complete\n",
      "0147547644c1: Pull complete\n",
      "daffe4a11a5d: Pull complete\n",
      "Digest: sha256:f1714efb94467b1b8f2dfadb27f4b6703568b616ceba85dec4df36dc581c339e\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      " ---> 58c045557af1\n",
      "Step 2/6 : WORKDIR /src\n",
      " ---> Running in 74231ff83221\n",
      "Removing intermediate container 74231ff83221\n",
      " ---> f293b036eb6c\n",
      "Step 3/6 : RUN pip install google-cloud-bigquery gcsfs\n",
      " ---> Running in b3d4588cff75\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.7.0-py2.py3-none-any.whl (215 kB)\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2023.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (3.19.6)\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.28.1)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (22.0)\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.15.0)\n",
      "Collecting fsspec==2023.3.0\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.57.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.8)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "\u001b[91mERROR: dask-cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement flatbuffers>=2.0, but you'll have flatbuffers 1.12 which is incompatible.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement tensorboard<2.11,>=2.10, but you'll have tensorboard 2.9.1 which is incompatible.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement tensorflow-estimator<2.11,>=2.10.0, but you'll have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: merlin-core 0.10.0 has requirement fsspec==2022.5.0, but you'll have fsspec 2023.3.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: grpcio-channelz 1.51.1 has requirement protobuf>=4.21.6, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement cuda-python<11.7.1,>=11.5, but you'll have cuda-python 11.8.1 which is incompatible.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement protobuf<3.21.0a0,>=3.20.1, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: grpcio-status 1.51.3 has requirement protobuf>=4.21.6, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: grpcio, grpcio-status, google-api-core, google-cloud-core, proto-plus, google-crc32c, google-resumable-media, google-cloud-bigquery, fsspec, google-cloud-storage, gcsfs\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "Successfully installed fsspec-2023.3.0 gcsfs-2023.3.0 google-api-core-2.11.0 google-cloud-bigquery-3.7.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 grpcio-1.51.3 grpcio-status-1.51.3 proto-plus-1.22.2\n",
      "Removing intermediate container b3d4588cff75\n",
      " ---> fb99bdc061b0\n",
      "Step 4/6 : RUN pip install google-cloud-aiplatform kfp\n",
      " ---> Running in ce0189840dbf\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.23.0-py2.py3-none-any.whl (2.5 MB)\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.19.tar.gz (304 kB)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (2.11.0)\n",
      "Collecting shapely<2.0.0\n",
      "  Downloading Shapely-1.8.5.post1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl (276 kB)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (3.7.0)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.8/dist-packages (from kfp) (1.3.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.2.0)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.15.0)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.17.3)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "Collecting kubernetes<20,>=8.0.0\n",
      "  Downloading kubernetes-19.15.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.4.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.57.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\" in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2; extra == \"grpc\" in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.3)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire<1,>=0.3.1->kfp) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire<1,>=0.3.1->kfp) (2.1.1)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (5.10.2)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (22.2.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.13)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from kubernetes<20,>=8.0.0->kfp) (1.4.2)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<20,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kubernetes<20,>=8.0.0->kfp) (45.2.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.34.2)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.8)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0; python_version < \"3.9\"->jsonschema<5,>=3.0.1->kfp) (3.11.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<20,>=8.0.0->kfp) (3.2.2)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.19-py3-none-any.whl size=426975 sha256=2578709c42fec297c76e475615dfd2c936d6604bfe0d205123941c520119ff78\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/48/49/09cd2b5fbd81315c167c7d58877eaaf825e8f9ff1d733ca618\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=7da0e60f4778eb38008965767d6a910ed64da4d21154309a862998d5094e4db0\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=e2fba92b95941ed59734699f02db73a404c832230925633dee921fd94fbe019c\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/b7/87/8884b574029455610a5b99752115d2ac857f8cfe8b846a1225\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=7dabd19798ab40ae1caee4b1ad8599e902df5299eed0e9ce010593ae9e840ffb\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/16/9b/7c21f4d08b98d02819658600b738d3453b2ffee3c9b757629e\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "\u001b[91mERROR: dask-cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement flatbuffers>=2.0, but you'll have flatbuffers 1.12 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: tensorflow 2.10.0+nv22.11 has requirement tensorboard<2.11,>=2.10, but you'll have tensorboard 2.9.1 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: tensorflow 2.10.0+nv22.11 has requirement tensorflow-estimator<2.11,>=2.10.0, but you'll have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "ERROR: merlin-core 0.10.0 has requirement fsspec==2022.5.0, but you'll have fsspec 2023.3.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: jupyter-server 2.0.6 has requirement tornado>=6.2.0, but you'll have tornado 6.1 which is incompatible.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement cuda-python<11.7.1,>=11.5, but you'll have cuda-python 11.8.1 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement protobuf<3.21.0a0,>=3.20.1, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: shapely, grpc-google-iam-v1, google-cloud-resource-manager, packaging, google-cloud-aiplatform, Deprecated, PyYAML, docstring-parser, fire, uritemplate, httplib2, google-auth-httplib2, google-api-python-client, kfp-pipeline-spec, kfp-server-api, kubernetes, pydantic, requests-toolbelt, strip-hints, tabulate, typer, kfp\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "Successfully installed Deprecated-1.2.13 PyYAML-5.4.1 docstring-parser-0.15 fire-0.5.0 google-api-python-client-1.12.11 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.23.0 google-cloud-resource-manager-1.9.0 grpc-google-iam-v1-0.12.6 httplib2-0.22.0 kfp-1.8.19 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-19.15.0 packaging-21.3 pydantic-1.10.6 requests-toolbelt-0.10.1 shapely-1.8.5.post1 strip-hints-0.1.10 tabulate-0.9.0 typer-0.7.0 uritemplate-3.0.1\n",
      "Removing intermediate container ce0189840dbf\n",
      " ---> bb8fdc061d1e\n",
      "Step 5/6 : COPY preprocessor/* ./\n",
      " ---> 7b6d862b044e\n",
      "Step 6/6 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      " ---> Running in a6cbdb17821e\n",
      "Removing intermediate container a6cbdb17821e\n",
      " ---> b5967628b78b\n",
      "Successfully built b5967628b78b\n",
      "Successfully tagged gcr.io/hybrid-vertex/nvt-preprocessing:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "The push refers to repository [gcr.io/hybrid-vertex/nvt-preprocessing]\n",
      "6db717bf2dfc: Preparing\n",
      "1c23df6156b5: Preparing\n",
      "efec7c651659: Preparing\n",
      "863f96316651: Preparing\n",
      "2f4c2e747ad7: Preparing\n",
      "f5b846caf956: Preparing\n",
      "12eec4f4a731: Preparing\n",
      "67516f8ac904: Preparing\n",
      "4c0655970ccc: Preparing\n",
      "73101e82e910: Preparing\n",
      "f4d87c23e98f: Preparing\n",
      "d07270ded861: Preparing\n",
      "8187da794091: Preparing\n",
      "d7ba493f5d44: Preparing\n",
      "ebd7caa9de82: Preparing\n",
      "242c03ad2a31: Preparing\n",
      "df5d5e1b9e48: Preparing\n",
      "3951f4addbfc: Preparing\n",
      "2e5281e7ae87: Preparing\n",
      "42ed3d5ddcb4: Preparing\n",
      "383fa992b719: Preparing\n",
      "e47df10e020a: Preparing\n",
      "317e7f70fff9: Preparing\n",
      "b45c2562fda1: Preparing\n",
      "0511decbb6f6: Preparing\n",
      "ac6f1cea2b76: Preparing\n",
      "d5f7fcaf77a9: Preparing\n",
      "bf10e31ca3c7: Preparing\n",
      "f1b1f58ddb35: Preparing\n",
      "71fe61786022: Preparing\n",
      "20d51f7b3d75: Preparing\n",
      "4357e5c17b0b: Preparing\n",
      "38dd15f32335: Preparing\n",
      "1014fb54d10a: Preparing\n",
      "f4c090be1950: Preparing\n",
      "bfc784fa095a: Preparing\n",
      "1f09cab8959e: Preparing\n",
      "ea787d764727: Preparing\n",
      "04b7d4d05916: Preparing\n",
      "3ace34553782: Preparing\n",
      "182ee86689d1: Preparing\n",
      "bbdb6055545b: Preparing\n",
      "9a161f9c0e0a: Preparing\n",
      "b7ad75a3e25e: Preparing\n",
      "7752ee8ae666: Preparing\n",
      "13f264ca1ed9: Preparing\n",
      "cd870b4929c1: Preparing\n",
      "d092d49243b5: Preparing\n",
      "aafb34ceba88: Preparing\n",
      "32710f0c03c7: Preparing\n",
      "6a55125f046a: Preparing\n",
      "2af1bd207a0a: Preparing\n",
      "ef25bc892765: Preparing\n",
      "e5171eae2464: Preparing\n",
      "f7bcdf2e75c1: Preparing\n",
      "ef257ce50d4c: Preparing\n",
      "d7d019fd6cda: Preparing\n",
      "bf08ecf416c0: Preparing\n",
      "19be279686b3: Preparing\n",
      "a42b9d3ddcc4: Preparing\n",
      "8a1a337e69bc: Preparing\n",
      "ca6dd165751d: Preparing\n",
      "ac02b4761a81: Preparing\n",
      "4be672edea27: Preparing\n",
      "3041a215c94e: Preparing\n",
      "db85811a91ed: Preparing\n",
      "6eb00409a05c: Preparing\n",
      "fb3ac13a8afd: Preparing\n",
      "e614e8f11c1c: Preparing\n",
      "70d54afe53fd: Preparing\n",
      "cbbf130153a1: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7173bbe6139c: Preparing\n",
      "23076d2c79f0: Preparing\n",
      "ed1a31ee03b7: Preparing\n",
      "fe17ea05bcc3: Preparing\n",
      "3204b933fb0b: Preparing\n",
      "4616162d4e6a: Preparing\n",
      "9a03c5ba8f95: Preparing\n",
      "764cbe4d1ae6: Preparing\n",
      "03a82e76641b: Preparing\n",
      "f874925be13d: Preparing\n",
      "2c4e7d9e38c0: Preparing\n",
      "67abb95254ee: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "38dd15f32335: Waiting\n",
      "1014fb54d10a: Waiting\n",
      "f4c090be1950: Waiting\n",
      "bfc784fa095a: Waiting\n",
      "1f09cab8959e: Waiting\n",
      "ea787d764727: Waiting\n",
      "04b7d4d05916: Waiting\n",
      "67516f8ac904: Waiting\n",
      "f5b846caf956: Waiting\n",
      "12eec4f4a731: Waiting\n",
      "4c0655970ccc: Waiting\n",
      "73101e82e910: Waiting\n",
      "f4d87c23e98f: Waiting\n",
      "d7ba493f5d44: Waiting\n",
      "8187da794091: Waiting\n",
      "242c03ad2a31: Waiting\n",
      "df5d5e1b9e48: Waiting\n",
      "3951f4addbfc: Waiting\n",
      "2e5281e7ae87: Waiting\n",
      "42ed3d5ddcb4: Waiting\n",
      "383fa992b719: Waiting\n",
      "e47df10e020a: Waiting\n",
      "317e7f70fff9: Waiting\n",
      "0511decbb6f6: Waiting\n",
      "ac6f1cea2b76: Waiting\n",
      "d5f7fcaf77a9: Waiting\n",
      "f1b1f58ddb35: Waiting\n",
      "71fe61786022: Waiting\n",
      "20d51f7b3d75: Waiting\n",
      "3ace34553782: Waiting\n",
      "2af1bd207a0a: Waiting\n",
      "182ee86689d1: Waiting\n",
      "ef25bc892765: Waiting\n",
      "bbdb6055545b: Waiting\n",
      "e5171eae2464: Waiting\n",
      "9a161f9c0e0a: Waiting\n",
      "f7bcdf2e75c1: Waiting\n",
      "b7ad75a3e25e: Waiting\n",
      "ef257ce50d4c: Waiting\n",
      "7752ee8ae666: Waiting\n",
      "13f264ca1ed9: Waiting\n",
      "d7d019fd6cda: Waiting\n",
      "cd870b4929c1: Waiting\n",
      "ca6dd165751d: Waiting\n",
      "d092d49243b5: Waiting\n",
      "aafb34ceba88: Waiting\n",
      "ac02b4761a81: Waiting\n",
      "bf08ecf416c0: Waiting\n",
      "32710f0c03c7: Waiting\n",
      "4be672edea27: Waiting\n",
      "6a55125f046a: Waiting\n",
      "3041a215c94e: Waiting\n",
      "fe17ea05bcc3: Waiting\n",
      "cbbf130153a1: Waiting\n",
      "3204b933fb0b: Waiting\n",
      "4616162d4e6a: Waiting\n",
      "19be279686b3: Waiting\n",
      "9a03c5ba8f95: Waiting\n",
      "a42b9d3ddcc4: Waiting\n",
      "8a1a337e69bc: Waiting\n",
      "6eb00409a05c: Waiting\n",
      "764cbe4d1ae6: Waiting\n",
      "fb3ac13a8afd: Waiting\n",
      "03a82e76641b: Waiting\n",
      "ed1a31ee03b7: Waiting\n",
      "23076d2c79f0: Waiting\n",
      "f874925be13d: Waiting\n",
      "e614e8f11c1c: Waiting\n",
      "2c4e7d9e38c0: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "67abb95254ee: Waiting\n",
      "70d54afe53fd: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "db85811a91ed: Waiting\n",
      "2f4c2e747ad7: Layer already exists\n",
      "f5b846caf956: Layer already exists\n",
      "12eec4f4a731: Layer already exists\n",
      "67516f8ac904: Layer already exists\n",
      "4c0655970ccc: Layer already exists\n",
      "73101e82e910: Layer already exists\n",
      "f4d87c23e98f: Layer already exists\n",
      "d07270ded861: Layer already exists\n",
      "6db717bf2dfc: Pushed\n",
      "863f96316651: Pushed\n",
      "8187da794091: Layer already exists\n",
      "ebd7caa9de82: Layer already exists\n",
      "d7ba493f5d44: Layer already exists\n",
      "242c03ad2a31: Layer already exists\n",
      "3951f4addbfc: Layer already exists\n",
      "df5d5e1b9e48: Layer already exists\n",
      "2e5281e7ae87: Layer already exists\n",
      "42ed3d5ddcb4: Layer already exists\n",
      "383fa992b719: Layer already exists\n",
      "e47df10e020a: Layer already exists\n",
      "b45c2562fda1: Layer already exists\n",
      "317e7f70fff9: Layer already exists\n",
      "0511decbb6f6: Layer already exists\n",
      "ac6f1cea2b76: Layer already exists\n",
      "d5f7fcaf77a9: Layer already exists\n",
      "bf10e31ca3c7: Layer already exists\n",
      "71fe61786022: Layer already exists\n",
      "f1b1f58ddb35: Layer already exists\n",
      "20d51f7b3d75: Layer already exists\n",
      "38dd15f32335: Layer already exists\n",
      "4357e5c17b0b: Layer already exists\n",
      "1014fb54d10a: Layer already exists\n",
      "bfc784fa095a: Layer already exists\n",
      "f4c090be1950: Layer already exists\n",
      "1f09cab8959e: Layer already exists\n",
      "04b7d4d05916: Layer already exists\n",
      "ea787d764727: Layer already exists\n",
      "3ace34553782: Layer already exists\n",
      "182ee86689d1: Layer already exists\n",
      "bbdb6055545b: Layer already exists\n",
      "9a161f9c0e0a: Layer already exists\n",
      "b7ad75a3e25e: Layer already exists\n",
      "7752ee8ae666: Layer already exists\n",
      "efec7c651659: Pushed\n",
      "13f264ca1ed9: Layer already exists\n",
      "cd870b4929c1: Layer already exists\n",
      "d092d49243b5: Layer already exists\n",
      "aafb34ceba88: Layer already exists\n",
      "32710f0c03c7: Layer already exists\n",
      "6a55125f046a: Layer already exists\n",
      "ef25bc892765: Layer already exists\n",
      "2af1bd207a0a: Layer already exists\n",
      "e5171eae2464: Layer already exists\n",
      "f7bcdf2e75c1: Layer already exists\n",
      "ef257ce50d4c: Layer already exists\n",
      "d7d019fd6cda: Layer already exists\n",
      "bf08ecf416c0: Layer already exists\n",
      "19be279686b3: Layer already exists\n",
      "ca6dd165751d: Layer already exists\n",
      "8a1a337e69bc: Layer already exists\n",
      "a42b9d3ddcc4: Layer already exists\n",
      "ac02b4761a81: Layer already exists\n",
      "3041a215c94e: Layer already exists\n",
      "4be672edea27: Layer already exists\n",
      "db85811a91ed: Layer already exists\n",
      "6eb00409a05c: Layer already exists\n",
      "fb3ac13a8afd: Layer already exists\n",
      "e614e8f11c1c: Layer already exists\n",
      "70d54afe53fd: Layer already exists\n",
      "cbbf130153a1: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "7173bbe6139c: Layer already exists\n",
      "23076d2c79f0: Layer already exists\n",
      "ed1a31ee03b7: Layer already exists\n",
      "fe17ea05bcc3: Layer already exists\n",
      "4616162d4e6a: Layer already exists\n",
      "3204b933fb0b: Layer already exists\n",
      "9a03c5ba8f95: Layer already exists\n",
      "764cbe4d1ae6: Layer already exists\n",
      "03a82e76641b: Layer already exists\n",
      "f874925be13d: Layer already exists\n",
      "2c4e7d9e38c0: Layer already exists\n",
      "67abb95254ee: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "1c23df6156b5: Pushed\n",
      "latest: digest: sha256:3e76ca909979ff9ce9e744f22a6cb37d64ebbf2ecb8e24901c60577292ae06c3 size: 18202\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                            STATUS\n",
      "185a0bb6-cde0-43df-b597-e6326f462215  2023-03-22T04:26:47+00:00  4M34S     gs://hybrid-vertex_cloudbuild/source/1679459207.072481-3fdaeee4e0f64f8a97b3df4a808f75ed.tgz  gcr.io/hybrid-vertex/nvt-preprocessing (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fd2ca-1003-4c1f-badf-1a78b6e4757c",
   "metadata": {},
   "source": [
    "# Vertex Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b940a83-6d48-4db9-9921-50e081bd142d",
   "metadata": {},
   "source": [
    "### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74544409-014c-45ff-85f0-538d989a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_limit': '2',\n",
      " 'gpu_type': 'nvidia_tesla_a100',\n",
      " 'instance_type': 'a2-highgpu-2g',\n",
      " 'memory_limit': '170g',\n",
      " 'version': 'latest-16'}\n"
     ]
    }
   ],
   "source": [
    "LABELS = {\n",
    "    'version': f'{VERSION}',\n",
    "    'gpu_type': f'{GPU_TYPE.lower()}',\n",
    "    'gpu_limit': f'{GPU_LIMIT}',\n",
    "    'memory_limit': f'{MEMORY_LIMIT.lower()}',\n",
    "    'instance_type': f'{INSTANCE_TYPE}',\n",
    "}\n",
    "pprint(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e34e6-9891-421e-a62d-2e6e49a66a44",
   "metadata": {},
   "source": [
    "## define pipe params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "105398a1-be24-4c85-94d9-fdfa45ee0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': 'spotify',\n",
      " 'bucket_data_output': 'jt-merlin-scaling',\n",
      " 'bucket_data_src': 'spotify-million-playlist-dataset',\n",
      " 'file_pattern': '*.snappy.parquet',\n",
      " 'num_output_files_train': 100,\n",
      " 'num_output_files_valid': 10,\n",
      " 'output_path_analyzed_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-16/nvt-analyzed',\n",
      " 'output_path_defined_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-16/nvt-defined',\n",
      " 'output_path_transformed_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-16/nvt-processed',\n",
      " 'shuffle': 'null',\n",
      " 'train_prefix': 'train_data_parquet',\n",
      " 'valid_prefix': 'valid_data_parquet',\n",
      " 'version': 'latest-16'}\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import storage\n",
    "\n",
    "# storage_client = storage.Client()\n",
    "\n",
    "TRAIN_PREFIX = 'train_data_parquet'\n",
    "VALID_PREFIX = 'valid_data_parquet'\n",
    "\n",
    "delimiter = '/'\n",
    "# FILE_PATTERN = \"*.parquet\"                    # full\n",
    "FILE_PATTERN = '*.snappy.parquet'    # subset\n",
    "\n",
    "# trying to achieve avg file size of ~100 mb\n",
    "num_output_files_train = 100 #0 # Number of output Parquet files\n",
    "num_output_files_valid = 10 #2 # Number of output Parquet files\n",
    "\n",
    "# Define output directories\n",
    "OUTPUT_DEFINED_DIR = os.path.join(WORKSPACE, \"nvt-defined\")\n",
    "OUTPUT_WORKFLOW_DIR = os.path.join(WORKSPACE, \"nvt-analyzed\")\n",
    "OUTPUT_TRANSFORMED_DIR = os.path.join(WORKSPACE, \"nvt-processed\")\n",
    "\n",
    "\n",
    "parq_parameter_values = {\n",
    "    'bucket_data_src': BUCKET_source,\n",
    "    'bucket_data_output': BUCKET_destin,\n",
    "    'train_prefix': f'{TRAIN_PREFIX}',\n",
    "    'valid_prefix': f'{VALID_PREFIX}',\n",
    "    'file_pattern': f'{FILE_PATTERN}',\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid,\n",
    "    'output_path_defined_dir': f'{OUTPUT_DEFINED_DIR}',\n",
    "    'output_path_analyzed_dir': f'{OUTPUT_WORKFLOW_DIR}',\n",
    "    'output_path_transformed_dir': f'{OUTPUT_TRANSFORMED_DIR}',\n",
    "    'version':f'{VERSION}',\n",
    "    'shuffle': json.dumps(None), # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "    'app':f'{APP}',\n",
    "}\n",
    "\n",
    "pprint(parq_parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6527bc-5ad8-4bea-9694-3e1b2763e290",
   "metadata": {},
   "source": [
    "## compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c34709df-46f9-4f2a-9422-e196cf287345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from src.process_pipes.preproc_pipelines import preprocessing_parquet\n",
    "\n",
    "_compiled_pipeline_path = f'{PREPROCESS_PARQUET_PIPELINE_NAME}.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_parquet,\n",
    "       package_path=_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c41f7b-5a55-4af0-8331-4e61f0fabadf",
   "metadata": {},
   "source": [
    "## submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18a23dfe-2f1e-4863-9868-ea62644b204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvt-parquet-latest-16'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESS_PARQUET_PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5305c2e3-d1bd-4349-9e83-e5539c11b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{PREPROCESS_PARQUET_PIPELINE_NAME}_{TIMESTAMP}' #{TIMESTAMP}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parq_parameter_values,\n",
    "    labels=LABELS,\n",
    ")\n",
    "\n",
    "pipeline_job.submit(service_account=VERTEX_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ead4c-47ec-45ec-9b67-cb158f0f5f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deab76-3e86-41e1-8953-be1b7e854787",
   "metadata": {},
   "source": [
    "### Define the NVTabular preprocessing graph\n",
    "\n",
    "```\n",
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = ['artist_name_can',\n",
    "        'track_name_can',\n",
    "        'artist_genres_can',\n",
    "    ]\n",
    "\n",
    "item_features_cont = [\n",
    "        'duration_ms_can',\n",
    "        'track_pop_can',\n",
    "        'artist_pop_can',\n",
    "        'artist_followers_can',\n",
    "    ]\n",
    "\n",
    "playlist_features_cat = [\n",
    "        'description_pl',\n",
    "        'name',\n",
    "        'collaborative',\n",
    "    ]\n",
    "\n",
    "playlist_features_cont = [\n",
    "        'duration_ms_seed_pl',\n",
    "        'n_songs_pl',\n",
    "        'num_artists_pl',\n",
    "        'num_albums_pl',\n",
    "    ]\n",
    "\n",
    "seq_feats_cat = [\n",
    "        'artist_name_pl',\n",
    "        'track_uri_pl',\n",
    "        'track_name_pl',\n",
    "        'album_name_pl',\n",
    "        'artist_genres_pl',\n",
    "    ]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
