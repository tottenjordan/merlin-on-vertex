{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592fa423-5ce2-4e2b-a7ab-97174c387d47",
   "metadata": {},
   "source": [
    "# Preprocessing with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17770281-7bed-4495-a70f-1397441b60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-pipeline-components \n",
    "# !pip install google-cloud-bigquery-storage \n",
    "# !pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fbd89bc-5668-4e36-a0b8-c8a4ad835f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# import nvtabular as nvt\n",
    "# from nvtabular.ops import (\n",
    "#     Categorify,\n",
    "#     TagAsUserID,\n",
    "#     TagAsItemID,\n",
    "#     TagAsItemFeatures,\n",
    "#     TagAsUserFeatures,\n",
    "#     AddMetadata,\n",
    "#     ListSlice\n",
    "# )\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.schema.tags import Tags\n",
    "\n",
    "# import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "# import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e737b-e271-445b-a0c3-3b2dca620352",
   "metadata": {},
   "source": [
    "### Load config from setup notebook\n",
    "\n",
    "Set the constants used in this notebook and load the config settings from the 00_environment_setup.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e733b8-6417-431a-9ca5-51b6e4924642",
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "# BUCKET_NAME = f\"{PROJECT_ID}-merlintowers\"\n",
    "# config = !gsutil cat gs://{BUCKET_NAME}/config/notebook_env.py\n",
    "# print(config.n)\n",
    "# exec(config.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baea87eb-a282-4ce5-ada0-bc1791b0da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1278545-3674-4552-832f-8bedc19d5791",
   "metadata": {},
   "source": [
    "# Define preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0466709-cfb6-42ae-9d91-f02a2e8fb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket definitions\n",
    "# VERSION = 'v1-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions\n",
    "# IMAGE_NAME = 'nvt-preprocessing'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# DOCKERNAME = f'nvtabular-160' # 150\n",
    "\n",
    "# # Pipeline definitions\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "# print(f\"VERSION: {VERSION}\")\n",
    "# print(f\"APP: {APP}\")\n",
    "# print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "# print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "# print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "# print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "# print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78b87f1-429e-4fe6-821f-5fcf2b4818bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0816f671-55c0-40a8-a0a9-3f280c16b44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list the current work dir\n",
    "# os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.chdir('/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ce7975-79fc-4280-9452-5162244cfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PREPROC_SUB_DIR = 'preprocessor'\n",
    "PIPELINE_SUB_DIR = 'process_pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa05f47a-e39a-4f81-b96c-3da03005520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/__init__.py\n",
    "\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933d24f-8c8e-46e8-a72b-a821f2be7fe3",
   "metadata": {},
   "source": [
    "## preprocessing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a5fe2c1-e2b9-4162-be43-91be39b7f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocessor/preprocess_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/preprocess_task.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "# logging.disable(logging.WARNING)\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import fsspec\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.io.shuffle import Shuffle\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "# import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "# =============================================\n",
    "# featutres\n",
    "# =============================================\n",
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = [\n",
    "    'artist_name_can',\n",
    "    'track_name_can',\n",
    "    'artist_genres_can',\n",
    "]\n",
    "\n",
    "item_features_cont = [\n",
    "    'duration_ms_can',\n",
    "    'track_pop_can',\n",
    "    'artist_pop_can',\n",
    "    'artist_followers_can',\n",
    "]\n",
    "\n",
    "playlist_features_cat = [\n",
    "    'description_pl',\n",
    "    'name',\n",
    "    'collaborative',\n",
    "]\n",
    "\n",
    "playlist_features_cont = [\n",
    "    'duration_ms_seed_pl',\n",
    "    'n_songs_pl',\n",
    "    'num_artists_pl',\n",
    "    'num_albums_pl',\n",
    "]\n",
    "\n",
    "seq_feats_cat = [\n",
    "    'artist_name_pl',\n",
    "    'track_uri_pl',\n",
    "    'track_name_pl',\n",
    "    'album_name_pl',\n",
    "    'artist_genres_pl',\n",
    "]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "# =============================================\n",
    "# create cluster\n",
    "# =============================================\n",
    "def create_cluster(\n",
    "    n_workers,\n",
    "    device_limit_frac,\n",
    "    device_pool_frac,\n",
    "    memory_limit\n",
    "):\n",
    "    \"\"\"Create a Dask cluster to apply the transformations steps to the Dataset.\"\"\"\n",
    "    device_size = device_mem_size()\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    rmm_pool_size = (device_pool_size // 256) * 256\n",
    "\n",
    "    cluster = LocalCUDACluster(\n",
    "        n_workers=n_workers,\n",
    "        device_memory_limit=device_limit,\n",
    "        rmm_pool_size=rmm_pool_size,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "    return Client(cluster)\n",
    "\n",
    "# =============================================\n",
    "#            Create & Save dataset\n",
    "# =============================================\n",
    "\n",
    "def create_parquet_nvt_dataset(\n",
    "    # data_path,\n",
    "    frac_size,\n",
    "    data_prefix,\n",
    "    bucket_name,\n",
    "    file_pattern,\n",
    "):\n",
    "    \"\"\"Create a nvt.Dataset definition for the parquet files.\"\"\"\n",
    "    \n",
    "    # BUCKET = 'gs://spotify-builtin-2t'\n",
    "    # DATA_PATH = f\"{BUCKET}/{data_prefix}/0000000000**.snappy.parquet\"\n",
    "    DATA_PATH = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\" #0000000000**.snappy.parquet\"\n",
    "    logging.info(f\"DATA_PATH: {DATA_PATH}\")\n",
    "    \n",
    "    fs = fsspec.filesystem('gs')\n",
    "    \n",
    "    file_list = fs.glob(DATA_PATH)\n",
    "        # os.path.join(data_path, '*.parquet')\n",
    "    # )\n",
    "\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError('Parquet file(s) not found')\n",
    "\n",
    "    file_list = [os.path.join('gs://', i) for i in file_list]\n",
    "    \n",
    "    logging.info(f\"Number of files: {len(file_list)}\")\n",
    "\n",
    "    # return nvt.Dataset(f\"{bucket_name}/{data_prefix}/0000000000**.snappy.parquet\", part_mem_fraction=frac_size)\n",
    "    return nvt.Dataset(\n",
    "        file_list,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size\n",
    "  )\n",
    "\n",
    "def save_dataset(\n",
    "    dataset,\n",
    "    output_path,\n",
    "    output_files,\n",
    "    # categorical_cols,\n",
    "    # continuous_cols,\n",
    "    shuffle=None,\n",
    "):\n",
    "    \"\"\"Save dataset to parquet files to path.\"\"\"\n",
    "    categorical_cols=CAT\n",
    "    continuous_cols=CONT\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in categorical_cols:\n",
    "        dict_dtypes[col] = np.int32\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        dict_dtypes[col] = np.float64\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path=output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files,\n",
    "        dtypes=dict_dtypes,\n",
    "        cats=categorical_cols,\n",
    "        conts=continuous_cols,\n",
    "    )\n",
    "\n",
    "# =============================================\n",
    "#            Workflow\n",
    "# =============================================\n",
    "def create_nvt_workflow():\n",
    "    '''\n",
    "    Create a nvt.Workflow definition with transformation all the steps\n",
    "    '''\n",
    "    item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "    playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "    item_features_cat = ['artist_name_can',\n",
    "            'track_name_can',\n",
    "            'artist_genres_can',\n",
    "        ]\n",
    "\n",
    "    item_features_cont = [\n",
    "            'duration_ms_can',\n",
    "            'track_pop_can',\n",
    "            'artist_pop_can',\n",
    "            'artist_followers_can',\n",
    "        ]\n",
    "\n",
    "    playlist_features_cat = [\n",
    "            'description_pl',\n",
    "            'name',\n",
    "            'collaborative',\n",
    "        ]\n",
    "\n",
    "    playlist_features_cont = [\n",
    "            'duration_ms_seed_pl',\n",
    "            'n_songs_pl',\n",
    "            'num_artists_pl',\n",
    "            'num_albums_pl',\n",
    "        ]\n",
    "\n",
    "    seq_feats_cat = [\n",
    "            'artist_name_pl',\n",
    "            'track_uri_pl',\n",
    "            'track_name_pl',\n",
    "            'album_name_pl',\n",
    "            'artist_genres_pl',\n",
    "        ]\n",
    "\n",
    "    CAT = playlist_features_cat + item_features_cat\n",
    "    CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "    item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "    item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "    playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "    playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "    playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    \n",
    "    # define a workflow\n",
    "    output = playlist_id + item_id \\\n",
    "    + item_feature_cat_node \\\n",
    "    + item_feature_cont_node \\\n",
    "    + playlist_feature_cat_node \\\n",
    "    + playlist_feature_cont_node \\\n",
    "    + playlist_feature_cat_seq_node \n",
    "\n",
    "    workflow = nvt.Workflow(output)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# =============================================\n",
    "#            Create Parquet Dataset \n",
    "# =============================================\n",
    "\n",
    "def create_parquet_dataset_definition(\n",
    "    # data_paths,\n",
    "    # recursive,\n",
    "    # col_dtypes,\n",
    "    frac_size,\n",
    "    bucket_name,\n",
    "    data_prefix,\n",
    "    file_pattern,\n",
    "    # sep='\\t'\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    DATASET_DEFINITION = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\"  # 0000000000**.snappy.parquet\"\n",
    "    \n",
    "    logging.info(f'DATASET_DEFINITION: {DATASET_DEFINITION}')\n",
    "    \n",
    "    fs = fsspec.filesystem('gs')\n",
    "    file_list = fs.glob(DATASET_DEFINITION)\n",
    "\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError('Parquet file(s) not found')\n",
    "\n",
    "    file_list = [os.path.join('gs://', i) for i in file_list]\n",
    "    logging.info(f\"Number of files: {len(file_list)}\")\n",
    "    \n",
    "    return nvt.Dataset(f\"{DATASET_DEFINITION}\", engine='parquet', part_mem_fraction=frac_size)\n",
    "\n",
    "\n",
    "def convert_definition_to_parquet(\n",
    "    output_path,\n",
    "    dataset,\n",
    "    output_files,\n",
    "    shuffle=None\n",
    "):\n",
    "    \"\"\"Convert Parquet files to parquet and write to GCS.\"\"\"\n",
    "    if shuffle == 'None':\n",
    "        shuffle = None\n",
    "    else:\n",
    "        try:\n",
    "            shuffle = getattr(Shuffle, shuffle)\n",
    "        except:\n",
    "            print('Shuffle method not available. Using default.')\n",
    "            shuffle = None\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Create nv-tabular definition\n",
    "# =============================================\n",
    "def main_convert(args):\n",
    "    \n",
    "    logging.info('Beginning main-convert from preprocess_task.py...')\n",
    "    logging.info(f'args.output_path: {args.output_path}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating parquet dataset definition')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        # data_paths=args.parq_data_path,\n",
    "        # recursive=False,\n",
    "        bucket_name=args.bucket_name,     # 'spotify-builtin-2t', # TODO: parameterize\n",
    "        data_prefix=args.data_prefix,     # 'train', # TODO: JT check\n",
    "        frac_size=args.frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "\n",
    "    logging.info('Converting definition to Parquet')\n",
    "    convert_definition_to_parquet(\n",
    "        args.output_path,\n",
    "        dataset,\n",
    "        args.output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Analyse Dataset \n",
    "# =============================================\n",
    "def main_analyze(args):\n",
    "    \n",
    "    logging.info('Beginning main-analyze from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating Parquet dataset')\n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=file_pattern #\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "  \n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "  \n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(args.output_path)\n",
    "    \n",
    "# =============================================\n",
    "#            Transform Dataset \n",
    "# =============================================\n",
    "def main_transform(args):\n",
    "    \n",
    "    logging.info('Beginning main-transform from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit,\n",
    "    )\n",
    "\n",
    "    # nvt_workflow = create_nvt_workflow()\n",
    "    nvt_workflow = nvt.Workflow.load(args.workflow_path, client)\n",
    "\n",
    "    # dataset = create_parquet_nvt_dataset(\n",
    "    #     args.parquet_data_path, \n",
    "    #     frac_size=args.frac_size)\n",
    "    \n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=file_pattern #\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    transformed_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info('Saving transformed dataset')\n",
    "    save_dataset(\n",
    "        transformed_dataset,\n",
    "        output_path=args.output_path,\n",
    "        output_files=args.output_files,\n",
    "        # categorical_cols=CAT,\n",
    "        # continuous_cols=CONT,\n",
    "        shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            args\n",
    "# =============================================\n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "  \n",
    "    parser.add_argument(\n",
    "        '--task',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bucket_name',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parquet_data_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parq_data_path',\n",
    "        required=False,\n",
    "        nargs='+'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_files',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_workers',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--frac_size',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--memory_limit',\n",
    "        type=int,\n",
    "        required=False,\n",
    "        default=100_000_000_000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_limit_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_pool_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.90\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    start_time = time.time()\n",
    "    logging.info('Timing task')\n",
    "\n",
    "    if parsed_args.task == 'transform':\n",
    "        main_transform(parsed_args)\n",
    "    elif parsed_args.task == 'analyze':\n",
    "        main_analyze(parsed_args)\n",
    "    elif parsed_args.task == 'convert':\n",
    "        main_convert(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Task completed. Elapsed time: %s', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df76071-ea5a-479b-bd27-0135cd3c2fdd",
   "metadata": {},
   "source": [
    "## pipe components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5eaef21-5eaa-4f4a-b1f4-d2d0392cefa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/pipe_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/pipe_components.py\n",
    "\"\"\"KFP components.\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "from . import config\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Dataset\n",
    "from kfp.v2.dsl import Input\n",
    "from kfp.v2.dsl import Model\n",
    "from kfp.v2.dsl import Output\n",
    "\n",
    "# =============================================\n",
    "#            convert_to_parquet_op\n",
    "# =============================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def convert_parquet_op(\n",
    "    output_dataset: Output[Dataset],\n",
    "    bucket_name: str,\n",
    "    data_prefix: str,\n",
    "    file_pattern: str,\n",
    "    output_path_defined_dir: str,\n",
    "    # data_dir_pattern: str,\n",
    "    # data_paths: list,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: Optional[str] = None,\n",
    "    recursive: Optional[bool] = False,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to create NVTabular definition.\n",
    "    \n",
    "    Args:\n",
    "    output_dataset: Output[Dataset]\n",
    "      Output metadata with references to the converted CSV files in GCS\n",
    "      and the split name.The path to the files are in GCS fuse format:\n",
    "      /gcs/<bucket name>/path/to/file\n",
    "    bucket: gcs bucket holding train & valid data\n",
    "    data_path_prefix: file path to GCS blobl object (e.g., gs://...data/path/prefix.../blob.xxx)\n",
    "    data_paths: list\n",
    "    split: str\n",
    "      Split name of the dataset. Example: train or valid\n",
    "    shuffle: str\n",
    "      How to shuffle the converted CSV, default to None. Options:\n",
    "        PER_PARTITION\n",
    "        PER_WORKER\n",
    "        FULL\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    #            import packages\n",
    "    # =========================================================\n",
    "    import os\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_parquet_dataset_definition,\n",
    "        convert_definition_to_parquet,\n",
    "        # get_criteo_col_dtypes,\n",
    "    )\n",
    "    \n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "    logging.info('Base path in %s', output_dataset.path)\n",
    "    # =========================================================\n",
    "    #            Define data paths\n",
    "    # =========================================================\n",
    "    logging.info(f'bucket_name: {bucket_name}')\n",
    "    logging.info(f'data_prefix: {data_prefix}')\n",
    "    \n",
    "    # Write metadata\n",
    "    output_dataset.metadata['split'] = split\n",
    "\n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating dataset definition from: {data_path_prefix}')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        bucket_name=bucket_name,\n",
    "        data_prefix=data_prefix,\n",
    "        frac_size=frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Converting Definition to Parquet; {output_dataset.uri}')\n",
    "    logging.info(f'Parquet Definition Output Path: ; {output_path_defined_dir}/{split}')\n",
    "    convert_definition_to_parquet(\n",
    "        output_path=f'{output_path_defined_dir}/{split}', # output_dataset.uri,\n",
    "        dataset=dataset,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "# =========================================================\n",
    "#            analyze_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def analyze_dataset_op(\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    workflow: Output[Artifact],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    n_workers: int,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to generate statistics from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    parquet_dataset: List of strings\n",
    "      Input metadata with references to the train and valid converted\n",
    "      datasets in GCS and the split name.\n",
    "    workflow: Output[Artifact]\n",
    "      Output metadata with the path to the fitted workflow artifacts\n",
    "      (statistics).\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    '''\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "  \n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_nvt_workflow,\n",
    "    )\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    create_cluster(\n",
    "      n_workers=n_workers,\n",
    "      device_limit_frac=device_limit_frac,\n",
    "      device_pool_frac=device_pool_frac,\n",
    "      memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating Parquet dataset:{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset output_path_defined_dir: {output_path_defined_dir}/train')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/train', # TODO: JT Check \"train\"    # parquet_dataset.uri,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "\n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "\n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "    \n",
    "# =========================================================\n",
    "#            transform_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def transform_dataset_op(\n",
    "    workflow: Input[Artifact],\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    transformed_dataset: Output[Dataset],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    version: str,\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    app: str,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: str = None,\n",
    "    device_limit_frac: float = 0.6,\n",
    "    device_pool_frac: float = 0.9,\n",
    "    frac_size: float = 0.10,\n",
    "    memory_limit: int = 100_000_000_000\n",
    "):\n",
    "    \"\"\"Component to transform a dataset according to the workflow definitions.\n",
    "    Args:\n",
    "        workflow: Input[Artifact]\n",
    "        Input metadata with the path to the fitted_workflow\n",
    "        parquet_dataset: Input[Dataset]\n",
    "              Location of the converted dataset in GCS and split name\n",
    "        transformed_dataset: Output[Dataset]\n",
    "        Split name of the transformed dataset.\n",
    "        shuffle: str\n",
    "            How to shuffle the converted CSV, default to None. Options:\n",
    "                PER_PARTITION\n",
    "                PER_WORKER\n",
    "                FULL\n",
    "    device_limit_frac: float = 0.6\n",
    "    device_pool_frac: float = 0.9\n",
    "    frac_size: float = 0.10\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "    from merlin.schema import Tags\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        save_dataset,\n",
    "    )\n",
    "    def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "        client = storage.Client()\n",
    "        blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "        blob.bucket._client = client\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "    def _read_blob_gcs(bucket_name, source_blob_name, destination_filename):\n",
    "        \"\"\"Downloads a file from GCS to local directory\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_filename)\n",
    "        \n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    transformed_dataset.metadata['split'] = split\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "   # logging.info(f'Creating Parquet dataset:gs://{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset:{output_path_defined_dir}/{split}')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/{split}', #f'gs://{parquet_dataset.uri}',\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "    \n",
    "    logging.info('Loading Workflow')\n",
    "    nvt_workflow = nvt.Workflow.load(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    trans_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info(f'transformed_dataset.uri: {transformed_dataset.uri}')\n",
    "    logging.info(f'Saving transformed dataset: {output_path_transformed_dir}/{split}')\n",
    "    save_dataset(\n",
    "        dataset=trans_dataset,\n",
    "        output_path=f'{output_path_transformed_dir}/{split}', # transformed_dataset.uri,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    logging.info(f'transformed_dataset saved!')\n",
    "    logging.info(f'transformed_dataset.path: {transformed_dataset.path}')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        read and upload files\n",
    "    # =========================================================\n",
    "    '''\n",
    "    nv-tabular creates a txt file with all `gs://` paths\n",
    "    create a copy that replaces `gs://` with `/gcs/`\n",
    "    '''\n",
    "    logging.info('Generating file list for training...')\n",
    "    \n",
    "#     # get loca directory\n",
    "#     # LOCAL_DIRECTORY = os.getcwd()\n",
    "#     LOCAL_DIRECTORY = '/tmp/directory'\n",
    "    \n",
    "#     # _bucket_name='spotify-merlin-v1' # bucket_data_src\n",
    "#     PREFIX = f'nvt-preprocessing-{app}-{version}/nvt-processed/{split}'\n",
    "#     FILENAME = '_file_list.txt'\n",
    "#     SOURCE_BLOB_NAME = f'{PREFIX}/{FILENAME}'\n",
    "#     logging.info(f'SOURCE_BLOB_NAME: {SOURCE_BLOB_NAME}')\n",
    "    \n",
    "#     # LOCAL_DESTINATION_FILENAME = f'{LOCAL_DIRECTORY}/local_file_list.txt'\n",
    "#     LOCAL_DESTINATION_FILENAME = 'local_file_list.txt'\n",
    "#     logging.info(f'LOCAL_DESTINATION_FILENAME: {LOCAL_DESTINATION_FILENAME}')\n",
    "    \n",
    "#     _read_blob_gcs(\n",
    "#         bucket_name=bucket_data_output,\n",
    "#         source_blob_name=f'{SOURCE_BLOB_NAME}', \n",
    "#         destination_filename=LOCAL_DESTINATION_FILENAME\n",
    "#     )\n",
    "    \n",
    "#     file_list = os.path.join(transformed_dataset.path, '_file_list.txt')\n",
    "    \n",
    "#     # write new '/gcs/' file\n",
    "#     new_lines = []\n",
    "#     with open(LOCAL_DESTINATION_FILENAME, 'r') as fp:\n",
    "#         lines = fp.readlines()\n",
    "#         new_lines.append(lines[0])\n",
    "#         for line in lines[1:]:\n",
    "#             new_lines.append(line.replace('gs://', '/gcs/'))\n",
    "\n",
    "#     NEW_LOCAL_FILENAME = f'{LOCAL_DIRECTORY}/_gcs_file_list.txt'\n",
    "#     logging.info(f'NEW_LOCAL_FILENAME: {NEW_LOCAL_FILENAME}')\n",
    "    \n",
    "#     with open(NEW_LOCAL_FILENAME, 'w') as fp:\n",
    "#         fp.writelines(new_lines)\n",
    "        \n",
    "#     GCS_URI_DESTINATION = f'{output_path_transformed_dir}/{split}'\n",
    "#     logging.info(f'GCS_URI_DESTINATION: {GCS_URI_DESTINATION}')\n",
    "    \n",
    "#     _upload_blob_gcs(\n",
    "#         gcs_uri=GCS_URI_DESTINATION, \n",
    "#         source_file_name=NEW_LOCAL_FILENAME, \n",
    "#         destination_blob_name='_gcs_file_list.txt'\n",
    "#     )\n",
    "# logging.info(f'List of /gcs/ file paths uploaded to {GCS_URI_DESTINATION}/_gcs_file_list.txt')\n",
    "\n",
    "#     file_list_name = '_file_list.txt'\n",
    "#     file_list_uri = f'{output_path_transformed_dir}/{split}/{file_list_name}'\n",
    "#     logging.info(f'file_list_uri : {file_list_uri}')\n",
    "\n",
    "#     new_lines = []\n",
    "#     with open(file_list_uri, 'r') as fp:\n",
    "#         lines = fp.readlines()\n",
    "#         new_lines.append(lines[0])\n",
    "#         for line in lines[1:]:\n",
    "#             new_lines.append(line.replace('gs://', '/gcs/'))\n",
    "\n",
    "#     gcs_file_list_name = '_gcs_file_list.txt'\n",
    "#     gcs_file_list_uri = f'{output_path_transformed_dir}/{split}/{gcs_file_list_name}'\n",
    "#     logging.info(f'gcs_file_list_uri : {gcs_file_list_uri}')\n",
    "    \n",
    "#     with open(gcs_file_list_uri, 'w') as fp:\n",
    "#         fp.writelines(new_lines)\n",
    "    \n",
    "#     logging.info(f'List of /gcs/ file paths uploaded to {gcs_file_list}')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        Saving cardinalities\n",
    "    # =========================================================\n",
    "    logging.info('Saving cardinalities')\n",
    "    \n",
    "    cols_schemas = nvt_workflow.output_schema.select_by_tag(Tags.CATEGORICAL)\n",
    "    cols_names = cols_schemas.column_names\n",
    "\n",
    "    cards = []\n",
    "    for c in cols_names:\n",
    "        col = cols_schemas.get(c)\n",
    "        cards.append(col.properties['embedding_sizes']['cardinality'])\n",
    "\n",
    "    transformed_dataset.metadata['cardinalities'] = cards\n",
    "    # transformed_dataset.metadata['dataset_gcs_uri'] = gcs_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729926e-c1f0-4389-995b-bc6db237c734",
   "metadata": {},
   "source": [
    "## preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88103c26-3f43-4835-b2ff-8a26245d6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/preproc_pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/preproc_pipelines.py\n",
    "\"\"\"Preprocessing pipelines.\"\"\"\n",
    "\n",
    "from . import pipe_components\n",
    "from . import config\n",
    "from kfp.v2 import dsl\n",
    "import os\n",
    "\n",
    "GKE_ACCELERATOR_KEY = 'cloud.google.com/gke-accelerator'\n",
    "\n",
    "# TODO: parametrize and fix config file \n",
    "# BUCKET_parquet = 'spotify-builtin-2t'\n",
    "# BUCKET = 'spotify-merlin-v1'\n",
    "# VERSION = 'v32-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-preprocessing-{APP}-{VERSION}'\n",
    "# WORKSPACE = f'gs://{config.BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvtabular-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'{config.PREPROCESS_PARQUET_PIPELINE_NAME}', \n",
    "    pipeline_root=f'{config.PREPROCESS_PARQUET_PIPELINE_ROOT}'\n",
    ")\n",
    "def preprocessing_parquet(\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    # train_pattern: str,\n",
    "    # valid_pattern: str,\n",
    "    train_prefix: str,\n",
    "    valid_prefix: str,\n",
    "    file_pattern: str,\n",
    "    num_output_files_train: int,\n",
    "    num_output_files_valid: int,\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    shuffle: str,\n",
    "    version: str,\n",
    "    app: str,\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Pipeline to preprocess parquet files in GCS.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    # TODO: extract from BQ to parquet \n",
    "    # =========================================================\n",
    "    \n",
    "    \n",
    "    # =========================================================\n",
    "    #             Convert from parquet to def \n",
    "    # =========================================================\n",
    "    # config.BUCKET_NAME = 'spotify-builtin-2t' # 'spotify-merlin-v1' # TODO: parameterize\n",
    "    \n",
    "    parquet_to_def_train = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=train_prefix,\n",
    "            # data_dir_pattern=train_pattern,\n",
    "            split='train',\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_train.set_display_name('Convert training split')\n",
    "    parquet_to_def_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_train.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # === Convert eval dataset from CSV to Parquet\n",
    "    parquet_to_def_valid = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=valid_prefix,\n",
    "            # data_dir_pattern=valid_pattern,\n",
    "            split='valid',\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_valid.set_display_name('Convert validation split')\n",
    "    parquet_to_def_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_valid.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Analyse train dataset \n",
    "    # =========================================================\n",
    "    \n",
    "    # === Analyze train data split\n",
    "    analyze_dataset = (\n",
    "        pipe_components.analyze_dataset_op(\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir\n",
    "        )\n",
    "    )\n",
    "    analyze_dataset.set_display_name('Analyze Dataset')\n",
    "    analyze_dataset.set_cpu_limit(config.CPU_LIMIT)\n",
    "    analyze_dataset.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    analyze_dataset.set_gpu_limit(config.GPU_LIMIT)\n",
    "    analyze_dataset.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    analyze_dataset.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Transform train split \n",
    "    # =========================================================\n",
    "\n",
    "    # === Transform train data split\n",
    "    transform_train = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='train',\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_train.set_display_name('Transform train split')\n",
    "    transform_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    transform_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_train.set_caching_options(enable_caching=True)\n",
    "\n",
    "    # =========================================================\n",
    "    #     Transform valid split\n",
    "    # =========================================================\n",
    "    \n",
    "    transform_valid = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='valid',\n",
    "            parquet_dataset=parquet_to_def_valid.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_valid.set_display_name('Transform valid split')\n",
    "    transform_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    transform_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_valid.set_caching_options(enable_caching=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16211a8f-23a4-4bf5-a8d3-ccc99b9ea669",
   "metadata": {},
   "source": [
    "### Set Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa92776-dc2d-4a86-8078-0119140082a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_source: spotify-beam-v3\n",
      "BUCKET_destin: jt-merlin-scaling\n",
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "\n",
      "GPU_LIMIT: 2\n",
      "GPU_TYPE: NVIDIA_TESLA_A100\n",
      "CPU_LIMIT: 24\n",
      "MEMORY_LIMIT: 170G\n",
      "INSTANCE_TYPE: a2-highgpu-2g\n",
      "\n",
      "VERSION: latest-12\n",
      "APP: spotify\n",
      "MODEL_DISPLAY_NAME: nvt-last5-latest-12\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-latest-12\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-latest-12\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-parquet-latest-12\n",
      "\n",
      "IMAGE_NAME: nvt-preprocessing\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "DOCKERNAME: nvt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "#           storage\n",
    "# =============================================\n",
    "BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "print(f\"BUCKET_source: {BUCKET_source}\")\n",
    "print(f\"BUCKET_destin: {BUCKET_destin}\")\n",
    "print(f\"TRAIN_SRC_DIR: {TRAIN_SRC_DIR}\")\n",
    "print(f\"VALID_SRC_DIR: {VALID_SRC_DIR}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           accelerators\n",
    "# =============================================\n",
    "# Instance configuration\n",
    "# GPU_LIMIT = '4'\n",
    "# GPU_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# CPU_LIMIT = '64'\n",
    "# MEMORY_LIMIT = '624G'\n",
    "# INSTANCE_TYPE = \"n1-highmem-64\"\n",
    "\n",
    "# Instance configuration\n",
    "GPU_LIMIT = '2'                   # 1\n",
    "GPU_TYPE = 'NVIDIA_TESLA_A100'\n",
    "CPU_LIMIT = '24'                  # '64' '96'\n",
    "MEMORY_LIMIT = '170G'              #'624G' | 680\n",
    "INSTANCE_TYPE = \"a2-highgpu-2g\"\n",
    "\n",
    "print(f\"GPU_LIMIT: {GPU_LIMIT}\")\n",
    "print(f\"GPU_TYPE: {GPU_TYPE}\")\n",
    "print(f\"CPU_LIMIT: {CPU_LIMIT}\")\n",
    "print(f\"MEMORY_LIMIT: {MEMORY_LIMIT}\")\n",
    "print(f\"INSTANCE_TYPE: {INSTANCE_TYPE}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           pipelines\n",
    "# =============================================\n",
    "VERSION = 'latest-12'\n",
    "APP = 'spotify'\n",
    "MODEL_DISPLAY_NAME = f'nvt-last5-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "# Pipeline definitions\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-{VERSION}'\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           custom image\n",
    "# =============================================\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt-preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = f'nvt' # 150\n",
    "\n",
    "print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"DOCKERNAME: {DOCKERNAME}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381dcfc7-6f7c-4427-8859-8375d92be415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/home/jupyter/spotify-merlin')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1776608-a23c-4aa5-acc3-2b965be1b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/process_pipes/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/config.py\n",
    "\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "#           Cloud Storage Directorires\n",
    "# =============================================\n",
    "BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "# =============================================\n",
    "#           Setup\n",
    "# =============================================\n",
    "VERSION = 'latest-12'\n",
    "APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "PROJECT_ID = \"hybrid-vertex\"\n",
    "REGION = \"us-central1\"\n",
    "VERTEX_SA = f\"vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# =============================================\n",
    "#           Artifacts\n",
    "# =============================================\n",
    "# MODEL_DISPLAY_NAME = f\"nvt-last5-{VERSION}\"\n",
    "# WORKSPACE = f\"gs://jt-merlin-scaling/nvt-last5-{VERSION}\"\n",
    "MODEL_DISPLAY_NAME = f'nvt-last5-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "NVT_IMAGE_URI = \"gcr.io/hybrid-vertex/nvt-preprocessing\"\n",
    "\n",
    "# =============================================\n",
    "#           Pipeline Configs\n",
    "# =============================================\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f\"nvt-parquet-{VERSION}\"\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = f\"gs://jt-merlin-scaling/{MODEL_DISPLAY_NAME}/{PREPROCESS_PARQUET_PIPELINE_NAME}\"\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-{VERSION}'\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "# INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"n1-highmem-64\")\n",
    "# CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"64\")\n",
    "# MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"624G\")\n",
    "# GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"4\")\n",
    "# GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_T4\")\n",
    "\n",
    "INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"a2-highgpu-2g\")\n",
    "CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"24\")\n",
    "MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"170G\")\n",
    "GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"2\")\n",
    "GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_A100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f0b96-b8df-4893-8bfc-83a508db9f4f",
   "metadata": {},
   "source": [
    "### check config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40adb058-a744-47fd-b68e-c224014d32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "VERSION: latest-12\n",
      "APP: spotify\n",
      "PROJECT_ID: hybrid-vertex\n",
      "REGION: us-central1\n",
      "VERTEX_SA: vertex-sa@hybrid-vertex.iam.gserviceaccount.com\n",
      "MODEL_DISPLAY_NAME: nvt-last5-latest-12\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-latest-12\n",
      "NVT_IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-latest-12\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-parquet-latest-12\n",
      "INSTANCE_TYPE: a2-highgpu-2g\n",
      "CPU_LIMIT: 24\n",
      "MEMORY_LIMIT: 170G\n",
      "GPU_LIMIT: 2\n",
      "GPU_TYPE: NVIDIA_TESLA_A100\n"
     ]
    }
   ],
   "source": [
    "from src.process_pipes import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9e4a9-4d2b-46e7-a51b-65b04ee6bcd6",
   "metadata": {},
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "981f2ba9-4e39-4873-96f9-81815d67784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/Dockerfile.nvt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "COPY preprocessor/* ./\n",
    "\n",
    "ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c7a71a5-e4d6-43b5-8c5e-32bbc3ffc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")\n",
    "\n",
    "MACHINE_TYPE ='e2-highcpu-32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "074c1600-7874-44a8-b1db-850882129e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/home/jupyter/spotify-merlin')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67384b3b-5a8d-4250-8910-c26188a3b191",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 63 file(s) totalling 1.7 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2023.02.23/18.58.09.022182.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1677178689.122931-d5f718771d004ab49024350136d0b8dd.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/c658b13d-6b5b-4d7c-8e8c-967c099f607b].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/c658b13d-6b5b-4d7c-8e8c-967c099f607b?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c658b13d-6b5b-4d7c-8e8c-967c099f607b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1677178689.122931-d5f718771d004ab49024350136d0b8dd.tgz#1677178689619614\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1677178689.122931-d5f718771d004ab49024350136d0b8dd.tgz#1677178689619614...\n",
      "/ [1 files][590.7 KiB/590.7 KiB]                                                \n",
      "Operation completed over 1 objects/590.7 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  152.6kB\n",
      "Step 1/8 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      "22.12: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "eaead16dc43b: Pulling fs layer\n",
      "d86e6ecee9ab: Pulling fs layer\n",
      "6b08e0981273: Pulling fs layer\n",
      "54de03fa67ca: Pulling fs layer\n",
      "d0067f8e4744: Pulling fs layer\n",
      "9e8473502f86: Pulling fs layer\n",
      "62cbcf4cbe13: Pulling fs layer\n",
      "9054d495b912: Pulling fs layer\n",
      "363a6cd12433: Pulling fs layer\n",
      "69633d945bde: Pulling fs layer\n",
      "030b5d54675a: Pulling fs layer\n",
      "282a25844090: Pulling fs layer\n",
      "51caa25c21ff: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "77bc21e94f24: Pulling fs layer\n",
      "784e1b6fd612: Pulling fs layer\n",
      "af1ee39433f4: Pulling fs layer\n",
      "52321cba559e: Pulling fs layer\n",
      "af00054fc370: Pulling fs layer\n",
      "0b6fc99b1680: Pulling fs layer\n",
      "6cee73498bf6: Pulling fs layer\n",
      "8b4cfd996372: Pulling fs layer\n",
      "b090c9ed50ff: Pulling fs layer\n",
      "0a8adb3dcf22: Pulling fs layer\n",
      "54de03fa67ca: Waiting\n",
      "6e88a7f6f15d: Pulling fs layer\n",
      "00f5ed83fb5b: Pulling fs layer\n",
      "e518f4c1fb4d: Pulling fs layer\n",
      "d0067f8e4744: Waiting\n",
      "8b6afde98e21: Pulling fs layer\n",
      "9e8473502f86: Waiting\n",
      "dcdde75d70d4: Pulling fs layer\n",
      "c131c32a822c: Pulling fs layer\n",
      "7ef98a0bb031: Pulling fs layer\n",
      "62cbcf4cbe13: Waiting\n",
      "7b2bf50cb482: Pulling fs layer\n",
      "7dd53e86e25e: Pulling fs layer\n",
      "228e876a170d: Pulling fs layer\n",
      "9054d495b912: Waiting\n",
      "2b8ce232b0f5: Pulling fs layer\n",
      "6acab9f9b6c9: Pulling fs layer\n",
      "363a6cd12433: Waiting\n",
      "49738298fc0b: Pulling fs layer\n",
      "69633d945bde: Waiting\n",
      "450e691b8934: Pulling fs layer\n",
      "c11b1ed1ee46: Pulling fs layer\n",
      "030b5d54675a: Waiting\n",
      "67e193f6923c: Pulling fs layer\n",
      "f3a3e4335302: Pulling fs layer\n",
      "282a25844090: Waiting\n",
      "31514946c388: Pulling fs layer\n",
      "4045938dbcf0: Pulling fs layer\n",
      "51caa25c21ff: Waiting\n",
      "ec3ab3614609: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "d59ff5de4fe3: Pulling fs layer\n",
      "c5e6195fcb5a: Pulling fs layer\n",
      "77bc21e94f24: Waiting\n",
      "b64eb74ea0f6: Pulling fs layer\n",
      "784e1b6fd612: Waiting\n",
      "fde2f9173aa1: Pulling fs layer\n",
      "05c48a632d89: Pulling fs layer\n",
      "af1ee39433f4: Waiting\n",
      "45643bc35dd6: Pulling fs layer\n",
      "dac08d81643b: Pulling fs layer\n",
      "52321cba559e: Waiting\n",
      "753e545517ec: Pulling fs layer\n",
      "af00054fc370: Waiting\n",
      "0b6fc99b1680: Waiting\n",
      "59fa5f6703c7: Pulling fs layer\n",
      "6cee73498bf6: Waiting\n",
      "c131c32a822c: Waiting\n",
      "5251fc0e145f: Pulling fs layer\n",
      "bdd807f81520: Pulling fs layer\n",
      "7ef98a0bb031: Waiting\n",
      "8b4cfd996372: Waiting\n",
      "cda9dd232241: Pulling fs layer\n",
      "7b2bf50cb482: Waiting\n",
      "c1c0778801ed: Pulling fs layer\n",
      "7dd53e86e25e: Waiting\n",
      "450e691b8934: Waiting\n",
      "228e876a170d: Waiting\n",
      "d2ac586dac26: Pulling fs layer\n",
      "c11b1ed1ee46: Waiting\n",
      "2b8ce232b0f5: Waiting\n",
      "c82a34ea3155: Pulling fs layer\n",
      "67e193f6923c: Waiting\n",
      "b090c9ed50ff: Waiting\n",
      "6acab9f9b6c9: Waiting\n",
      "07391a0f51be: Pulling fs layer\n",
      "f3a3e4335302: Waiting\n",
      "7e85ce497f3c: Pulling fs layer\n",
      "49738298fc0b: Waiting\n",
      "0a8adb3dcf22: Waiting\n",
      "c5e6195fcb5a: Waiting\n",
      "732c433b96b6: Pulling fs layer\n",
      "31514946c388: Waiting\n",
      "d59ff5de4fe3: Waiting\n",
      "b64eb74ea0f6: Waiting\n",
      "4045938dbcf0: Waiting\n",
      "6e88a7f6f15d: Waiting\n",
      "ec3ab3614609: Waiting\n",
      "fde2f9173aa1: Waiting\n",
      "de409ef6b855: Pulling fs layer\n",
      "36a7c7813429: Pulling fs layer\n",
      "cc688c082c64: Pulling fs layer\n",
      "92681808abed: Pulling fs layer\n",
      "8b6afde98e21: Waiting\n",
      "8b7d3b755746: Pulling fs layer\n",
      "07973a04f293: Pulling fs layer\n",
      "84b159fc1d60: Pulling fs layer\n",
      "161c50cb1ab8: Pulling fs layer\n",
      "dcdde75d70d4: Waiting\n",
      "5251fc0e145f: Waiting\n",
      "c1c0778801ed: Waiting\n",
      "59fa5f6703c7: Waiting\n",
      "cc688c082c64: Waiting\n",
      "07391a0f51be: Waiting\n",
      "92681808abed: Waiting\n",
      "d2ac586dac26: Waiting\n",
      "36a7c7813429: Waiting\n",
      "c82a34ea3155: Waiting\n",
      "de409ef6b855: Waiting\n",
      "7e85ce497f3c: Waiting\n",
      "45643bc35dd6: Waiting\n",
      "dac08d81643b: Waiting\n",
      "8b7d3b755746: Waiting\n",
      "3adfd786ac79: Pulling fs layer\n",
      "07973a04f293: Waiting\n",
      "b687b9abe298: Pulling fs layer\n",
      "00f5ed83fb5b: Waiting\n",
      "84b159fc1d60: Waiting\n",
      "4d55d43d5c97: Pulling fs layer\n",
      "161c50cb1ab8: Waiting\n",
      "2bda520db73d: Pulling fs layer\n",
      "3adfd786ac79: Waiting\n",
      "f006e76f83a3: Pulling fs layer\n",
      "7b8995abb34f: Pulling fs layer\n",
      "70651e98f007: Pulling fs layer\n",
      "2bda520db73d: Waiting\n",
      "7e50d9a71c79: Pulling fs layer\n",
      "fe9e4cebdbb5: Pulling fs layer\n",
      "0147547644c1: Pulling fs layer\n",
      "daffe4a11a5d: Pulling fs layer\n",
      "70651e98f007: Waiting\n",
      "fe9e4cebdbb5: Waiting\n",
      "7b8995abb34f: Waiting\n",
      "0147547644c1: Waiting\n",
      "05c48a632d89: Waiting\n",
      "e518f4c1fb4d: Waiting\n",
      "7e50d9a71c79: Waiting\n",
      "4d55d43d5c97: Waiting\n",
      "cda9dd232241: Waiting\n",
      "f006e76f83a3: Waiting\n",
      "753e545517ec: Waiting\n",
      "bdd807f81520: Waiting\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "54de03fa67ca: Verifying Checksum\n",
      "54de03fa67ca: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "6b08e0981273: Verifying Checksum\n",
      "6b08e0981273: Download complete\n",
      "9e8473502f86: Verifying Checksum\n",
      "9e8473502f86: Download complete\n",
      "d86e6ecee9ab: Verifying Checksum\n",
      "d86e6ecee9ab: Download complete\n",
      "62cbcf4cbe13: Verifying Checksum\n",
      "62cbcf4cbe13: Download complete\n",
      "9054d495b912: Verifying Checksum\n",
      "9054d495b912: Download complete\n",
      "363a6cd12433: Verifying Checksum\n",
      "363a6cd12433: Download complete\n",
      "69633d945bde: Verifying Checksum\n",
      "69633d945bde: Download complete\n",
      "282a25844090: Verifying Checksum\n",
      "282a25844090: Download complete\n",
      "030b5d54675a: Download complete\n",
      "4f4fb700ef54: Download complete\n",
      "77bc21e94f24: Download complete\n",
      "784e1b6fd612: Verifying Checksum\n",
      "784e1b6fd612: Download complete\n",
      "af1ee39433f4: Verifying Checksum\n",
      "af1ee39433f4: Download complete\n",
      "d86e6ecee9ab: Pull complete\n",
      "6b08e0981273: Pull complete\n",
      "54de03fa67ca: Pull complete\n",
      "51caa25c21ff: Verifying Checksum\n",
      "51caa25c21ff: Download complete\n",
      "af00054fc370: Verifying Checksum\n",
      "af00054fc370: Download complete\n",
      "0b6fc99b1680: Verifying Checksum\n",
      "0b6fc99b1680: Download complete\n",
      "6cee73498bf6: Verifying Checksum\n",
      "6cee73498bf6: Download complete\n",
      "52321cba559e: Verifying Checksum\n",
      "52321cba559e: Download complete\n",
      "b090c9ed50ff: Verifying Checksum\n",
      "b090c9ed50ff: Download complete\n",
      "0a8adb3dcf22: Verifying Checksum\n",
      "0a8adb3dcf22: Download complete\n",
      "6e88a7f6f15d: Verifying Checksum\n",
      "6e88a7f6f15d: Download complete\n",
      "00f5ed83fb5b: Verifying Checksum\n",
      "00f5ed83fb5b: Download complete\n",
      "e518f4c1fb4d: Verifying Checksum\n",
      "e518f4c1fb4d: Download complete\n",
      "8b6afde98e21: Verifying Checksum\n",
      "8b6afde98e21: Download complete\n",
      "dcdde75d70d4: Verifying Checksum\n",
      "dcdde75d70d4: Download complete\n",
      "c131c32a822c: Download complete\n",
      "7ef98a0bb031: Verifying Checksum\n",
      "7ef98a0bb031: Download complete\n",
      "7b2bf50cb482: Download complete\n",
      "7dd53e86e25e: Verifying Checksum\n",
      "7dd53e86e25e: Download complete\n",
      "228e876a170d: Verifying Checksum\n",
      "228e876a170d: Download complete\n",
      "2b8ce232b0f5: Verifying Checksum\n",
      "2b8ce232b0f5: Download complete\n",
      "6acab9f9b6c9: Verifying Checksum\n",
      "6acab9f9b6c9: Download complete\n",
      "49738298fc0b: Verifying Checksum\n",
      "49738298fc0b: Download complete\n",
      "450e691b8934: Download complete\n",
      "c11b1ed1ee46: Download complete\n",
      "67e193f6923c: Verifying Checksum\n",
      "67e193f6923c: Download complete\n",
      "8b4cfd996372: Verifying Checksum\n",
      "8b4cfd996372: Download complete\n",
      "f3a3e4335302: Verifying Checksum\n",
      "f3a3e4335302: Download complete\n",
      "31514946c388: Verifying Checksum\n",
      "31514946c388: Download complete\n",
      "ec3ab3614609: Download complete\n",
      "4045938dbcf0: Verifying Checksum\n",
      "4045938dbcf0: Download complete\n",
      "d59ff5de4fe3: Download complete\n",
      "c5e6195fcb5a: Download complete\n",
      "fde2f9173aa1: Verifying Checksum\n",
      "fde2f9173aa1: Download complete\n",
      "b64eb74ea0f6: Download complete\n",
      "05c48a632d89: Verifying Checksum\n",
      "05c48a632d89: Download complete\n",
      "dac08d81643b: Verifying Checksum\n",
      "dac08d81643b: Download complete\n",
      "45643bc35dd6: Verifying Checksum\n",
      "45643bc35dd6: Download complete\n",
      "753e545517ec: Verifying Checksum\n",
      "753e545517ec: Download complete\n",
      "59fa5f6703c7: Verifying Checksum\n",
      "59fa5f6703c7: Download complete\n",
      "5251fc0e145f: Verifying Checksum\n",
      "5251fc0e145f: Download complete\n",
      "bdd807f81520: Download complete\n",
      "cda9dd232241: Verifying Checksum\n",
      "cda9dd232241: Download complete\n",
      "c1c0778801ed: Verifying Checksum\n",
      "c1c0778801ed: Download complete\n",
      "d2ac586dac26: Verifying Checksum\n",
      "d2ac586dac26: Download complete\n",
      "c82a34ea3155: Verifying Checksum\n",
      "c82a34ea3155: Download complete\n",
      "7e85ce497f3c: Verifying Checksum\n",
      "7e85ce497f3c: Download complete\n",
      "732c433b96b6: Verifying Checksum\n",
      "732c433b96b6: Download complete\n",
      "de409ef6b855: Download complete\n",
      "07391a0f51be: Download complete\n",
      "36a7c7813429: Verifying Checksum\n",
      "36a7c7813429: Download complete\n",
      "cc688c082c64: Verifying Checksum\n",
      "cc688c082c64: Download complete\n",
      "92681808abed: Verifying Checksum\n",
      "92681808abed: Download complete\n",
      "8b7d3b755746: Verifying Checksum\n",
      "8b7d3b755746: Download complete\n",
      "84b159fc1d60: Verifying Checksum\n",
      "84b159fc1d60: Download complete\n",
      "161c50cb1ab8: Verifying Checksum\n",
      "161c50cb1ab8: Download complete\n",
      "3adfd786ac79: Verifying Checksum\n",
      "3adfd786ac79: Download complete\n",
      "b687b9abe298: Download complete\n",
      "07973a04f293: Verifying Checksum\n",
      "07973a04f293: Download complete\n",
      "4d55d43d5c97: Verifying Checksum\n",
      "4d55d43d5c97: Download complete\n",
      "f006e76f83a3: Verifying Checksum\n",
      "f006e76f83a3: Download complete\n",
      "2bda520db73d: Verifying Checksum\n",
      "2bda520db73d: Download complete\n",
      "7b8995abb34f: Verifying Checksum\n",
      "7b8995abb34f: Download complete\n",
      "7e50d9a71c79: Verifying Checksum\n",
      "7e50d9a71c79: Download complete\n",
      "70651e98f007: Verifying Checksum\n",
      "70651e98f007: Download complete\n",
      "fe9e4cebdbb5: Verifying Checksum\n",
      "fe9e4cebdbb5: Download complete\n",
      "daffe4a11a5d: Verifying Checksum\n",
      "daffe4a11a5d: Download complete\n",
      "d0067f8e4744: Verifying Checksum\n",
      "d0067f8e4744: Download complete\n",
      "0147547644c1: Verifying Checksum\n",
      "0147547644c1: Download complete\n",
      "d0067f8e4744: Pull complete\n",
      "9e8473502f86: Pull complete\n",
      "62cbcf4cbe13: Pull complete\n",
      "9054d495b912: Pull complete\n",
      "363a6cd12433: Pull complete\n",
      "69633d945bde: Pull complete\n",
      "030b5d54675a: Pull complete\n",
      "282a25844090: Pull complete\n",
      "51caa25c21ff: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "77bc21e94f24: Pull complete\n",
      "784e1b6fd612: Pull complete\n",
      "af1ee39433f4: Pull complete\n",
      "52321cba559e: Pull complete\n",
      "af00054fc370: Pull complete\n",
      "0b6fc99b1680: Pull complete\n",
      "6cee73498bf6: Pull complete\n",
      "8b4cfd996372: Pull complete\n",
      "b090c9ed50ff: Pull complete\n",
      "0a8adb3dcf22: Pull complete\n",
      "6e88a7f6f15d: Pull complete\n",
      "00f5ed83fb5b: Pull complete\n",
      "e518f4c1fb4d: Pull complete\n",
      "8b6afde98e21: Pull complete\n",
      "dcdde75d70d4: Pull complete\n",
      "c131c32a822c: Pull complete\n",
      "7ef98a0bb031: Pull complete\n",
      "7b2bf50cb482: Pull complete\n",
      "7dd53e86e25e: Pull complete\n",
      "228e876a170d: Pull complete\n",
      "2b8ce232b0f5: Pull complete\n",
      "6acab9f9b6c9: Pull complete\n",
      "49738298fc0b: Pull complete\n",
      "450e691b8934: Pull complete\n",
      "c11b1ed1ee46: Pull complete\n",
      "67e193f6923c: Pull complete\n",
      "f3a3e4335302: Pull complete\n",
      "31514946c388: Pull complete\n",
      "4045938dbcf0: Pull complete\n",
      "ec3ab3614609: Pull complete\n",
      "d59ff5de4fe3: Pull complete\n",
      "c5e6195fcb5a: Pull complete\n",
      "b64eb74ea0f6: Pull complete\n",
      "fde2f9173aa1: Pull complete\n",
      "05c48a632d89: Pull complete\n",
      "45643bc35dd6: Pull complete\n",
      "dac08d81643b: Pull complete\n",
      "753e545517ec: Pull complete\n",
      "59fa5f6703c7: Pull complete\n",
      "5251fc0e145f: Pull complete\n",
      "bdd807f81520: Pull complete\n",
      "cda9dd232241: Pull complete\n",
      "c1c0778801ed: Pull complete\n",
      "d2ac586dac26: Pull complete\n",
      "c82a34ea3155: Pull complete\n",
      "07391a0f51be: Pull complete\n",
      "7e85ce497f3c: Pull complete\n",
      "732c433b96b6: Pull complete\n",
      "de409ef6b855: Pull complete\n",
      "36a7c7813429: Pull complete\n",
      "cc688c082c64: Pull complete\n",
      "92681808abed: Pull complete\n",
      "8b7d3b755746: Pull complete\n",
      "07973a04f293: Pull complete\n",
      "84b159fc1d60: Pull complete\n",
      "161c50cb1ab8: Pull complete\n",
      "3adfd786ac79: Pull complete\n",
      "b687b9abe298: Pull complete\n",
      "4d55d43d5c97: Pull complete\n",
      "2bda520db73d: Pull complete\n",
      "f006e76f83a3: Pull complete\n",
      "7b8995abb34f: Pull complete\n",
      "70651e98f007: Pull complete\n",
      "7e50d9a71c79: Pull complete\n",
      "fe9e4cebdbb5: Pull complete\n",
      "0147547644c1: Pull complete\n",
      "daffe4a11a5d: Pull complete\n",
      "Digest: sha256:f1714efb94467b1b8f2dfadb27f4b6703568b616ceba85dec4df36dc581c339e\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      " ---> 58c045557af1\n",
      "Step 2/8 : WORKDIR /src\n",
      " ---> Running in d1b1d66030ee\n",
      "Removing intermediate container d1b1d66030ee\n",
      " ---> 35d5c6d4f698\n",
      "Step 3/8 : RUN pip install -U pip\n",
      " ---> Running in f8eecced0f08\n",
      "Collecting pip\n",
      "  Downloading pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'pip'. No files were found to uninstall.\n",
      "Successfully installed pip-23.0.1\n",
      "Removing intermediate container f8eecced0f08\n",
      " ---> aafb42c78ec4\n",
      "Step 4/8 : RUN pip install google-cloud-bigquery gcsfs\n",
      " ---> Running in fddc532f2ffc\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.6.0-py2.py3-none-any.whl (215 kB)\n",
      "      215.1/215.1 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2023.1.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.15.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "      47.9/47.9 kB 7.9 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "      4.8/4.8 MB 72.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (22.0)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.6.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.28.1)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "      77.7/77.7 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (3.19.6)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "      120.3/120.3 kB 20.0 MB/s eta 0:00:00\n",
      "Collecting fsspec==2023.1.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "      143.0/143.0 kB 22.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "      110.2/110.2 kB 18.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.15.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.57.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2019.11.28)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.0-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "      302.4/302.4 kB 34.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.2)\n",
      "Installing collected packages: protobuf, grpcio, google-crc32c, fsspec, proto-plus, google-resumable-media, grpcio-status, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-bigquery, gcsfs\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "tensorflow 2.10.0+nv22.11 requires flatbuffers>=2.0, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow 2.10.0+nv22.11 requires protobuf<4.0.0,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\n",
      "tensorflow 2.10.0+nv22.11 requires tensorboard<2.11,>=2.10, but you have tensorboard 2.9.1 which is incompatible.\n",
      "tensorflow 2.10.0+nv22.11 requires tensorflow-estimator<2.11,>=2.10.0, but you have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "tensorflow-metadata 1.12.0 requires protobuf<4,>=3.13, but you have protobuf 4.22.0 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\n",
      "merlin-core 0.10.0 requires fsspec==2022.5.0, but you have fsspec 2023.1.0 which is incompatible.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cuda-python<11.7.1,>=11.5, but you have cuda-python 11.8.1 which is incompatible.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires protobuf<3.21.0a0,>=3.20.1, but you have protobuf 4.22.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed fsspec-2023.1.0 gcsfs-2023.1.0 google-api-core-2.11.0 google-cloud-bigquery-3.6.0 google-cloud-core-2.3.2 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 grpcio-1.51.3 grpcio-status-1.51.3 proto-plus-1.22.2 protobuf-4.22.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container fddc532f2ffc\n",
      " ---> 519de33c4336\n",
      "Step 5/8 : RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular\n",
      " ---> Running in ba3316d28cdc\n",
      "Collecting google-cloud-aiplatform[cloud_profiler]\n",
      "  Downloading google_cloud_aiplatform-1.22.0-py2.py3-none-any.whl (2.4 MB)\n",
      "      2.4/2.4 MB 39.1 MB/s eta 0:00:00\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.19.tar.gz (304 kB)\n",
      "      304.8/304.8 kB 34.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nvtabular in /usr/local/lib/python3.8/dist-packages (1.8.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.11.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (4.22.0)\n",
      "Collecting shapely<2.0.0\n",
      "  Downloading Shapely-1.8.5.post1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "      2.1/2.1 MB 60.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.7.0)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "      40.8/40.8 kB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (3.6.0)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.8.1-py2.py3-none-any.whl (235 kB)\n",
      "      235.7/235.7 kB 28.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tensorflow<3.0.0dev,>=2.4.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.10.0+nv22.11)\n",
      "Collecting werkzeug<2.1.0dev,>=2.0.0\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "      289.2/289.2 kB 33.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-profile<3.0.0dev,>=2.4.0\n",
      "  Downloading tensorboard_plugin_profile-2.11.1-py3-none-any.whl (5.4 MB)\n",
      "      5.4/5.4 MB 86.9 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "      662.4/662.4 kB 52.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.8/dist-packages (from kfp) (1.3.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.2.0)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
      "      88.3/88.3 kB 12.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "      62.1/62.1 kB 9.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.15.0)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.17.3)\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "      58.1/58.1 kB 8.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<20,>=8.0.0\n",
      "  Downloading kubernetes-19.15.0-py2.py3-none-any.whl (1.7 MB)\n",
      "      1.7/1.7 MB 73.8 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "      1.0/1.0 MB 66.3 MB/s eta 0:00:00\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "      3.2/3.2 MB 93.0 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "      54.5/54.5 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.4.0)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nvtabular) (0.10.0)\n",
      "Requirement already satisfied: merlin-dataloader>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from nvtabular) (0.0.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from nvtabular) (1.9.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire<1,>=0.3.1->kfp) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire<1,>=0.3.1->kfp) (2.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.57.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (2.28.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.51.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.51.3)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.21.0-py3-none-any.whl (96 kB)\n",
      "      96.8/96.8 kB 15.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.4.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.3.2)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (5.10.2)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (22.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (0.19.3)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /usr/local/lib/python3.8/dist-packages (from jsonschema<5,>=3.0.1->kfp) (1.3.10)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.13)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<20,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /usr/lib/python3/dist-packages (from kubernetes<20,>=8.0.0->kfp) (45.2.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from kubernetes<20,>=8.0.0->kfp) (1.4.2)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (2022.7.1)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (1.12.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (0.56.4)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (1.3.5)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "      140.6/140.6 kB 21.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (4.64.1)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (1.2.5)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (2022.7.1)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular) (8.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform[cloud_profiler]) (3.0.9)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.34.2)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (14.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.3.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (2.10.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.7.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.22.4)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "      5.9/5.9 MB 79.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.1.2)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "      438.7/438.7 kB 40.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (0.4.3)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular) (0.12.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (1.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (2.2.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (5.9.4)\n",
      "Requirement already satisfied: tornado<6.2,>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (6.1)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (1.7.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (1.0.0)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (2.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (3.1.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (1.5.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl (5.1 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema<5,>=3.0.1->kfp) (3.11.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular) (5.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular) (2022.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (2.8)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Using cached protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.4.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<20,>=8.0.0->kfp) (3.2.2)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (1.0.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (6.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular) (2.1.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular) (4.0.0)\n",
      "Building wheels for collected packages: kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.19-py3-none-any.whl size=426975 sha256=c8e9dcd45edd0e95065800ee35578c6521011490dfa321a8a2d57ad196c844b4\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/48/49/09cd2b5fbd81315c167c7d58877eaaf825e8f9ff1d733ca618\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=1fe2b40ea6ca991c74fa087b10c8afc47311423bfc41e4e1351aa315ba2cf766\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=89bbfbb2e40c1cdea57f59368a3a055785b2117d3958dc5b870e5e0938167e4c\n",
      "  Stored in directory: /root/.cache/pip/wheels/93/b7/87/8884b574029455610a5b99752115d2ac857f8cfe8b846a1225\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=22471bca9195ae520f3b1240c46c620eececae958b034d31021348343d570521\n",
      "  Stored in directory: /root/.cache/pip/wheels/28/16/9b/7c21f4d08b98d02819658600b738d3453b2ffee3c9b757629e\n",
      "Successfully built kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: flatbuffers, werkzeug, uritemplate, typer, tensorflow-estimator, tabulate, strip-hints, shapely, PyYAML, pydantic, protobuf, packaging, httplib2, gviz-api, fsspec, fire, docstring-parser, Deprecated, tensorboard-plugin-profile, requests-toolbelt, kfp-server-api, kfp-pipeline-spec, kubernetes, grpcio-status, google-auth-httplib2, tensorboard, grpc-google-iam-v1, google-api-python-client, google-cloud-resource-manager, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.2.2\n",
      "    Uninstalling Werkzeug-2.2.2:\n",
      "      Successfully uninstalled Werkzeug-2.2.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.0\n",
      "    Uninstalling protobuf-4.22.0:\n",
      "      Successfully uninstalled protobuf-4.22.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.51.3\n",
      "    Uninstalling grpcio-status-1.51.3:\n",
      "      Successfully uninstalled grpcio-status-1.51.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "jupyter-server 2.0.6 requires tornado>=6.2.0, but you have tornado 6.1 which is incompatible.\n",
      "grpcio-channelz 1.51.1 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
      "gcsfs 2023.1.0 requires fsspec==2023.1.0, but you have fsspec 2022.5.0 which is incompatible.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cuda-python<11.7.1,>=11.5, but you have cuda-python 11.8.1 which is incompatible.\n",
      "cudf 22.8.0a0+304.g6ca81bbc78.dirty requires protobuf<3.21.0a0,>=3.20.1, but you have protobuf 3.19.6 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 docstring-parser-0.15 fire-0.5.0 flatbuffers-23.1.21 fsspec-2022.5.0 google-api-python-client-1.12.11 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.22.0 google-cloud-resource-manager-1.8.1 grpc-google-iam-v1-0.12.6 grpcio-status-1.48.2 gviz-api-1.10.0 httplib2-0.21.0 kfp-1.8.19 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-19.15.0 packaging-21.3 protobuf-3.19.6 pydantic-1.10.5 requests-toolbelt-0.10.1 shapely-1.8.5.post1 strip-hints-0.1.10 tabulate-0.9.0 tensorboard-2.10.1 tensorboard-plugin-profile-2.11.1 tensorflow-estimator-2.10.0 typer-0.7.0 uritemplate-3.0.1 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container ba3316d28cdc\n",
      " ---> 6de6701fb7c5\n",
      "Step 6/8 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      " ---> Running in 4349d89efab8\n",
      "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1210  100  1210    0     0  60500      0 --:--:-- --:--:-- --:--:-- 60500:-\u001b[0m\u001b[91m-     0\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [397 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [871 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1299 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2970 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1937 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2066 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2496 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [998 kB]\n",
      "Fetched 26.6 MB in 3s (9467 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "Suggested packages:\n",
      "  google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-python\n",
      "  google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator\n",
      "  google-cloud-sdk-datastore-emulator kubectl\n",
      "The following NEW packages will be installed:\n",
      "  google-cloud-sdk\n",
      "0 upgraded, 1 newly installed, 0 to remove and 74 not upgraded.\n",
      "Need to get 157 MB of archives.\n",
      "After this operation, 816 MB of additional disk space will be used.\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk/main amd64 google-cloud-sdk all 419.0.0-0 [157 MB]\n",
      "Fetched 157 MB in 3s (57.6 MB/s)\n",
      "Selecting previously unselected package google-cloud-sdk.\n",
      "(Reading database ... 41590 files and directories currently installed.)\n",
      "Preparing to unpack .../google-cloud-sdk_419.0.0-0_all.deb ...\n",
      "Unpacking google-cloud-sdk (419.0.0-0) ...\n",
      "Setting up google-cloud-sdk (419.0.0-0) ...\n",
      "Removing intermediate container 4349d89efab8\n",
      " ---> 97095983e8d1\n",
      "Step 7/8 : COPY preprocessor/* ./\n",
      " ---> 0c2fb9040b81\n",
      "Step 8/8 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      " ---> Running in b82fa4f0d98c\n",
      "Removing intermediate container b82fa4f0d98c\n",
      " ---> 0a060ac04156\n",
      "Successfully built 0a060ac04156\n",
      "Successfully tagged gcr.io/hybrid-vertex/nvt-preprocessing:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "The push refers to repository [gcr.io/hybrid-vertex/nvt-preprocessing]\n",
      "75e1688fd42e: Preparing\n",
      "0f27bd87fd06: Preparing\n",
      "777a871dc609: Preparing\n",
      "07ad89260393: Preparing\n",
      "96daf0a7e039: Preparing\n",
      "eac909a0118d: Preparing\n",
      "2f4c2e747ad7: Preparing\n",
      "f5b846caf956: Preparing\n",
      "2f4c2e747ad7: Waiting\n",
      "eac909a0118d: Waiting\n",
      "12eec4f4a731: Preparing\n",
      "67516f8ac904: Preparing\n",
      "f5b846caf956: Waiting\n",
      "4c0655970ccc: Preparing\n",
      "73101e82e910: Preparing\n",
      "f4d87c23e98f: Preparing\n",
      "d07270ded861: Preparing\n",
      "67516f8ac904: Waiting\n",
      "8187da794091: Preparing\n",
      "d7ba493f5d44: Preparing\n",
      "ebd7caa9de82: Preparing\n",
      "242c03ad2a31: Preparing\n",
      "df5d5e1b9e48: Preparing\n",
      "3951f4addbfc: Preparing\n",
      "d07270ded861: Waiting\n",
      "73101e82e910: Waiting\n",
      "2e5281e7ae87: Preparing\n",
      "4c0655970ccc: Waiting\n",
      "42ed3d5ddcb4: Preparing\n",
      "f4d87c23e98f: Waiting\n",
      "8187da794091: Waiting\n",
      "383fa992b719: Preparing\n",
      "e47df10e020a: Preparing\n",
      "317e7f70fff9: Preparing\n",
      "b45c2562fda1: Preparing\n",
      "df5d5e1b9e48: Waiting\n",
      "0511decbb6f6: Preparing\n",
      "3951f4addbfc: Waiting\n",
      "ac6f1cea2b76: Preparing\n",
      "d7ba493f5d44: Waiting\n",
      "d5f7fcaf77a9: Preparing\n",
      "bf10e31ca3c7: Preparing\n",
      "f1b1f58ddb35: Preparing\n",
      "2e5281e7ae87: Waiting\n",
      "317e7f70fff9: Waiting\n",
      "71fe61786022: Preparing\n",
      "20d51f7b3d75: Preparing\n",
      "42ed3d5ddcb4: Waiting\n",
      "4357e5c17b0b: Preparing\n",
      "b45c2562fda1: Waiting\n",
      "383fa992b719: Waiting\n",
      "38dd15f32335: Preparing\n",
      "1014fb54d10a: Preparing\n",
      "ebd7caa9de82: Waiting\n",
      "0511decbb6f6: Waiting\n",
      "f4c090be1950: Preparing\n",
      "bfc784fa095a: Preparing\n",
      "1f09cab8959e: Preparing\n",
      "e47df10e020a: Waiting\n",
      "ac6f1cea2b76: Waiting\n",
      "242c03ad2a31: Waiting\n",
      "ea787d764727: Preparing\n",
      "04b7d4d05916: Preparing\n",
      "d5f7fcaf77a9: Waiting\n",
      "3ace34553782: Preparing\n",
      "71fe61786022: Waiting\n",
      "182ee86689d1: Preparing\n",
      "4357e5c17b0b: Waiting\n",
      "bf10e31ca3c7: Waiting\n",
      "bbdb6055545b: Preparing\n",
      "9a161f9c0e0a: Preparing\n",
      "b7ad75a3e25e: Preparing\n",
      "7752ee8ae666: Preparing\n",
      "13f264ca1ed9: Preparing\n",
      "cd870b4929c1: Preparing\n",
      "d092d49243b5: Preparing\n",
      "20d51f7b3d75: Waiting\n",
      "aafb34ceba88: Preparing\n",
      "32710f0c03c7: Preparing\n",
      "6a55125f046a: Preparing\n",
      "1014fb54d10a: Waiting\n",
      "04b7d4d05916: Waiting\n",
      "bbdb6055545b: Waiting\n",
      "f4c090be1950: Waiting\n",
      "2af1bd207a0a: Preparing\n",
      "bfc784fa095a: Waiting\n",
      "3ace34553782: Waiting\n",
      "9a161f9c0e0a: Waiting\n",
      "ef25bc892765: Preparing\n",
      "ea787d764727: Waiting\n",
      "182ee86689d1: Waiting\n",
      "e5171eae2464: Preparing\n",
      "f7bcdf2e75c1: Preparing\n",
      "b7ad75a3e25e: Waiting\n",
      "1f09cab8959e: Waiting\n",
      "cd870b4929c1: Waiting\n",
      "ef257ce50d4c: Preparing\n",
      "d7d019fd6cda: Preparing\n",
      "bf08ecf416c0: Preparing\n",
      "19be279686b3: Preparing\n",
      "a42b9d3ddcc4: Preparing\n",
      "8a1a337e69bc: Preparing\n",
      "ca6dd165751d: Preparing\n",
      "ac02b4761a81: Preparing\n",
      "4be672edea27: Preparing\n",
      "d092d49243b5: Waiting\n",
      "3041a215c94e: Preparing\n",
      "db85811a91ed: Preparing\n",
      "aafb34ceba88: Waiting\n",
      "6eb00409a05c: Preparing\n",
      "fb3ac13a8afd: Preparing\n",
      "e614e8f11c1c: Preparing\n",
      "70d54afe53fd: Preparing\n",
      "cbbf130153a1: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "7173bbe6139c: Preparing\n",
      "23076d2c79f0: Preparing\n",
      "ed1a31ee03b7: Preparing\n",
      "32710f0c03c7: Waiting\n",
      "fe17ea05bcc3: Preparing\n",
      "3204b933fb0b: Preparing\n",
      "4616162d4e6a: Preparing\n",
      "9a03c5ba8f95: Preparing\n",
      "6a55125f046a: Waiting\n",
      "764cbe4d1ae6: Preparing\n",
      "03a82e76641b: Preparing\n",
      "f874925be13d: Preparing\n",
      "2c4e7d9e38c0: Preparing\n",
      "67abb95254ee: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "d7d019fd6cda: Waiting\n",
      "2af1bd207a0a: Waiting\n",
      "ef257ce50d4c: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "f7bcdf2e75c1: Waiting\n",
      "bf08ecf416c0: Waiting\n",
      "7173bbe6139c: Waiting\n",
      "a42b9d3ddcc4: Waiting\n",
      "23076d2c79f0: Waiting\n",
      "db85811a91ed: Waiting\n",
      "8a1a337e69bc: Waiting\n",
      "6eb00409a05c: Waiting\n",
      "ca6dd165751d: Waiting\n",
      "fb3ac13a8afd: Waiting\n",
      "ac02b4761a81: Waiting\n",
      "ed1a31ee03b7: Waiting\n",
      "4be672edea27: Waiting\n",
      "e614e8f11c1c: Waiting\n",
      "f1b1f58ddb35: Waiting\n",
      "fe17ea05bcc3: Waiting\n",
      "70d54afe53fd: Waiting\n",
      "3041a215c94e: Waiting\n",
      "cbbf130153a1: Waiting\n",
      "2c4e7d9e38c0: Waiting\n",
      "764cbe4d1ae6: Waiting\n",
      "4616162d4e6a: Waiting\n",
      "f874925be13d: Waiting\n",
      "03a82e76641b: Waiting\n",
      "67abb95254ee: Waiting\n",
      "e5171eae2464: Waiting\n",
      "f4462d5b2da2: Waiting\n",
      "ef25bc892765: Waiting\n",
      "7752ee8ae666: Waiting\n",
      "75e1688fd42e: Pushed\n",
      "96daf0a7e039: Pushed\n",
      "2f4c2e747ad7: Layer already exists\n",
      "f5b846caf956: Layer already exists\n",
      "12eec4f4a731: Layer already exists\n",
      "67516f8ac904: Layer already exists\n",
      "4c0655970ccc: Layer already exists\n",
      "73101e82e910: Layer already exists\n",
      "f4d87c23e98f: Layer already exists\n",
      "eac909a0118d: Pushed\n",
      "d07270ded861: Layer already exists\n",
      "8187da794091: Layer already exists\n",
      "d7ba493f5d44: Layer already exists\n",
      "ebd7caa9de82: Layer already exists\n",
      "242c03ad2a31: Layer already exists\n",
      "3951f4addbfc: Layer already exists\n",
      "2e5281e7ae87: Layer already exists\n",
      "df5d5e1b9e48: Layer already exists\n",
      "42ed3d5ddcb4: Layer already exists\n",
      "383fa992b719: Layer already exists\n",
      "317e7f70fff9: Layer already exists\n",
      "e47df10e020a: Layer already exists\n",
      "b45c2562fda1: Layer already exists\n",
      "ac6f1cea2b76: Layer already exists\n",
      "0511decbb6f6: Layer already exists\n",
      "bf10e31ca3c7: Layer already exists\n",
      "07ad89260393: Pushed\n",
      "d5f7fcaf77a9: Layer already exists\n",
      "f1b1f58ddb35: Layer already exists\n",
      "71fe61786022: Layer already exists\n",
      "20d51f7b3d75: Layer already exists\n",
      "4357e5c17b0b: Layer already exists\n",
      "38dd15f32335: Layer already exists\n",
      "1014fb54d10a: Layer already exists\n",
      "bfc784fa095a: Layer already exists\n",
      "f4c090be1950: Layer already exists\n",
      "1f09cab8959e: Layer already exists\n",
      "04b7d4d05916: Layer already exists\n",
      "3ace34553782: Layer already exists\n",
      "ea787d764727: Layer already exists\n",
      "182ee86689d1: Layer already exists\n",
      "bbdb6055545b: Layer already exists\n",
      "9a161f9c0e0a: Layer already exists\n",
      "b7ad75a3e25e: Layer already exists\n",
      "7752ee8ae666: Layer already exists\n",
      "cd870b4929c1: Layer already exists\n",
      "13f264ca1ed9: Layer already exists\n",
      "aafb34ceba88: Layer already exists\n",
      "d092d49243b5: Layer already exists\n",
      "32710f0c03c7: Layer already exists\n",
      "6a55125f046a: Layer already exists\n",
      "ef25bc892765: Layer already exists\n",
      "2af1bd207a0a: Layer already exists\n",
      "e5171eae2464: Layer already exists\n",
      "f7bcdf2e75c1: Layer already exists\n",
      "ef257ce50d4c: Layer already exists\n",
      "d7d019fd6cda: Layer already exists\n",
      "19be279686b3: Layer already exists\n",
      "bf08ecf416c0: Layer already exists\n",
      "a42b9d3ddcc4: Layer already exists\n",
      "8a1a337e69bc: Layer already exists\n",
      "ca6dd165751d: Layer already exists\n",
      "ac02b4761a81: Layer already exists\n",
      "4be672edea27: Layer already exists\n",
      "3041a215c94e: Layer already exists\n",
      "6eb00409a05c: Layer already exists\n",
      "db85811a91ed: Layer already exists\n",
      "fb3ac13a8afd: Layer already exists\n",
      "e614e8f11c1c: Layer already exists\n",
      "70d54afe53fd: Layer already exists\n",
      "cbbf130153a1: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "7173bbe6139c: Layer already exists\n",
      "ed1a31ee03b7: Layer already exists\n",
      "23076d2c79f0: Layer already exists\n",
      "fe17ea05bcc3: Layer already exists\n",
      "3204b933fb0b: Layer already exists\n",
      "4616162d4e6a: Layer already exists\n",
      "9a03c5ba8f95: Layer already exists\n",
      "777a871dc609: Pushed\n",
      "764cbe4d1ae6: Layer already exists\n",
      "03a82e76641b: Layer already exists\n",
      "f874925be13d: Layer already exists\n",
      "2c4e7d9e38c0: Layer already exists\n",
      "67abb95254ee: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "0f27bd87fd06: Pushed\n",
      "latest: digest: sha256:f192b27bddff157c178f021ce1c0b260fe3d82ad732042501272f0fdfbe38353 size: 18626\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                            STATUS\n",
      "c658b13d-6b5b-4d7c-8e8c-967c099f607b  2023-02-23T18:58:09+00:00  7M35S     gs://hybrid-vertex_cloudbuild/source/1677178689.122931-d5f718771d004ab49024350136d0b8dd.tgz  gcr.io/hybrid-vertex/nvt-preprocessing (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fd2ca-1003-4c1f-badf-1a78b6e4757c",
   "metadata": {},
   "source": [
    "# Vertex Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b940a83-6d48-4db9-9921-50e081bd142d",
   "metadata": {},
   "source": [
    "### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74544409-014c-45ff-85f0-538d989a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_limit': '2',\n",
      " 'gpu_type': 'nvidia_tesla_a100',\n",
      " 'instance_type': 'a2-highgpu-2g',\n",
      " 'memory_limit': '170g',\n",
      " 'version': 'latest-12'}\n"
     ]
    }
   ],
   "source": [
    "LABELS = {\n",
    "    'version': f'{VERSION}',\n",
    "    'gpu_type': f'{GPU_TYPE.lower()}',\n",
    "    'gpu_limit': f'{GPU_LIMIT}',\n",
    "    'memory_limit': f'{MEMORY_LIMIT.lower()}',\n",
    "    'instance_type': f'{INSTANCE_TYPE}',\n",
    "}\n",
    "pprint(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e34e6-9891-421e-a62d-2e6e49a66a44",
   "metadata": {},
   "source": [
    "## define pipe params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "105398a1-be24-4c85-94d9-fdfa45ee0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': 'spotify',\n",
      " 'bucket_data_output': 'jt-merlin-scaling',\n",
      " 'bucket_data_src': 'spotify-beam-v3',\n",
      " 'file_pattern': '*.snappy.parquet',\n",
      " 'num_output_files_train': 100,\n",
      " 'num_output_files_valid': 10,\n",
      " 'output_path_analyzed_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed',\n",
      " 'output_path_defined_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-defined',\n",
      " 'output_path_transformed_dir': 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed',\n",
      " 'shuffle': 'null',\n",
      " 'train_prefix': 'train_data_parquet',\n",
      " 'valid_prefix': 'valid_data_parquet',\n",
      " 'version': 'latest-12'}\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import storage\n",
    "\n",
    "# storage_client = storage.Client()\n",
    "\n",
    "TRAIN_PREFIX = 'train_data_parquet'\n",
    "VALID_PREFIX = 'valid_data_parquet'\n",
    "\n",
    "delimiter = '/'\n",
    "# FILE_PATTERN = \"*.parquet\"                    # full\n",
    "FILE_PATTERN = '*.snappy.parquet'    # subset\n",
    "\n",
    "# trying to achieve avg file size of ~100 mb\n",
    "num_output_files_train = 100 #0 # Number of output Parquet files\n",
    "num_output_files_valid = 10 #2 # Number of output Parquet files\n",
    "\n",
    "# Define output directories\n",
    "OUTPUT_DEFINED_DIR = os.path.join(WORKSPACE, \"nvt-defined\")\n",
    "OUTPUT_WORKFLOW_DIR = os.path.join(WORKSPACE, \"nvt-analyzed\")\n",
    "OUTPUT_TRANSFORMED_DIR = os.path.join(WORKSPACE, \"nvt-processed\")\n",
    "\n",
    "\n",
    "parq_parameter_values = {\n",
    "    'bucket_data_src': BUCKET_source,\n",
    "    'bucket_data_output': BUCKET_destin,\n",
    "    'train_prefix': f'{TRAIN_PREFIX}',\n",
    "    'valid_prefix': f'{VALID_PREFIX}',\n",
    "    'file_pattern': f'{FILE_PATTERN}',\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid,\n",
    "    'output_path_defined_dir': f'{OUTPUT_DEFINED_DIR}',\n",
    "    'output_path_analyzed_dir': f'{OUTPUT_WORKFLOW_DIR}',\n",
    "    'output_path_transformed_dir': f'{OUTPUT_TRANSFORMED_DIR}',\n",
    "    'version':f'{VERSION}',\n",
    "    'shuffle': json.dumps(None), # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "    'app':f'{APP}',\n",
    "}\n",
    "\n",
    "pprint(parq_parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6527bc-5ad8-4bea-9694-3e1b2763e290",
   "metadata": {},
   "source": [
    "## compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75150bb6-c0ad-4e8c-9a08-bc707d2dce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list the current work dir\n",
    "# os.chdir('/home/jupyter/spotify-merlin/src')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d07797b-3541-4f2b-869b-bc1e7184fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-data-preprocess-pipeline.ipynb  custom_container_pipeline_spec.json\n",
      "02-merlin-vertex-training.ipynb    imgs\n",
      "03-query-model-inference.ipynb\t   nvt-parquet-full-1a100.json\n",
      "04-train-deploy-pipeline.ipynb\t   nvt-parquet-full-2a100.json\n",
      "05-recs-for-your-spotify.ipynb\t   nvt-parquet-full-4t4.json\n",
      "README.md\t\t\t   src\n",
      "archive\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c34709df-46f9-4f2a-9422-e196cf287345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from src.process_pipes.preproc_pipelines import preprocessing_parquet\n",
    "\n",
    "_compiled_pipeline_path = f'{PREPROCESS_PARQUET_PIPELINE_NAME}.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_parquet,\n",
    "       package_path=_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c41f7b-5a55-4af0-8331-4e61f0fabadf",
   "metadata": {},
   "source": [
    "## submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18a23dfe-2f1e-4863-9868-ea62644b204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvt-parquet-latest-12'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESS_PARQUET_PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5305c2e3-d1bd-4349-9e83-e5539c11b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{PREPROCESS_PARQUET_PIPELINE_NAME}_{TIMESTAMP}' #{TIMESTAMP}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parq_parameter_values,\n",
    "    labels=LABELS,\n",
    ")\n",
    "\n",
    "pipeline_job.submit(service_account=VERTEX_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ead4c-47ec-45ec-9b67-cb158f0f5f2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deab76-3e86-41e1-8953-be1b7e854787",
   "metadata": {},
   "source": [
    "### Define the NVTabular preprocessing graph\n",
    "\n",
    "```\n",
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = ['artist_name_can',\n",
    "        'track_name_can',\n",
    "        'artist_genres_can',\n",
    "    ]\n",
    "\n",
    "item_features_cont = [\n",
    "        'duration_ms_can',\n",
    "        'track_pop_can',\n",
    "        'artist_pop_can',\n",
    "        'artist_followers_can',\n",
    "    ]\n",
    "\n",
    "playlist_features_cat = [\n",
    "        'description_pl',\n",
    "        'name',\n",
    "        'collaborative',\n",
    "    ]\n",
    "\n",
    "playlist_features_cont = [\n",
    "        'duration_ms_seed_pl',\n",
    "        'n_songs_pl',\n",
    "        'num_artists_pl',\n",
    "        'num_albums_pl',\n",
    "    ]\n",
    "\n",
    "seq_feats_cat = [\n",
    "        'artist_name_pl',\n",
    "        'track_uri_pl',\n",
    "        'track_name_pl',\n",
    "        'album_name_pl',\n",
    "        'artist_genres_pl',\n",
    "    ]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
