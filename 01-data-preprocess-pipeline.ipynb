{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592fa423-5ce2-4e2b-a7ab-97174c387d47",
   "metadata": {},
   "source": [
    "# Preprocessing with NVTabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17770281-7bed-4495-a70f-1397441b60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-cloud-pipeline-components \n",
    "# !pip install google-cloud-bigquery-storage \n",
    "# !pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fbd89bc-5668-4e36-a0b8-c8a4ad835f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baea87eb-a282-4ce5-ada0-bc1791b0da12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "VERTEX_SA: 934903580331-compute@developer.gserviceaccount.com\n",
      "REGION: us-central1\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = 'hybrid-vertex' # Change to your project ID.\n",
    "REGION = 'us-central1'\n",
    "\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"VERTEX_SA: {VERTEX_SA}\")\n",
    "print(f\"REGION: {REGION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1278545-3674-4552-832f-8bedc19d5791",
   "metadata": {},
   "source": [
    "# Define preprocess pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0466709-cfb6-42ae-9d91-f02a2e8fb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bucket definitions\n",
    "# VERSION = 'v1-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions\n",
    "# IMAGE_NAME = 'nvt-preprocessing'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# DOCKERNAME = f'nvtabular-160' # 150\n",
    "\n",
    "# # Pipeline definitions\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "# print(f\"VERSION: {VERSION}\")\n",
    "# print(f\"APP: {APP}\")\n",
    "# print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "# print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "# print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "# print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "# print(f\"DOCKERNAME: {DOCKERNAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "# print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0816f671-55c0-40a8-a0a9-3f280c16b44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list the current work dir\n",
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23ce7975-79fc-4280-9452-5162244cfa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PREPROC_SUB_DIR = 'preprocessor'\n",
    "PIPELINE_SUB_DIR = 'pipes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa05f47a-e39a-4f81-b96c-3da03005520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}\n",
    "! touch {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/__init__.py\n",
    "\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2933d24f-8c8e-46e8-a72b-a821f2be7fe3",
   "metadata": {},
   "source": [
    "## preprocessing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a5fe2c1-e2b9-4162-be43-91be39b7f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocessor/preprocess_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PREPROC_SUB_DIR}/preprocess_task.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "# logging.disable(logging.WARNING)\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import fsspec\n",
    "\n",
    "import nvtabular as nvt\n",
    "from merlin.io.shuffle import Shuffle\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "from nvtabular.utils import device_mem_size\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "# import merlin.models.tf as mm\n",
    "from merlin.io.dataset import Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "# =============================================\n",
    "# featutres\n",
    "# =============================================\n",
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = [\n",
    "    'artist_name_can',\n",
    "    'track_name_can',\n",
    "    'artist_genres_can',\n",
    "]\n",
    "\n",
    "item_features_cont = [\n",
    "    'duration_ms_can',\n",
    "    'track_pop_can',\n",
    "    'artist_pop_can',\n",
    "    'artist_followers_can',\n",
    "]\n",
    "\n",
    "playlist_features_cat = [\n",
    "    'description_pl',\n",
    "    'name',\n",
    "    'collaborative',\n",
    "]\n",
    "\n",
    "playlist_features_cont = [\n",
    "    'duration_ms_seed_pl',\n",
    "    'n_songs_pl',\n",
    "    'num_artists_pl',\n",
    "    'num_albums_pl',\n",
    "]\n",
    "\n",
    "seq_feats_cat = [\n",
    "    'artist_name_pl',\n",
    "    'track_uri_pl',\n",
    "    'track_name_pl',\n",
    "    'album_name_pl',\n",
    "    'artist_genres_pl',\n",
    "]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "# item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "# item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "# playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "# playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "# playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "# =============================================\n",
    "# create cluster\n",
    "# =============================================\n",
    "def create_cluster(\n",
    "    n_workers,\n",
    "    device_limit_frac,\n",
    "    device_pool_frac,\n",
    "    memory_limit\n",
    "):\n",
    "    \"\"\"Create a Dask cluster to apply the transformations steps to the Dataset.\"\"\"\n",
    "    device_size = device_mem_size()\n",
    "    device_limit = int(device_limit_frac * device_size)\n",
    "    device_pool_size = int(device_pool_frac * device_size)\n",
    "    rmm_pool_size = (device_pool_size // 256) * 256\n",
    "\n",
    "    cluster = LocalCUDACluster(\n",
    "        n_workers=n_workers,\n",
    "        device_memory_limit=device_limit,\n",
    "        rmm_pool_size=rmm_pool_size,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "    return Client(cluster)\n",
    "\n",
    "# =============================================\n",
    "#            Create & Save dataset\n",
    "# =============================================\n",
    "\n",
    "def create_parquet_nvt_dataset(\n",
    "    # data_path,\n",
    "    frac_size,\n",
    "    data_prefix,\n",
    "    bucket_name,\n",
    "    file_pattern,\n",
    "):\n",
    "    \"\"\"Create a nvt.Dataset definition for the parquet files.\"\"\"\n",
    "    \n",
    "    # BUCKET = 'gs://spotify-builtin-2t'\n",
    "    # DATA_PATH = f\"{BUCKET}/{data_prefix}/0000000000**.snappy.parquet\"\n",
    "    DATA_PATH = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\" #0000000000**.snappy.parquet\"\n",
    "    fs = fsspec.filesystem('gs')\n",
    "    \n",
    "    file_list = fs.glob(DATA_PATH)\n",
    "        # os.path.join(data_path, '*.parquet')\n",
    "    # )\n",
    "\n",
    "    if not file_list:\n",
    "        raise FileNotFoundError('Parquet file(s) not found')\n",
    "\n",
    "    file_list = [os.path.join('gs://', i) for i in file_list]\n",
    "\n",
    "    # return nvt.Dataset(f\"{bucket_name}/{data_prefix}/0000000000**.snappy.parquet\", part_mem_fraction=frac_size)\n",
    "    return nvt.Dataset(\n",
    "        file_list,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size\n",
    "  )\n",
    "\n",
    "def save_dataset(\n",
    "    dataset,\n",
    "    output_path,\n",
    "    output_files,\n",
    "    # categorical_cols,\n",
    "    # continuous_cols,\n",
    "    shuffle=None,\n",
    "):\n",
    "    \"\"\"Save dataset to parquet files to path.\"\"\"\n",
    "    categorical_cols=CAT\n",
    "    continuous_cols=CONT\n",
    "\n",
    "    dict_dtypes = {}\n",
    "    for col in categorical_cols:\n",
    "        dict_dtypes[col] = np.int32\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        dict_dtypes[col] = np.float64\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path=output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files,\n",
    "        dtypes=dict_dtypes,\n",
    "        cats=categorical_cols,\n",
    "        conts=continuous_cols,\n",
    "    )\n",
    "\n",
    "# =============================================\n",
    "#            Workflow\n",
    "# =============================================\n",
    "def create_nvt_workflow():\n",
    "    '''\n",
    "    Create a nvt.Workflow definition with transformation all the steps\n",
    "    '''\n",
    "    item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "    playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "    item_features_cat = ['artist_name_can',\n",
    "            'track_name_can',\n",
    "            'artist_genres_can',\n",
    "        ]\n",
    "\n",
    "    item_features_cont = [\n",
    "            'duration_ms_can',\n",
    "            'track_pop_can',\n",
    "            'artist_pop_can',\n",
    "            'artist_followers_can',\n",
    "        ]\n",
    "\n",
    "    playlist_features_cat = [\n",
    "            'description_pl',\n",
    "            'name',\n",
    "            'collaborative',\n",
    "        ]\n",
    "\n",
    "    playlist_features_cont = [\n",
    "            'duration_ms_seed_pl',\n",
    "            'n_songs_pl',\n",
    "            'num_artists_pl',\n",
    "            'num_albums_pl',\n",
    "        ]\n",
    "\n",
    "    seq_feats_cat = [\n",
    "            'artist_name_pl',\n",
    "            'track_uri_pl',\n",
    "            'track_name_pl',\n",
    "            'album_name_pl',\n",
    "            'artist_genres_pl',\n",
    "        ]\n",
    "\n",
    "    CAT = playlist_features_cat + item_features_cat\n",
    "    CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "    item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "    item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "    playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "    playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "    playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures()\n",
    "    \n",
    "    # define a workflow\n",
    "    output = playlist_id + item_id \\\n",
    "    + item_feature_cat_node \\\n",
    "    + item_feature_cont_node \\\n",
    "    + playlist_feature_cat_node \\\n",
    "    + playlist_feature_cont_node \\\n",
    "    + playlist_feature_cat_seq_node \n",
    "\n",
    "    workflow = nvt.Workflow(output)\n",
    "    \n",
    "    return workflow\n",
    "\n",
    "# =============================================\n",
    "#            Create Parquet Dataset \n",
    "# =============================================\n",
    "\n",
    "def create_parquet_dataset_definition(\n",
    "    # data_paths,\n",
    "    # recursive,\n",
    "    # col_dtypes,\n",
    "    frac_size,\n",
    "    bucket_name,\n",
    "    data_prefix,\n",
    "    file_pattern,\n",
    "    # sep='\\t'\n",
    "):\n",
    "    from google.cloud import storage\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    DATASET_DEFINITION = f\"gs://{bucket_name}/{data_prefix}/{file_pattern}\"  # 0000000000**.snappy.parquet\"\n",
    "    \n",
    "    logging.info(f'DATASET_DEFINITION: {DATASET_DEFINITION}')\n",
    "    \n",
    "    return nvt.Dataset(f\"{DATASET_DEFINITION}\", engine='parquet', part_mem_fraction=frac_size)\n",
    "\n",
    "\n",
    "def convert_definition_to_parquet(\n",
    "    output_path,\n",
    "    dataset,\n",
    "    output_files,\n",
    "    shuffle=None\n",
    "):\n",
    "    \"\"\"Convert Parquet files to parquet and write to GCS.\"\"\"\n",
    "    if shuffle == 'None':\n",
    "        shuffle = None\n",
    "    else:\n",
    "        try:\n",
    "            shuffle = getattr(Shuffle, shuffle)\n",
    "        except:\n",
    "            print('Shuffle method not available. Using default.')\n",
    "            shuffle = None\n",
    "\n",
    "    dataset.to_parquet(\n",
    "        output_path,\n",
    "        shuffle=shuffle,\n",
    "        output_files=output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Create nv-tabular definition\n",
    "# =============================================\n",
    "def main_convert(args):\n",
    "    \n",
    "    logging.info('Beginning main-convert from preprocess_task.py...')\n",
    "    logging.info(f'args.output_path: {args.output_path}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating parquet dataset definition')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        # data_paths=args.parq_data_path,\n",
    "        # recursive=False,\n",
    "        bucket_name=args.bucket_name,     # 'spotify-builtin-2t', # TODO: parameterize\n",
    "        data_prefix=args.data_prefix,     # 'train', # TODO: JT check\n",
    "        frac_size=args.frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "\n",
    "    logging.info('Converting definition to Parquet')\n",
    "    convert_definition_to_parquet(\n",
    "        args.output_path,\n",
    "        dataset,\n",
    "        args.output_files\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            Analyse Dataset \n",
    "# =============================================\n",
    "def main_analyze(args):\n",
    "    \n",
    "    logging.info('Beginning main-analyze from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit\n",
    "    )\n",
    "    \n",
    "    logging.info('Creating Parquet dataset')\n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "  \n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "  \n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(args.output_path)\n",
    "    \n",
    "# =============================================\n",
    "#            Transform Dataset \n",
    "# =============================================\n",
    "def main_transform(args):\n",
    "    \n",
    "    logging.info('Beginning main-transform from preprocess_task.py...')\n",
    "    logging.info(f'args.bucket_name: {args.bucket_name}')\n",
    "    \n",
    "    client = create_cluster(\n",
    "        args.n_workers,\n",
    "        args.device_limit_frac,\n",
    "        args.device_pool_frac,\n",
    "        args.memory_limit,\n",
    "    )\n",
    "\n",
    "    # nvt_workflow = create_nvt_workflow()\n",
    "    nvt_workflow = nvt.Workflow.load(args.workflow_path, client)\n",
    "\n",
    "    # dataset = create_parquet_nvt_dataset(\n",
    "    #     args.parquet_data_path, \n",
    "    #     frac_size=args.frac_size)\n",
    "    \n",
    "    dataset = create_parquet_nvt_dataset(\n",
    "        # data_dir=args.parquet_data_path,\n",
    "        frac_size=args.frac_size,\n",
    "        data_prefix='train_data_parquet', # TODO: JT check\n",
    "        bucket_name=args.bucket_name,\n",
    "        file_pattern=\"0000000000**.snappy.parquet\",\n",
    "    )\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    transformed_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info('Saving transformed dataset')\n",
    "    save_dataset(\n",
    "        transformed_dataset,\n",
    "        output_path=args.output_path,\n",
    "        output_files=args.output_files,\n",
    "        # categorical_cols=CAT,\n",
    "        # continuous_cols=CONT,\n",
    "        shuffle=nvt.io.Shuffle.PER_PARTITION,\n",
    "    )\n",
    "    \n",
    "# =============================================\n",
    "#            args\n",
    "# =============================================\n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "  \n",
    "    parser.add_argument(\n",
    "        '--task',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--bucket_name',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parquet_data_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--parq_data_path',\n",
    "        required=False,\n",
    "        nargs='+'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_files',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_path',\n",
    "        type=str,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--n_workers',\n",
    "        type=int,\n",
    "        required=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--frac_size',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--memory_limit',\n",
    "        type=int,\n",
    "        required=False,\n",
    "        default=100_000_000_000\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_limit_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.60\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device_pool_frac',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=0.90\n",
    "    )\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    start_time = time.time()\n",
    "    logging.info('Timing task')\n",
    "\n",
    "    if parsed_args.task == 'transform':\n",
    "        main_transform(parsed_args)\n",
    "    elif parsed_args.task == 'analyze':\n",
    "        main_analyze(parsed_args)\n",
    "    elif parsed_args.task == 'convert':\n",
    "        main_convert(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Task completed. Elapsed time: %s', elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df76071-ea5a-479b-bd27-0135cd3c2fdd",
   "metadata": {},
   "source": [
    "## pipe components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5eaef21-5eaa-4f4a-b1f4-d2d0392cefa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipes/pipe_components.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/pipe_components.py\n",
    "\"\"\"KFP components.\"\"\"\n",
    "\n",
    "from typing import Optional\n",
    "from . import config\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact\n",
    "from kfp.v2.dsl import Dataset\n",
    "from kfp.v2.dsl import Input\n",
    "from kfp.v2.dsl import Model\n",
    "from kfp.v2.dsl import Output\n",
    "\n",
    "# =============================================\n",
    "#            convert_to_parquet_op\n",
    "# =============================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def convert_parquet_op(\n",
    "    output_dataset: Output[Dataset],\n",
    "    bucket_name: str,\n",
    "    data_prefix: str,\n",
    "    file_pattern: str,\n",
    "    output_path_defined_dir: str,\n",
    "    # data_dir_pattern: str,\n",
    "    # data_paths: list,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: Optional[str] = None,\n",
    "    recursive: Optional[bool] = False,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to create NVTabular definition.\n",
    "    \n",
    "    Args:\n",
    "    output_dataset: Output[Dataset]\n",
    "      Output metadata with references to the converted CSV files in GCS\n",
    "      and the split name.The path to the files are in GCS fuse format:\n",
    "      /gcs/<bucket name>/path/to/file\n",
    "    bucket: gcs bucket holding train & valid data\n",
    "    data_path_prefix: file path to GCS blobl object (e.g., gs://...data/path/prefix.../blob.xxx)\n",
    "    data_paths: list\n",
    "    split: str\n",
    "      Split name of the dataset. Example: train or valid\n",
    "    shuffle: str\n",
    "      How to shuffle the converted CSV, default to None. Options:\n",
    "        PER_PARTITION\n",
    "        PER_WORKER\n",
    "        FULL\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    #            import packages\n",
    "    # =========================================================\n",
    "    import os\n",
    "    import logging\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_parquet_dataset_definition,\n",
    "        convert_definition_to_parquet,\n",
    "        # get_criteo_col_dtypes,\n",
    "    )\n",
    "    \n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "    logging.info('Base path in %s', output_dataset.path)\n",
    "    # =========================================================\n",
    "    #            Define data paths\n",
    "    # =========================================================\n",
    "    logging.info(f'bucket_name: {bucket_name}')\n",
    "    logging.info(f'data_prefix: {data_prefix}')\n",
    "    \n",
    "    # Write metadata\n",
    "    output_dataset.metadata['split'] = split\n",
    "\n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating dataset definition from: {data_path_prefix}')\n",
    "    dataset = create_parquet_dataset_definition(\n",
    "        bucket_name=bucket_name,\n",
    "        data_prefix=data_prefix,\n",
    "        frac_size=frac_size,\n",
    "        file_pattern=file_pattern,\n",
    "    )\n",
    "    \n",
    "    logging.info(f'Converting Definition to Parquet; {output_dataset.uri}')\n",
    "    logging.info(f'Parquet Definition Output Path: ; {output_path_defined_dir}/{split}')\n",
    "    convert_definition_to_parquet(\n",
    "        output_path=f'{output_path_defined_dir}/{split}', # output_dataset.uri,\n",
    "        dataset=dataset,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "# =========================================================\n",
    "#            analyze_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def analyze_dataset_op(\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    workflow: Output[Artifact],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    n_workers: int,\n",
    "    device_limit_frac: Optional[float] = 0.6,\n",
    "    device_pool_frac: Optional[float] = 0.9,\n",
    "    frac_size: Optional[float] = 0.10,\n",
    "    memory_limit: Optional[int] = 100_000_000_000\n",
    "):\n",
    "    '''\n",
    "    Component to generate statistics from the dataset.\n",
    "    \n",
    "    Args:\n",
    "    parquet_dataset: List of strings\n",
    "      Input metadata with references to the train and valid converted\n",
    "      datasets in GCS and the split name.\n",
    "    workflow: Output[Artifact]\n",
    "      Output metadata with the path to the fitted workflow artifacts\n",
    "      (statistics).\n",
    "    device_limit_frac: Optional[float] = 0.6\n",
    "    device_pool_frac: Optional[float] = 0.9\n",
    "    frac_size: Optional[float] = 0.10\n",
    "    '''\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "  \n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        create_nvt_workflow,\n",
    "    )\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    create_cluster(\n",
    "      n_workers=n_workers,\n",
    "      device_limit_frac=device_limit_frac,\n",
    "      device_pool_frac=device_pool_frac,\n",
    "      memory_limit=memory_limit\n",
    "    )\n",
    "    \n",
    "    # logging.info(f'Creating Parquet dataset:{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset output_path_defined_dir: {output_path_defined_dir}/train')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/train', # TODO: JT Check \"train\"    # parquet_dataset.uri,\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "\n",
    "    logging.info('Creating Workflow')\n",
    "    # Create Workflow\n",
    "    nvt_workflow = create_nvt_workflow()\n",
    "\n",
    "    logging.info('Analyzing dataset')\n",
    "    nvt_workflow = nvt_workflow.fit(dataset)\n",
    "\n",
    "    logging.info('Saving Workflow')\n",
    "    nvt_workflow.save(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "    \n",
    "# =========================================================\n",
    "#            transform_dataset_op\n",
    "# =========================================================\n",
    "@dsl.component(\n",
    "    base_image=config.NVT_IMAGE_URI,\n",
    "    install_kfp_package=False\n",
    ")\n",
    "def transform_dataset_op(\n",
    "    workflow: Input[Artifact],\n",
    "    parquet_dataset: Input[Dataset],\n",
    "    transformed_dataset: Output[Dataset],\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    version: str,\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    app: str,\n",
    "    split: str,\n",
    "    num_output_files: int,\n",
    "    n_workers: int,\n",
    "    shuffle: str = None,\n",
    "    device_limit_frac: float = 0.6,\n",
    "    device_pool_frac: float = 0.9,\n",
    "    frac_size: float = 0.10,\n",
    "    memory_limit: int = 100_000_000_000\n",
    "):\n",
    "    \"\"\"Component to transform a dataset according to the workflow definitions.\n",
    "    Args:\n",
    "        workflow: Input[Artifact]\n",
    "        Input metadata with the path to the fitted_workflow\n",
    "        parquet_dataset: Input[Dataset]\n",
    "              Location of the converted dataset in GCS and split name\n",
    "        transformed_dataset: Output[Dataset]\n",
    "        Split name of the transformed dataset.\n",
    "        shuffle: str\n",
    "            How to shuffle the converted CSV, default to None. Options:\n",
    "                PER_PARTITION\n",
    "                PER_WORKER\n",
    "                FULL\n",
    "    device_limit_frac: float = 0.6\n",
    "    device_pool_frac: float = 0.9\n",
    "    frac_size: float = 0.10\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import nvtabular as nvt\n",
    "    from merlin.schema import Tags\n",
    "\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage.bucket import Bucket\n",
    "    from google.cloud.storage.blob import Blob\n",
    "\n",
    "    from preprocess_task import (\n",
    "        create_cluster,\n",
    "        save_dataset,\n",
    "    )\n",
    "    def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "        client = storage.Client()\n",
    "        blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "        blob.bucket._client = client\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "    def _read_blob_gcs(bucket_name, source_blob_name, destination_filename):\n",
    "        \"\"\"Downloads a file from GCS to local directory\"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "        blob.download_to_filename(destination_filename)\n",
    "        \n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    transformed_dataset.metadata['split'] = split\n",
    "    \n",
    "    logging.info('Creating cluster')\n",
    "    create_cluster(\n",
    "        n_workers=n_workers,\n",
    "        device_limit_frac=device_limit_frac,\n",
    "        device_pool_frac=device_pool_frac,\n",
    "        memory_limit=memory_limit\n",
    "    )\n",
    "\n",
    "   # logging.info(f'Creating Parquet dataset:gs://{parquet_dataset.uri}')\n",
    "    logging.info(f'Creating Parquet dataset:{output_path_defined_dir}/{split}')\n",
    "    dataset = nvt.Dataset(\n",
    "        path_or_source=f'{output_path_defined_dir}/{split}', #f'gs://{parquet_dataset.uri}',\n",
    "        engine='parquet',\n",
    "        part_mem_fraction=frac_size,\n",
    "        suffix='.parquet'\n",
    "    )\n",
    "    \n",
    "    logging.info('Loading Workflow')\n",
    "    nvt_workflow = nvt.Workflow.load(f'{output_path_analyzed_dir}') # workflow.path)\n",
    "\n",
    "    logging.info('Transforming Dataset')\n",
    "    trans_dataset = nvt_workflow.transform(dataset)\n",
    "\n",
    "    logging.info(f'transformed_dataset.uri: {transformed_dataset.uri}')\n",
    "    logging.info(f'Saving transformed dataset: {output_path_transformed_dir}/{split}')\n",
    "    save_dataset(\n",
    "        dataset=trans_dataset,\n",
    "        output_path=f'{output_path_transformed_dir}/{split}', # transformed_dataset.uri,\n",
    "        output_files=num_output_files,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    logging.info(f'transformed_dataset saved!')\n",
    "    logging.info(f'transformed_dataset.path: {transformed_dataset.path}')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        read and upload files\n",
    "    # =========================================================\n",
    "    '''\n",
    "    nv-tabular creates a txt file with all `gs://` paths\n",
    "    create a copy that replaces `gs://` with `/gcs/`\n",
    "    '''\n",
    "    logging.info('Generating file list for training...')\n",
    "    \n",
    "#     # get loca directory\n",
    "#     # LOCAL_DIRECTORY = os.getcwd()\n",
    "#     LOCAL_DIRECTORY = '/tmp/directory'\n",
    "    \n",
    "#     # _bucket_name='spotify-merlin-v1' # bucket_data_src\n",
    "#     PREFIX = f'nvt-preprocessing-{app}-{version}/nvt-processed/{split}'\n",
    "#     FILENAME = '_file_list.txt'\n",
    "#     SOURCE_BLOB_NAME = f'{PREFIX}/{FILENAME}'\n",
    "#     logging.info(f'SOURCE_BLOB_NAME: {SOURCE_BLOB_NAME}')\n",
    "    \n",
    "#     # LOCAL_DESTINATION_FILENAME = f'{LOCAL_DIRECTORY}/local_file_list.txt'\n",
    "#     LOCAL_DESTINATION_FILENAME = 'local_file_list.txt'\n",
    "#     logging.info(f'LOCAL_DESTINATION_FILENAME: {LOCAL_DESTINATION_FILENAME}')\n",
    "    \n",
    "#     _read_blob_gcs(\n",
    "#         bucket_name=bucket_data_output,\n",
    "#         source_blob_name=f'{SOURCE_BLOB_NAME}', \n",
    "#         destination_filename=LOCAL_DESTINATION_FILENAME\n",
    "#     )\n",
    "    \n",
    "#     file_list = os.path.join(transformed_dataset.path, '_file_list.txt')\n",
    "    \n",
    "#     # write new '/gcs/' file\n",
    "#     new_lines = []\n",
    "#     with open(LOCAL_DESTINATION_FILENAME, 'r') as fp:\n",
    "#         lines = fp.readlines()\n",
    "#         new_lines.append(lines[0])\n",
    "#         for line in lines[1:]:\n",
    "#             new_lines.append(line.replace('gs://', '/gcs/'))\n",
    "\n",
    "#     NEW_LOCAL_FILENAME = f'{LOCAL_DIRECTORY}/_gcs_file_list.txt'\n",
    "#     logging.info(f'NEW_LOCAL_FILENAME: {NEW_LOCAL_FILENAME}')\n",
    "    \n",
    "#     with open(NEW_LOCAL_FILENAME, 'w') as fp:\n",
    "#         fp.writelines(new_lines)\n",
    "        \n",
    "#     GCS_URI_DESTINATION = f'{output_path_transformed_dir}/{split}'\n",
    "#     logging.info(f'GCS_URI_DESTINATION: {GCS_URI_DESTINATION}')\n",
    "    \n",
    "#     _upload_blob_gcs(\n",
    "#         gcs_uri=GCS_URI_DESTINATION, \n",
    "#         source_file_name=NEW_LOCAL_FILENAME, \n",
    "#         destination_blob_name='_gcs_file_list.txt'\n",
    "#     )\n",
    "# logging.info(f'List of /gcs/ file paths uploaded to {GCS_URI_DESTINATION}/_gcs_file_list.txt')\n",
    "\n",
    "#     file_list_name = '_file_list.txt'\n",
    "#     file_list_uri = f'{output_path_transformed_dir}/{split}/{file_list_name}'\n",
    "#     logging.info(f'file_list_uri : {file_list_uri}')\n",
    "\n",
    "#     new_lines = []\n",
    "#     with open(file_list_uri, 'r') as fp:\n",
    "#         lines = fp.readlines()\n",
    "#         new_lines.append(lines[0])\n",
    "#         for line in lines[1:]:\n",
    "#             new_lines.append(line.replace('gs://', '/gcs/'))\n",
    "\n",
    "#     gcs_file_list_name = '_gcs_file_list.txt'\n",
    "#     gcs_file_list_uri = f'{output_path_transformed_dir}/{split}/{gcs_file_list_name}'\n",
    "#     logging.info(f'gcs_file_list_uri : {gcs_file_list_uri}')\n",
    "    \n",
    "#     with open(gcs_file_list_uri, 'w') as fp:\n",
    "#         fp.writelines(new_lines)\n",
    "    \n",
    "#     logging.info(f'List of /gcs/ file paths uploaded to {gcs_file_list}')\n",
    "    \n",
    "    # =========================================================\n",
    "    #        Saving cardinalities\n",
    "    # =========================================================\n",
    "    logging.info('Saving cardinalities')\n",
    "    \n",
    "    cols_schemas = nvt_workflow.output_schema.select_by_tag(Tags.CATEGORICAL)\n",
    "    cols_names = cols_schemas.column_names\n",
    "\n",
    "    cards = []\n",
    "    for c in cols_names:\n",
    "        col = cols_schemas.get(c)\n",
    "        cards.append(col.properties['embedding_sizes']['cardinality'])\n",
    "\n",
    "    transformed_dataset.metadata['cardinalities'] = cards\n",
    "    # transformed_dataset.metadata['dataset_gcs_uri'] = gcs_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729926e-c1f0-4389-995b-bc6db237c734",
   "metadata": {},
   "source": [
    "## preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88103c26-3f43-4835-b2ff-8a26245d6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/pipes/preproc_pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/preproc_pipelines.py\n",
    "\"\"\"Preprocessing pipelines.\"\"\"\n",
    "\n",
    "from . import pipe_components\n",
    "from . import config\n",
    "from kfp.v2 import dsl\n",
    "import os\n",
    "\n",
    "GKE_ACCELERATOR_KEY = 'cloud.google.com/gke-accelerator'\n",
    "\n",
    "# TODO: parametrize and fix config file \n",
    "# BUCKET_parquet = 'spotify-builtin-2t'\n",
    "# BUCKET = 'spotify-merlin-v1'\n",
    "# VERSION = 'v32-subset'\n",
    "# APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-preprocessing-{APP}-{VERSION}'\n",
    "# WORKSPACE = f'gs://{config.BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_NAME = f'nvtabular-parquet-pipeline-{VERSION}'\n",
    "# PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=f'{config.PREPROCESS_PARQUET_PIPELINE_NAME}', \n",
    "    pipeline_root=f'{config.PREPROCESS_PARQUET_PIPELINE_ROOT}'\n",
    ")\n",
    "def preprocessing_parquet(\n",
    "    bucket_data_src: str,\n",
    "    bucket_data_output: str,\n",
    "    # train_pattern: str,\n",
    "    # valid_pattern: str,\n",
    "    train_prefix: str,\n",
    "    valid_prefix: str,\n",
    "    file_pattern: str,\n",
    "    num_output_files_train: int,\n",
    "    num_output_files_valid: int,\n",
    "    output_path_defined_dir: str,\n",
    "    output_path_analyzed_dir: str,\n",
    "    output_path_transformed_dir: str,\n",
    "    shuffle: str,\n",
    "    version: str,\n",
    "    app: str,\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Pipeline to preprocess parquet files in GCS.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # =========================================================\n",
    "    # TODO: extract from BQ to parquet \n",
    "    # =========================================================\n",
    "    \n",
    "    \n",
    "    # =========================================================\n",
    "    #             Convert from parquet to def \n",
    "    # =========================================================\n",
    "    # config.BUCKET_NAME = 'spotify-builtin-2t' # 'spotify-merlin-v1' # TODO: parameterize\n",
    "    \n",
    "    parquet_to_def_train = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=train_prefix,\n",
    "            # data_dir_pattern=train_pattern,\n",
    "            split='train',\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_train.set_display_name('Convert training split')\n",
    "    parquet_to_def_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_train.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # === Convert eval dataset from CSV to Parquet\n",
    "    parquet_to_def_valid = (\n",
    "        pipe_components.convert_parquet_op(\n",
    "            bucket_name=bucket_data_src,\n",
    "            data_prefix=valid_prefix,\n",
    "            # data_dir_pattern=valid_pattern,\n",
    "            split='valid',\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            shuffle=shuffle,\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            file_pattern=file_pattern,\n",
    "        )\n",
    "    )\n",
    "    parquet_to_def_valid.set_display_name('Convert validation split')\n",
    "    parquet_to_def_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    parquet_to_def_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    parquet_to_def_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    parquet_to_def_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    parquet_to_def_valid.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Analyse train dataset \n",
    "    # =========================================================\n",
    "    \n",
    "    # === Analyze train data split\n",
    "    analyze_dataset = (\n",
    "        pipe_components.analyze_dataset_op(\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir\n",
    "        )\n",
    "    )\n",
    "    analyze_dataset.set_display_name('Analyze Dataset')\n",
    "    analyze_dataset.set_cpu_limit(config.CPU_LIMIT)\n",
    "    analyze_dataset.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    analyze_dataset.set_gpu_limit(config.GPU_LIMIT)\n",
    "    analyze_dataset.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    analyze_dataset.set_caching_options(enable_caching=True)\n",
    "    \n",
    "    # =========================================================\n",
    "    # Transform train split \n",
    "    # =========================================================\n",
    "\n",
    "    # === Transform train data split\n",
    "    transform_train = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='train',\n",
    "            # parquet_dataset=config.TRAIN_DIR_PARQUET,\n",
    "            parquet_dataset=parquet_to_def_train.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_train,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_train.set_display_name('Transform train split')\n",
    "    transform_train.set_cpu_limit(config.CPU_LIMIT)\n",
    "    transform_train.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_train.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_train.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_train.set_caching_options(enable_caching=True)\n",
    "\n",
    "    # =========================================================\n",
    "    #     Transform valid split\n",
    "    # =========================================================\n",
    "    \n",
    "    transform_valid = (\n",
    "        pipe_components.transform_dataset_op(\n",
    "            workflow=analyze_dataset.outputs['workflow'],\n",
    "            split='valid',\n",
    "            parquet_dataset=parquet_to_def_valid.outputs['output_dataset'],\n",
    "            output_path_defined_dir=output_path_defined_dir,\n",
    "            output_path_transformed_dir=f'{output_path_transformed_dir}',\n",
    "            output_path_analyzed_dir=output_path_analyzed_dir,\n",
    "            num_output_files=num_output_files_valid,\n",
    "            n_workers=int(config.GPU_LIMIT),\n",
    "            version=version,\n",
    "            bucket_data_src=bucket_data_src,\n",
    "            bucket_data_output=bucket_data_output,\n",
    "            app=app,\n",
    "        )\n",
    "    )\n",
    "    transform_valid.set_display_name('Transform valid split')\n",
    "    transform_valid.set_cpu_limit(config.CPU_LIMIT)\n",
    "    transform_valid.set_memory_limit(config.MEMORY_LIMIT)\n",
    "    transform_valid.set_gpu_limit(config.GPU_LIMIT)\n",
    "    transform_valid.add_node_selector_constraint(GKE_ACCELERATOR_KEY, config.GPU_TYPE)\n",
    "    transform_valid.set_caching_options(enable_caching=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16211a8f-23a4-4bf5-a8d3-ccc99b9ea669",
   "metadata": {},
   "source": [
    "### Set Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6aa92776-dc2d-4a86-8078-0119140082a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_source: spotify-beam-v3\n",
      "BUCKET_destin: jt-merlin-scaling\n",
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "\n",
      "GPU_LIMIT: 4\n",
      "GPU_TYPE: NVIDIA_TESLA_T4\n",
      "CPU_LIMIT: 64\n",
      "MEMORY_LIMIT: 624G\n",
      "INSTANCE_TYPE: n1-highmem-64\n",
      "\n",
      "VERSION: v1full\n",
      "APP: spotify\n",
      "MODEL_DISPLAY_NAME: nvt-last5-v1full\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-v1full\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-v1full\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-v1full/nvt-parquet-v1full\n",
      "\n",
      "IMAGE_NAME: nvt-preprocessing\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "DOCKERNAME: nvt-133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "#           storage\n",
    "# =============================================\n",
    "BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "print(f\"BUCKET_source: {BUCKET_source}\")\n",
    "print(f\"BUCKET_destin: {BUCKET_destin}\")\n",
    "print(f\"TRAIN_SRC_DIR: {TRAIN_SRC_DIR}\")\n",
    "print(f\"VALID_SRC_DIR: {VALID_SRC_DIR}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           accelerators\n",
    "# =============================================\n",
    "# Instance configuration\n",
    "GPU_LIMIT = '4'\n",
    "GPU_TYPE = 'NVIDIA_TESLA_T4'\n",
    "CPU_LIMIT = '64'\n",
    "MEMORY_LIMIT = '624G'\n",
    "INSTANCE_TYPE = \"n1-highmem-64\"\n",
    "\n",
    "# Instance configuration\n",
    "# GPU_LIMIT = '8'                   # 1\n",
    "# GPU_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# CPU_LIMIT = '96'                  # '64'\n",
    "# MEMORY_LIMIT = '680'              #'624G'\n",
    "# INSTANCE_TYPE = \"a2-highgpu-1g\"\n",
    "\n",
    "print(f\"GPU_LIMIT: {GPU_LIMIT}\")\n",
    "print(f\"GPU_TYPE: {GPU_TYPE}\")\n",
    "print(f\"CPU_LIMIT: {CPU_LIMIT}\")\n",
    "print(f\"MEMORY_LIMIT: {MEMORY_LIMIT}\")\n",
    "print(f\"INSTANCE_TYPE: {INSTANCE_TYPE}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           pipelines\n",
    "# =============================================\n",
    "VERSION = 'v1full'\n",
    "APP = 'spotify'\n",
    "MODEL_DISPLAY_NAME = f'nvt-last5-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "# Pipeline definitions\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = f'nvt-parquet-{VERSION}'\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_PARQUET_PIPELINE_NAME)\n",
    "\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"APP: {APP}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_NAME: {PREPROCESS_PARQUET_PIPELINE_NAME}\")\n",
    "print(f\"PREPROCESS_PARQUET_PIPELINE_ROOT: {PREPROCESS_PARQUET_PIPELINE_ROOT}\\n\")\n",
    "\n",
    "# =============================================\n",
    "#           custom image\n",
    "# =============================================\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt-preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = f'nvt-133' # 150\n",
    "\n",
    "print(f\"IMAGE_NAME: {IMAGE_NAME}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"DOCKERNAME: {DOCKERNAME}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "381dcfc7-6f7c-4427-8859-8375d92be415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1776608-a23c-4aa5-acc3-2b965be1b62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/pipes/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINE_SUB_DIR}/config.py\n",
    "\n",
    "import os\n",
    "\n",
    "# =============================================\n",
    "#           Cloud Storage Directorires\n",
    "# =============================================\n",
    "BUCKET_source = 'spotify-beam-v3'\n",
    "BUCKET_destin = 'jt-merlin-scaling'\n",
    "TRAIN_SRC_DIR = 'train_data_parquet'\n",
    "VALID_SRC_DIR = 'valid_data_parquet'\n",
    "\n",
    "# =============================================\n",
    "#           Setup\n",
    "# =============================================\n",
    "VERSION = 'v1full'\n",
    "APP = 'spotify'\n",
    "# MODEL_DISPLAY_NAME = f'nvt-prep-last5-{VERSION}'\n",
    "# WORKSPACE = f'gs://{BUCKET_destin}/{MODEL_DISPLAY_NAME}'\n",
    "PROJECT_ID = \"hybrid-vertex\"\n",
    "REGION = \"us-central1\"\n",
    "VERTEX_SA = f\"vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "\n",
    "# =============================================\n",
    "#           Artifacts\n",
    "# =============================================\n",
    "MODEL_DISPLAY_NAME = \"nvt-last5-v1full\"\n",
    "WORKSPACE = \"gs://jt-merlin-scaling/nvt-last5-v1full\"\n",
    "NVT_IMAGE_URI = \"gcr.io/hybrid-vertex/nvt-preprocessing\"\n",
    "\n",
    "# =============================================\n",
    "#           Pipeline Configs\n",
    "# =============================================\n",
    "PREPROCESS_PARQUET_PIPELINE_NAME = \"nvt-parquet-v1full\"\n",
    "PREPROCESS_PARQUET_PIPELINE_ROOT = \"gs://jt-merlin-scaling/nvt-last5-v1full/nvt-parquet-v1full\"\n",
    "\n",
    "INSTANCE_TYPE = os.getenv(\"INSTANCE_TYPE\", \"n1-highmem-64\")\n",
    "CPU_LIMIT = os.getenv(\"CPU_LIMIT\", \"64\")\n",
    "MEMORY_LIMIT = os.getenv(\"MEMORY_LIMIT\", \"624G\")\n",
    "GPU_LIMIT = os.getenv(\"GPU_LIMIT\", \"4\")\n",
    "GPU_TYPE = os.getenv(\"GPU_TYPE\", \"NVIDIA_TESLA_T4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f0b96-b8df-4893-8bfc-83a508db9f4f",
   "metadata": {},
   "source": [
    "### check config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40adb058-a744-47fd-b68e-c224014d32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_SRC_DIR: train_data_parquet\n",
      "VALID_SRC_DIR: valid_data_parquet\n",
      "VERSION: v1full\n",
      "APP: spotify\n",
      "PROJECT_ID: hybrid-vertex\n",
      "REGION: us-central1\n",
      "VERTEX_SA: vertex-sa@hybrid-vertex.iam.gserviceaccount.com\n",
      "MODEL_DISPLAY_NAME: nvt-last5-v1full\n",
      "WORKSPACE: gs://jt-merlin-scaling/nvt-last5-v1full\n",
      "NVT_IMAGE_URI: gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "PREPROCESS_PARQUET_PIPELINE_NAME: nvt-parquet-v1full\n",
      "PREPROCESS_PARQUET_PIPELINE_ROOT: gs://jt-merlin-scaling/nvt-last5-v1full/nvt-parquet-v1full\n",
      "INSTANCE_TYPE: n1-highmem-64\n",
      "CPU_LIMIT: 64\n",
      "MEMORY_LIMIT: 624G\n",
      "GPU_LIMIT: 4\n",
      "GPU_TYPE: NVIDIA_TESLA_T4\n"
     ]
    }
   ],
   "source": [
    "from src.pipes import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9e4a9-4d2b-46e7-a51b-65b04ee6bcd6",
   "metadata": {},
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dc6b0-6050-4ac7-ad3a-1da1e0b00836",
   "metadata": {},
   "source": [
    "Tried these:\n",
    "* `FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.07`\n",
    "* `FROM nvcr.io/nvidia/merlin/merlin-tensorflow:nightly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981f2ba9-4e39-4873-96f9-81815d67784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.nvt-133\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
    "RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular==1.3.3\n",
    "RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "COPY preprocessor/* ./\n",
    "\n",
    "ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c7a71a5-e4d6-43b5-8c5e-32bbc3ffc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")\n",
    "\n",
    "MACHINE_TYPE ='e2-highcpu-32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "074c1600-7874-44a8-b1db-850882129e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67384b3b-5a8d-4250-8910-c26188a3b191",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 76 file(s) totalling 1.9 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1666877837.763472-feeb9049dfb84553a32a137b46cc7d95.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/07b08f81-8f13-40eb-b93f-4a6ecaf4006b].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/07b08f81-8f13-40eb-b93f-4a6ecaf4006b?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"07b08f81-8f13-40eb-b93f-4a6ecaf4006b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1666877837.763472-feeb9049dfb84553a32a137b46cc7d95.tgz#1666877838358497\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1666877837.763472-feeb9049dfb84553a32a137b46cc7d95.tgz#1666877838358497...\n",
      "/ [1 files][320.8 KiB/320.8 KiB]                                                \n",
      "Operation completed over 1 objects/320.8 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon    426kB\n",
      "Step 1/8 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      "22.09: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "3b65ec22a9e9: Pulling fs layer\n",
      "fd80d866e8b2: Pulling fs layer\n",
      "a364ca75fd6d: Pulling fs layer\n",
      "3d4731d03623: Pulling fs layer\n",
      "53a5c2e0251f: Pulling fs layer\n",
      "b00ff40d02d9: Pulling fs layer\n",
      "3036e9b94123: Pulling fs layer\n",
      "453fdcdda788: Pulling fs layer\n",
      "35e12ec5e515: Pulling fs layer\n",
      "11f61a475a23: Pulling fs layer\n",
      "24280cf31c9a: Pulling fs layer\n",
      "79007799e2ed: Pulling fs layer\n",
      "03eb76abf1e5: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "5e9434e8ae41: Pulling fs layer\n",
      "88a3e778b5bf: Pulling fs layer\n",
      "729af2b35d14: Pulling fs layer\n",
      "30e0a3a7e9e5: Pulling fs layer\n",
      "0852b4bd65a1: Pulling fs layer\n",
      "81cb421c2c25: Pulling fs layer\n",
      "3d6b664afa23: Pulling fs layer\n",
      "3e8f37aba8a2: Pulling fs layer\n",
      "91bba9bd0f1f: Pulling fs layer\n",
      "a422be4dcb08: Pulling fs layer\n",
      "ca10bb6dc143: Pulling fs layer\n",
      "e48bbfc7d00e: Pulling fs layer\n",
      "860a23551ac0: Pulling fs layer\n",
      "cc78be876588: Pulling fs layer\n",
      "3d4731d03623: Waiting\n",
      "ad6568ad37e5: Pulling fs layer\n",
      "258a31babfce: Pulling fs layer\n",
      "116ca0069c88: Pulling fs layer\n",
      "ede4a5022f61: Pulling fs layer\n",
      "154d6414dd17: Pulling fs layer\n",
      "e1f68d1c5137: Pulling fs layer\n",
      "53a5c2e0251f: Waiting\n",
      "b00ff40d02d9: Waiting\n",
      "0d4b5cd36c43: Pulling fs layer\n",
      "fc9b6547dc7c: Pulling fs layer\n",
      "de51b5b1b318: Pulling fs layer\n",
      "d684c579871f: Pulling fs layer\n",
      "4a39f6623824: Pulling fs layer\n",
      "3036e9b94123: Waiting\n",
      "30bba30584e3: Pulling fs layer\n",
      "c104fa3a7626: Pulling fs layer\n",
      "22f09e497c63: Pulling fs layer\n",
      "5f66cd739f31: Pulling fs layer\n",
      "11f61a475a23: Waiting\n",
      "c9c9a9e1cad6: Pulling fs layer\n",
      "630fcaa7a158: Pulling fs layer\n",
      "35e12ec5e515: Waiting\n",
      "1b8090778700: Pulling fs layer\n",
      "24280cf31c9a: Waiting\n",
      "4ca3cacea924: Pulling fs layer\n",
      "4aa043daa566: Pulling fs layer\n",
      "c1d86dc35ba1: Pulling fs layer\n",
      "3d6b664afa23: Waiting\n",
      "55f4be7d7adc: Pulling fs layer\n",
      "3e8f37aba8a2: Waiting\n",
      "79007799e2ed: Waiting\n",
      "f8e1ddeececb: Pulling fs layer\n",
      "91bba9bd0f1f: Waiting\n",
      "bf1f5a5d15b8: Pulling fs layer\n",
      "a422be4dcb08: Waiting\n",
      "453fdcdda788: Waiting\n",
      "ca10bb6dc143: Waiting\n",
      "03eb76abf1e5: Waiting\n",
      "68d63385def6: Pulling fs layer\n",
      "e48bbfc7d00e: Waiting\n",
      "865f53eecc40: Pulling fs layer\n",
      "860a23551ac0: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "cc78be876588: Waiting\n",
      "70ac11bbf381: Pulling fs layer\n",
      "5e9434e8ae41: Waiting\n",
      "91a761249212: Pulling fs layer\n",
      "88a3e778b5bf: Waiting\n",
      "2cb953eac694: Pulling fs layer\n",
      "0852b4bd65a1: Waiting\n",
      "2e6386dccac0: Pulling fs layer\n",
      "ad6568ad37e5: Waiting\n",
      "81cb421c2c25: Waiting\n",
      "67793606273a: Pulling fs layer\n",
      "258a31babfce: Waiting\n",
      "116ca0069c88: Waiting\n",
      "5f66cd739f31: Waiting\n",
      "729af2b35d14: Waiting\n",
      "ede4a5022f61: Waiting\n",
      "c9c9a9e1cad6: Waiting\n",
      "630fcaa7a158: Waiting\n",
      "bfdaa5a6754f: Pulling fs layer\n",
      "3e0d0b3b5c1c: Pulling fs layer\n",
      "8d6d2c98840c: Pulling fs layer\n",
      "2559630e37d2: Pulling fs layer\n",
      "d0f13d58f7fa: Pulling fs layer\n",
      "929ce3ad5dc9: Pulling fs layer\n",
      "7a42c1978b2e: Pulling fs layer\n",
      "70ac11bbf381: Waiting\n",
      "91a761249212: Waiting\n",
      "30e0a3a7e9e5: Waiting\n",
      "4a39f6623824: Waiting\n",
      "2cb953eac694: Waiting\n",
      "154d6414dd17: Waiting\n",
      "22f09e497c63: Waiting\n",
      "30bba30584e3: Waiting\n",
      "2e6386dccac0: Waiting\n",
      "2559630e37d2: Waiting\n",
      "67793606273a: Waiting\n",
      "1b8090778700: Waiting\n",
      "d0f13d58f7fa: Waiting\n",
      "55f4be7d7adc: Waiting\n",
      "bfdaa5a6754f: Waiting\n",
      "4aa043daa566: Waiting\n",
      "929ce3ad5dc9: Waiting\n",
      "7a42c1978b2e: Waiting\n",
      "68d63385def6: Waiting\n",
      "bf1f5a5d15b8: Waiting\n",
      "8d6d2c98840c: Waiting\n",
      "3e0d0b3b5c1c: Waiting\n",
      "865f53eecc40: Waiting\n",
      "fc9b6547dc7c: Waiting\n",
      "e1f68d1c5137: Waiting\n",
      "de51b5b1b318: Waiting\n",
      "d684c579871f: Waiting\n",
      "c104fa3a7626: Waiting\n",
      "3b65ec22a9e9: Verifying Checksum\n",
      "3b65ec22a9e9: Download complete\n",
      "3d4731d03623: Verifying Checksum\n",
      "3d4731d03623: Download complete\n",
      "a364ca75fd6d: Verifying Checksum\n",
      "a364ca75fd6d: Download complete\n",
      "fd80d866e8b2: Download complete\n",
      "3b65ec22a9e9: Pull complete\n",
      "b00ff40d02d9: Verifying Checksum\n",
      "b00ff40d02d9: Download complete\n",
      "3036e9b94123: Verifying Checksum\n",
      "3036e9b94123: Download complete\n",
      "453fdcdda788: Verifying Checksum\n",
      "453fdcdda788: Download complete\n",
      "35e12ec5e515: Verifying Checksum\n",
      "35e12ec5e515: Download complete\n",
      "11f61a475a23: Download complete\n",
      "24280cf31c9a: Verifying Checksum\n",
      "24280cf31c9a: Download complete\n",
      "79007799e2ed: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "5e9434e8ae41: Verifying Checksum\n",
      "5e9434e8ae41: Download complete\n",
      "88a3e778b5bf: Download complete\n",
      "fd80d866e8b2: Pull complete\n",
      "729af2b35d14: Verifying Checksum\n",
      "729af2b35d14: Download complete\n",
      "03eb76abf1e5: Verifying Checksum\n",
      "03eb76abf1e5: Download complete\n",
      "0852b4bd65a1: Verifying Checksum\n",
      "0852b4bd65a1: Download complete\n",
      "81cb421c2c25: Verifying Checksum\n",
      "81cb421c2c25: Download complete\n",
      "3d6b664afa23: Download complete\n",
      "a364ca75fd6d: Pull complete\n",
      "3d4731d03623: Pull complete\n",
      "30e0a3a7e9e5: Verifying Checksum\n",
      "30e0a3a7e9e5: Download complete\n",
      "53a5c2e0251f: Verifying Checksum\n",
      "53a5c2e0251f: Download complete\n",
      "91bba9bd0f1f: Verifying Checksum\n",
      "91bba9bd0f1f: Download complete\n",
      "ca10bb6dc143: Verifying Checksum\n",
      "ca10bb6dc143: Download complete\n",
      "a422be4dcb08: Verifying Checksum\n",
      "a422be4dcb08: Download complete\n",
      "e48bbfc7d00e: Verifying Checksum\n",
      "e48bbfc7d00e: Download complete\n",
      "cc78be876588: Verifying Checksum\n",
      "cc78be876588: Download complete\n",
      "860a23551ac0: Download complete\n",
      "ad6568ad37e5: Verifying Checksum\n",
      "ad6568ad37e5: Download complete\n",
      "258a31babfce: Verifying Checksum\n",
      "258a31babfce: Download complete\n",
      "116ca0069c88: Verifying Checksum\n",
      "116ca0069c88: Download complete\n",
      "ede4a5022f61: Download complete\n",
      "154d6414dd17: Verifying Checksum\n",
      "154d6414dd17: Download complete\n",
      "0d4b5cd36c43: Verifying Checksum\n",
      "0d4b5cd36c43: Download complete\n",
      "e1f68d1c5137: Verifying Checksum\n",
      "e1f68d1c5137: Download complete\n",
      "fc9b6547dc7c: Verifying Checksum\n",
      "fc9b6547dc7c: Download complete\n",
      "d684c579871f: Verifying Checksum\n",
      "d684c579871f: Download complete\n",
      "4a39f6623824: Verifying Checksum\n",
      "4a39f6623824: Download complete\n",
      "30bba30584e3: Verifying Checksum\n",
      "30bba30584e3: Download complete\n",
      "3e8f37aba8a2: Verifying Checksum\n",
      "3e8f37aba8a2: Download complete\n",
      "c104fa3a7626: Verifying Checksum\n",
      "c104fa3a7626: Download complete\n",
      "22f09e497c63: Verifying Checksum\n",
      "22f09e497c63: Download complete\n",
      "5f66cd739f31: Download complete\n",
      "c9c9a9e1cad6: Verifying Checksum\n",
      "c9c9a9e1cad6: Download complete\n",
      "630fcaa7a158: Verifying Checksum\n",
      "630fcaa7a158: Download complete\n",
      "1b8090778700: Verifying Checksum\n",
      "1b8090778700: Download complete\n",
      "de51b5b1b318: Download complete\n",
      "4ca3cacea924: Verifying Checksum\n",
      "4ca3cacea924: Download complete\n",
      "c1d86dc35ba1: Verifying Checksum\n",
      "c1d86dc35ba1: Download complete\n",
      "4aa043daa566: Verifying Checksum\n",
      "4aa043daa566: Download complete\n",
      "55f4be7d7adc: Verifying Checksum\n",
      "55f4be7d7adc: Download complete\n",
      "bf1f5a5d15b8: Verifying Checksum\n",
      "bf1f5a5d15b8: Download complete\n",
      "f8e1ddeececb: Verifying Checksum\n",
      "f8e1ddeececb: Download complete\n",
      "865f53eecc40: Verifying Checksum\n",
      "865f53eecc40: Download complete\n",
      "70ac11bbf381: Verifying Checksum\n",
      "70ac11bbf381: Download complete\n",
      "68d63385def6: Verifying Checksum\n",
      "68d63385def6: Download complete\n",
      "2e6386dccac0: Verifying Checksum\n",
      "2e6386dccac0: Download complete\n",
      "67793606273a: Verifying Checksum\n",
      "67793606273a: Download complete\n",
      "2cb953eac694: Verifying Checksum\n",
      "2cb953eac694: Download complete\n",
      "bfdaa5a6754f: Verifying Checksum\n",
      "bfdaa5a6754f: Download complete\n",
      "3e0d0b3b5c1c: Download complete\n",
      "91a761249212: Download complete\n",
      "8d6d2c98840c: Verifying Checksum\n",
      "8d6d2c98840c: Download complete\n",
      "2559630e37d2: Verifying Checksum\n",
      "2559630e37d2: Download complete\n",
      "d0f13d58f7fa: Verifying Checksum\n",
      "d0f13d58f7fa: Download complete\n",
      "7a42c1978b2e: Verifying Checksum\n",
      "7a42c1978b2e: Download complete\n",
      "53a5c2e0251f: Pull complete\n",
      "b00ff40d02d9: Pull complete\n",
      "3036e9b94123: Pull complete\n",
      "453fdcdda788: Pull complete\n",
      "35e12ec5e515: Pull complete\n",
      "11f61a475a23: Pull complete\n",
      "929ce3ad5dc9: Verifying Checksum\n",
      "929ce3ad5dc9: Download complete\n",
      "24280cf31c9a: Pull complete\n",
      "79007799e2ed: Pull complete\n",
      "03eb76abf1e5: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "5e9434e8ae41: Pull complete\n",
      "88a3e778b5bf: Pull complete\n",
      "729af2b35d14: Pull complete\n",
      "30e0a3a7e9e5: Pull complete\n",
      "0852b4bd65a1: Pull complete\n",
      "81cb421c2c25: Pull complete\n",
      "3d6b664afa23: Pull complete\n",
      "3e8f37aba8a2: Pull complete\n",
      "91bba9bd0f1f: Pull complete\n",
      "a422be4dcb08: Pull complete\n",
      "ca10bb6dc143: Pull complete\n",
      "e48bbfc7d00e: Pull complete\n",
      "860a23551ac0: Pull complete\n",
      "cc78be876588: Pull complete\n",
      "ad6568ad37e5: Pull complete\n",
      "258a31babfce: Pull complete\n",
      "116ca0069c88: Pull complete\n",
      "ede4a5022f61: Pull complete\n",
      "154d6414dd17: Pull complete\n",
      "e1f68d1c5137: Pull complete\n",
      "0d4b5cd36c43: Pull complete\n",
      "fc9b6547dc7c: Pull complete\n",
      "de51b5b1b318: Pull complete\n",
      "d684c579871f: Pull complete\n",
      "4a39f6623824: Pull complete\n",
      "30bba30584e3: Pull complete\n",
      "c104fa3a7626: Pull complete\n",
      "22f09e497c63: Pull complete\n",
      "5f66cd739f31: Pull complete\n",
      "c9c9a9e1cad6: Pull complete\n",
      "630fcaa7a158: Pull complete\n",
      "1b8090778700: Pull complete\n",
      "4ca3cacea924: Pull complete\n",
      "4aa043daa566: Pull complete\n",
      "c1d86dc35ba1: Pull complete\n",
      "55f4be7d7adc: Pull complete\n",
      "f8e1ddeececb: Pull complete\n",
      "bf1f5a5d15b8: Pull complete\n",
      "68d63385def6: Pull complete\n",
      "865f53eecc40: Pull complete\n",
      "70ac11bbf381: Pull complete\n",
      "91a761249212: Pull complete\n",
      "2cb953eac694: Pull complete\n",
      "2e6386dccac0: Pull complete\n",
      "67793606273a: Pull complete\n",
      "bfdaa5a6754f: Pull complete\n",
      "3e0d0b3b5c1c: Pull complete\n",
      "8d6d2c98840c: Pull complete\n",
      "2559630e37d2: Pull complete\n",
      "d0f13d58f7fa: Pull complete\n",
      "929ce3ad5dc9: Pull complete\n",
      "7a42c1978b2e: Pull complete\n",
      "Digest: sha256:2475b7062a16cd7ba0e5eda0ff58f206400714aafd061d4d8a1a1e8aacd59668\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      " ---> ec90adb8185e\n",
      "Step 2/8 : WORKDIR /src\n",
      " ---> Running in ea17a51952aa\n",
      "Removing intermediate container ea17a51952aa\n",
      " ---> e01e9003cc07\n",
      "Step 3/8 : RUN pip install -U pip\n",
      " ---> Running in baa3a0b66ffe\n",
      "Collecting pip\n",
      "  Downloading pip-22.3-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.0.2\n",
      "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'pip'. No files were found to uninstall.\n",
      "Successfully installed pip-22.3\n",
      "Removing intermediate container baa3a0b66ffe\n",
      " ---> fbad5e76a2c4\n",
      "Step 4/8 : RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
      " ---> Running in b7b20fef346a\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.3.5-py2.py3-none-any.whl (211 kB)\n",
      "      211.9/211.9 kB 22.1 MB/s eta 0:00:00\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.10.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.0.0\n",
      "  Downloading google_cloud_bigquery_storage-2.16.2-py2.py3-none-any.whl (185 kB)\n",
      "      185.4/185.4 kB 32.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (21.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (3.19.5)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "      47.9/47.9 kB 9.9 MB/s eta 0:00:00\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-cloud-bigquery) (2.22.0)\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "      77.4/77.4 kB 16.2 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0dev,>=1.47.0\n",
      "  Downloading grpcio-1.50.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "      4.7/4.7 MB 90.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: pyarrow<10.0dev,>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (7.0.0)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "      115.6/115.6 kB 23.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.12.0)\n",
      "Collecting fsspec==2022.10.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "      138.8/138.8 kB 27.6 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.5.0-py2.py3-none-any.whl (106 kB)\n",
      "      107.0/107.0 kB 21.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery) (1.56.4)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-bigquery) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from pyarrow<10.0dev,>=3.0.0->google-cloud-bigquery) (1.22.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.9-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "      408.4/408.4 kB 53.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.8)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3986 sha256=438d4fdd88213ef462428251fe4f4ba66d45e01491b5212673cf3a0846de26f7\n",
      "  Stored in directory: /root/.cache/pip/wheels/02/f7/20/63b767d529308c530275a9738e4a46a848757cc86040dafe19\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, protobuf, grpcio, google-crc32c, fsspec, proto-plus, google-resumable-media, grpcio-status, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-bigquery-storage, google-cloud-bigquery, gcsfs\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.5\n",
      "    Uninstalling protobuf-3.19.5:\n",
      "      Successfully uninstalled protobuf-3.19.5\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.41.0\n",
      "    Uninstalling grpcio-1.41.0:\n",
      "      Successfully uninstalled grpcio-1.41.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "wandb 0.13.3 requires protobuf<4.0dev,>=3.12.0, but you have protobuf 4.21.9 which is incompatible.\n",
      "tensorflow-metadata 1.10.0 requires protobuf<4,>=3.13, but you have protobuf 4.21.9 which is incompatible.\n",
      "tensorboard 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.9 which is incompatible.\n",
      "merlin-core 0.7.0 requires fsspec==2022.5.0, but you have fsspec 2022.10.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed cloudml-hypertune-0.1.0.dev6 fsspec-2022.10.0 gcsfs-2022.10.0 google-api-core-2.10.2 google-cloud-bigquery-3.3.5 google-cloud-bigquery-storage-2.16.2 google-cloud-core-2.3.2 google-cloud-storage-2.5.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 grpcio-1.50.0 grpcio-status-1.50.0 proto-plus-1.22.1 protobuf-4.21.9\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container b7b20fef346a\n",
      " ---> a3fc27890ddf\n",
      "Step 5/8 : RUN pip install google-cloud-aiplatform[cloud_profiler] kfp nvtabular==1.3.3\n",
      " ---> Running in a3d6fc010e04\n",
      "Collecting google-cloud-aiplatform[cloud_profiler]\n",
      "  Downloading google_cloud_aiplatform-1.18.2-py2.py3-none-any.whl (2.3 MB)\n",
      "      2.3/2.3 MB 73.3 MB/s eta 0:00:00\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.14.tar.gz (304 kB)\n",
      "      304.3/304.3 kB 45.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting nvtabular==1.3.3\n",
      "  Downloading nvtabular-1.3.3.tar.gz (132 kB)\n",
      "      132.6/132.6 kB 25.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from nvtabular==1.3.3) (0.7.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.5.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (2.10.2)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "      233.8/233.8 kB 37.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (1.22.1)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform[cloud_profiler]) (4.21.9)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "      206.6/206.6 kB 34.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-profile<3.0.0dev,>=2.4.0\n",
      "  Downloading tensorboard_plugin_profile-2.8.0-py3-none-any.whl (5.3 MB)\n",
      "      5.3/5.3 MB 93.3 MB/s eta 0:00:00\n",
      "Collecting werkzeug<2.1.0dev,>=2.0.0\n",
      "  Downloading Werkzeug-2.0.3-py3-none-any.whl (289 kB)\n",
      "      289.2/289.2 kB 43.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow<3.0.0dev,>=2.4.0\n",
      "  Downloading tensorflow-2.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
      "      578.1/578.1 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "      662.4/662.4 kB 66.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.8/dist-packages (from kfp) (1.2.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from kfp) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.2.0)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "      87.7/87.7 kB 19.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "      62.1/62.1 kB 13.9 MB/s eta 0:00:00\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "      152.9/152.9 kB 30.3 MB/s eta 0:00:00\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "      106.8/106.8 kB 19.9 MB/s eta 0:00:00\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "      56.3/56.3 kB 12.6 MB/s eta 0:00:00\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.16\n",
      "  Downloading kfp_pipeline_spec-0.1.16-py3-none-any.whl (19 kB)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.5.tar.gz (58 kB)\n",
      "      58.1/58.1 kB 12.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "      1.6/1.6 MB 84.5 MB/s eta 0:00:00\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "      1.0/1.0 MB 74.7 MB/s eta 0:00:00\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "      13.6/13.6 MB 96.9 MB/s eta 0:00:00\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "      54.5/54.5 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.6.1-py3-none-any.whl (38 kB)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from kfp) (4.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.14.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire<1,>=0.3.1->kfp) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire<1,>=0.3.1->kfp) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/lib/python3/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (2.22.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.56.4)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.50.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform[cloud_profiler]) (1.50.0)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "      96.6/96.6 kB 19.3 MB/s eta 0:00:00\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.1->kfp) (45.2.0)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.8.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.4.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (2.3.2)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (22.1.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2019.11.28)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.12)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.4.1)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.1)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (0.56.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (7.0.0)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (2022.5.1)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (1.2.5)\n",
      "Collecting fsspec==2022.5.0\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "      140.6/140.6 kB 26.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (4.64.1)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (1.3.5)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (1.10.0)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->nvtabular==1.3.3) (2022.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform[cloud_profiler]) (3.0.9)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.34.2)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "      438.7/438.7 kB 53.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.6.3)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.19.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "      1.1/1.1 MB 76.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.7.0)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "      1.7/1.7 MB 89.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.2.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "      5.9/5.9 MB 100.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.22.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.27.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.3.0)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (0.4.3)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (0.12.0)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (3.1.2)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (2.4.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (2.2.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.7.0)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (5.9.2)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (6.2)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.0.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform[cloud_profiler]) (1.5.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.3.3) (4.12.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->nvtabular==1.3.3) (0.39.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->nvtabular==1.3.3) (2022.2.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (3.4.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (1.8.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3.0.0dev,>=2.4.0->google-cloud-aiplatform[cloud_profiler]) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.2.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.54->merlin-core>=0.2.0->nvtabular==1.3.3) (3.8.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (1.0.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (4.1.0)\n",
      "Requirement already satisfied: multidict in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (6.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->nvtabular==1.3.3) (2.1.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->nvtabular==1.3.3) (4.0.0)\n",
      "Building wheels for collected packages: nvtabular, kfp, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for nvtabular (pyproject.toml): started\n",
      "  Building wheel for nvtabular (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nvtabular: filename=nvtabular-1.3.3-cp38-cp38-linux_x86_64.whl size=267325 sha256=9a65325c78da14889a26572d547608ecc2098169d7fa5d6c5562375a321629f3\n",
      "  Stored in directory: /root/.cache/pip/wheels/83/44/8a/3d74bc9b4c674eac4c79f3cb557b028d12ccfe221a2b1aacb4\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.14-py3-none-any.whl size=426465 sha256=4678eba9f079c62d611cebb87a1c4efe735bcd261e84141fda5a7d0c8ff0231c\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/c5/4d/d3fe2a6c76b64ebb4ae7a2a7a35b00d01bb7dcfa741e460152\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115925 sha256=8ecf23f756a76cd9dc8aa974adeeb9cb2decb28885f8fb7a4210de405f88f1ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/61/05/8d/5951e074fe660f634ca0a3402fa9903d9f772014fdb0e593dd\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.5-py3-none-any.whl size=99699 sha256=92b0e5d6bfac5465f74cfc403101faa8246ac2485927d220947d65b72e7ece85\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/3a/82/1f0a3c4193b6b4420910b6c3a6ff206358533e968e86aa4ecc\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22279 sha256=a09d719eb980a329110283dd12e495288689f3b4bf53aa578c4318e80996b0bd\n",
      "  Stored in directory: /root/.cache/pip/wheels/20/55/09/dc55b59d831b95b88ad0f65d4026aa36e9974601abe17f8659\n",
      "Successfully built nvtabular kfp fire kfp-server-api strip-hints\n",
      "Installing collected packages: keras, flatbuffers, werkzeug, uritemplate, typer, tensorflow-estimator, tabulate, strip-hints, requests-toolbelt, PyYAML, pydantic, protobuf, jsonschema, httplib2, gviz-api, fsspec, fire, docstring-parser, Deprecated, cachetools, tensorboard-plugin-profile, kfp-server-api, kfp-pipeline-spec, google-auth, kubernetes, grpcio-status, google-auth-httplib2, tensorboard, grpc-google-iam-v1, google-api-python-client, tensorflow, nvtabular, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 2.2.2\n",
      "    Uninstalling Werkzeug-2.2.2:\n",
      "      Successfully uninstalled Werkzeug-2.2.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.9.0\n",
      "    Uninstalling tensorflow-estimator-2.9.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.21.9\n",
      "    Uninstalling protobuf-4.21.9:\n",
      "      Successfully uninstalled protobuf-4.21.9\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.16.0\n",
      "    Uninstalling jsonschema-4.16.0:\n",
      "      Successfully uninstalled jsonschema-4.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.10.0\n",
      "    Uninstalling fsspec-2022.10.0:\n",
      "      Successfully uninstalled fsspec-2022.10.0\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 5.2.0\n",
      "    Uninstalling cachetools-5.2.0:\n",
      "      Successfully uninstalled cachetools-5.2.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.12.0\n",
      "    Uninstalling google-auth-2.12.0:\n",
      "      Successfully uninstalled google-auth-2.12.0\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.50.0\n",
      "    Uninstalling grpcio-status-1.50.0:\n",
      "      Successfully uninstalled grpcio-status-1.50.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.9.1\n",
      "    Uninstalling tensorboard-2.9.1:\n",
      "      Successfully uninstalled tensorboard-2.9.1\n",
      "  Attempting uninstall: nvtabular\n",
      "    Found existing installation: nvtabular 1.5.0\n",
      "    Uninstalling nvtabular-1.5.0:\n",
      "      Successfully uninstalled nvtabular-1.5.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.5.0\n",
      "    Uninstalling google-cloud-storage-2.5.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.5.0\n",
      "  Attempting uninstall: google-cloud-bigquery\n",
      "    Found existing installation: google-cloud-bigquery 3.3.5\n",
      "    Uninstalling google-cloud-bigquery-3.3.5:\n",
      "      Successfully uninstalled google-cloud-bigquery-3.3.5\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-channelz 1.49.1 requires protobuf>=4.21.3, but you have protobuf 3.19.6 which is incompatible.\n",
      "gcsfs 2022.10.0 requires fsspec==2022.10.0, but you have fsspec 2022.5.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 cachetools-4.2.4 docstring-parser-0.15 fire-0.4.0 flatbuffers-22.10.26 fsspec-2022.5.0 google-api-python-client-1.12.11 google-auth-1.35.0 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.18.2 google-cloud-bigquery-2.34.4 google-cloud-resource-manager-1.6.3 google-cloud-storage-1.44.0 grpc-google-iam-v1-0.12.4 grpcio-status-1.48.2 gviz-api-1.10.0 httplib2-0.20.4 jsonschema-3.2.0 keras-2.10.0 kfp-1.8.14 kfp-pipeline-spec-0.1.16 kfp-server-api-1.8.5 kubernetes-18.20.0 nvtabular-1.3.3 protobuf-3.19.6 pydantic-1.10.2 requests-toolbelt-0.10.1 strip-hints-0.1.10 tabulate-0.9.0 tensorboard-2.10.1 tensorboard-plugin-profile-2.8.0 tensorflow-2.10.0 tensorflow-estimator-2.10.0 typer-0.6.1 uritemplate-3.0.1 werkzeug-2.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container a3d6fc010e04\n",
      " ---> 1f5d630d5833\n",
      "Step 6/8 : RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
      " ---> Running in fdfe103fa34f\n",
      "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\n",
      "\u001b[91m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "               \u001b[0m\u001b[91m                  Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2537  100  2537    0 \u001b[0m\u001b[91m    0   130k      0 --:--:-- --:--:-- --:--:--  130k\n",
      "\u001b[0m\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:6 http://packages.cloud.google.com/apt cloud-sdk InRelease [6751 B]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [740 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [37.7 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1772 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2737 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1225 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [27.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Get:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2267 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [926 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [27.5 kB]\n",
      "Get:21 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1655 kB]\n",
      "Get:22 http://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [346 kB]\n",
      "Fetched 25.3 MB in 2s (12.6 MB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "Suggested packages:\n",
      "  google-cloud-sdk-app-engine-java google-cloud-sdk-app-engine-python\n",
      "  google-cloud-sdk-pubsub-emulator google-cloud-sdk-bigtable-emulator\n",
      "  google-cloud-sdk-datastore-emulator kubectl\n",
      "The following NEW packages will be installed:\n",
      "  google-cloud-sdk\n",
      "0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\n",
      "Need to get 129 MB of archives.\n",
      "After this operation, 714 MB of additional disk space will be used.\n",
      "Get:1 http://packages.cloud.google.com/apt cloud-sdk/main amd64 google-cloud-sdk all 407.0.0-0 [129 MB]\n",
      "Fetched 129 MB in 2s (76.5 MB/s)\n",
      "Selecting previously unselected package google-cloud-sdk.\n",
      "(Reading database ... 41552 files and directories currently installed.)\n",
      "Preparing to unpack .../google-cloud-sdk_407.0.0-0_all.deb ...\n",
      "Unpacking google-cloud-sdk (407.0.0-0) ...\n",
      "Setting up google-cloud-sdk (407.0.0-0) ...\n",
      "Removing intermediate container fdfe103fa34f\n",
      " ---> 938409930721\n",
      "Step 7/8 : COPY preprocessor/* ./\n",
      " ---> 0c4eee21238e\n",
      "Step 8/8 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n",
      " ---> Running in 91b9c487f61e\n",
      "Removing intermediate container 91b9c487f61e\n",
      " ---> fe0c4bee8460\n",
      "Successfully built fe0c4bee8460\n",
      "Successfully tagged gcr.io/hybrid-vertex/nvt-preprocessing:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/nvt-preprocessing\n",
      "The push refers to repository [gcr.io/hybrid-vertex/nvt-preprocessing]\n",
      "d73e90bc69b7: Preparing\n",
      "15376ad02676: Preparing\n",
      "df9e3fb9a7dc: Preparing\n",
      "d2fc693e1b55: Preparing\n",
      "affca008ccc9: Preparing\n",
      "5d53658e3821: Preparing\n",
      "bd113aa55fbd: Preparing\n",
      "e58f9c500afd: Preparing\n",
      "c3c9ca29c9f2: Preparing\n",
      "e844c8057c95: Preparing\n",
      "0b07555d3a5b: Preparing\n",
      "a1537cf26842: Preparing\n",
      "00679b7c9426: Preparing\n",
      "add0b850bdf8: Preparing\n",
      "c2e412b87e2d: Preparing\n",
      "7730d4c011cd: Preparing\n",
      "f83c32bae622: Preparing\n",
      "86147dce553c: Preparing\n",
      "a86afd489635: Preparing\n",
      "725647e88671: Preparing\n",
      "78e008bc66d2: Preparing\n",
      "e7d002ddb49b: Preparing\n",
      "f2b540bc31be: Preparing\n",
      "442b3d22fc2d: Preparing\n",
      "72f4d03b40d8: Preparing\n",
      "54244453f24a: Preparing\n",
      "c9dfb1d8d420: Preparing\n",
      "c1af80eb8994: Preparing\n",
      "0b07555d3a5b: Waiting\n",
      "5c0e49e0fefd: Preparing\n",
      "64579a0c8694: Preparing\n",
      "a1537cf26842: Waiting\n",
      "82432f6543d2: Preparing\n",
      "efbb58199899: Preparing\n",
      "00679b7c9426: Waiting\n",
      "f390faf5524c: Preparing\n",
      "daee9dea71d9: Preparing\n",
      "dc74b4fa6312: Preparing\n",
      "add0b850bdf8: Waiting\n",
      "cf7ec5236059: Preparing\n",
      "2784fc353e53: Preparing\n",
      "c2e412b87e2d: Waiting\n",
      "4748e1954466: Preparing\n",
      "17dd0d3a32ca: Preparing\n",
      "1a4d9b216faa: Preparing\n",
      "29ec3f10d323: Preparing\n",
      "7730d4c011cd: Waiting\n",
      "b103452845b7: Preparing\n",
      "5de10b8eda77: Preparing\n",
      "86147dce553c: Waiting\n",
      "93a1a17119ba: Preparing\n",
      "f83c32bae622: Waiting\n",
      "0b949eaca829: Preparing\n",
      "76dffad7db12: Preparing\n",
      "a86afd489635: Waiting\n",
      "103f14ded07d: Preparing\n",
      "05a0d2d578a6: Preparing\n",
      "725647e88671: Waiting\n",
      "c1af80eb8994: Waiting\n",
      "1f7bd087086a: Preparing\n",
      "a03ce844e2ad: Preparing\n",
      "78e008bc66d2: Waiting\n",
      "b5583e44add1: Preparing\n",
      "5c0e49e0fefd: Waiting\n",
      "5d53658e3821: Waiting\n",
      "3ff439c0455c: Preparing\n",
      "e7d002ddb49b: Waiting\n",
      "2ee8c052052a: Preparing\n",
      "bd113aa55fbd: Waiting\n",
      "64579a0c8694: Waiting\n",
      "f2b540bc31be: Waiting\n",
      "f3154f787b0f: Preparing\n",
      "e58f9c500afd: Waiting\n",
      "82432f6543d2: Waiting\n",
      "944a1106424f: Preparing\n",
      "442b3d22fc2d: Waiting\n",
      "01386fafb257: Preparing\n",
      "c3c9ca29c9f2: Waiting\n",
      "efbb58199899: Waiting\n",
      "8a9d499564b0: Preparing\n",
      "d882bfae03e4: Preparing\n",
      "f390faf5524c: Waiting\n",
      "e844c8057c95: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "72f4d03b40d8: Waiting\n",
      "913f47d5362d: Preparing\n",
      "daee9dea71d9: Waiting\n",
      "54244453f24a: Waiting\n",
      "06f02804b89d: Preparing\n",
      "54beb86c2dbe: Preparing\n",
      "c9dfb1d8d420: Waiting\n",
      "2784fc353e53: Waiting\n",
      "aa57b43dc9e0: Preparing\n",
      "cf7ec5236059: Waiting\n",
      "ae5c80704277: Preparing\n",
      "4748e1954466: Waiting\n",
      "b103452845b7: Waiting\n",
      "d1cc4baf7a93: Preparing\n",
      "5de10b8eda77: Waiting\n",
      "29ec3f10d323: Waiting\n",
      "17dd0d3a32ca: Waiting\n",
      "05a0d2d578a6: Waiting\n",
      "0b949eaca829: Waiting\n",
      "8fd21a588646: Preparing\n",
      "3ff439c0455c: Waiting\n",
      "1a4d9b216faa: Waiting\n",
      "b470f3b3096a: Preparing\n",
      "b5583e44add1: Waiting\n",
      "76dffad7db12: Waiting\n",
      "a03ce844e2ad: Waiting\n",
      "f3154f787b0f: Waiting\n",
      "103f14ded07d: Waiting\n",
      "944a1106424f: Waiting\n",
      "54beb86c2dbe: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "8a9d499564b0: Waiting\n",
      "d1cc4baf7a93: Waiting\n",
      "d882bfae03e4: Waiting\n",
      "06f02804b89d: Waiting\n",
      "8fd21a588646: Waiting\n",
      "913f47d5362d: Waiting\n",
      "ae5c80704277: Waiting\n",
      "9af2b05f2c3b: Preparing\n",
      "b470f3b3096a: Waiting\n",
      "4cf9aed48cda: Preparing\n",
      "57f574ab1503: Preparing\n",
      "c5b9544e7743: Preparing\n",
      "c3f11d77a5de: Preparing\n",
      "9af2b05f2c3b: Waiting\n",
      "4cf9aed48cda: Waiting\n",
      "d73e90bc69b7: Pushed\n",
      "affca008ccc9: Pushed\n",
      "5d53658e3821: Pushed\n",
      "bd113aa55fbd: Layer already exists\n",
      "e58f9c500afd: Layer already exists\n",
      "c3c9ca29c9f2: Layer already exists\n",
      "0b07555d3a5b: Layer already exists\n",
      "e844c8057c95: Layer already exists\n",
      "a1537cf26842: Layer already exists\n",
      "00679b7c9426: Layer already exists\n",
      "d2fc693e1b55: Pushed\n",
      "add0b850bdf8: Layer already exists\n",
      "c2e412b87e2d: Layer already exists\n",
      "7730d4c011cd: Layer already exists\n",
      "f83c32bae622: Layer already exists\n",
      "86147dce553c: Layer already exists\n",
      "a86afd489635: Layer already exists\n",
      "78e008bc66d2: Layer already exists\n",
      "725647e88671: Layer already exists\n",
      "e7d002ddb49b: Layer already exists\n",
      "f2b540bc31be: Layer already exists\n",
      "442b3d22fc2d: Layer already exists\n",
      "72f4d03b40d8: Layer already exists\n",
      "54244453f24a: Layer already exists\n",
      "c9dfb1d8d420: Layer already exists\n",
      "c1af80eb8994: Layer already exists\n",
      "64579a0c8694: Layer already exists\n",
      "5c0e49e0fefd: Layer already exists\n",
      "82432f6543d2: Layer already exists\n",
      "efbb58199899: Layer already exists\n",
      "f390faf5524c: Layer already exists\n",
      "daee9dea71d9: Layer already exists\n",
      "dc74b4fa6312: Layer already exists\n",
      "cf7ec5236059: Layer already exists\n",
      "2784fc353e53: Layer already exists\n",
      "17dd0d3a32ca: Layer already exists\n",
      "4748e1954466: Layer already exists\n",
      "1a4d9b216faa: Layer already exists\n",
      "b103452845b7: Layer already exists\n",
      "29ec3f10d323: Layer already exists\n",
      "5de10b8eda77: Layer already exists\n",
      "0b949eaca829: Layer already exists\n",
      "93a1a17119ba: Layer already exists\n",
      "103f14ded07d: Layer already exists\n",
      "76dffad7db12: Layer already exists\n",
      "05a0d2d578a6: Layer already exists\n",
      "1f7bd087086a: Layer already exists\n",
      "b5583e44add1: Layer already exists\n",
      "a03ce844e2ad: Layer already exists\n",
      "2ee8c052052a: Layer already exists\n",
      "3ff439c0455c: Layer already exists\n",
      "f3154f787b0f: Layer already exists\n",
      "8a9d499564b0: Layer already exists\n",
      "944a1106424f: Layer already exists\n",
      "01386fafb257: Layer already exists\n",
      "d882bfae03e4: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "913f47d5362d: Layer already exists\n",
      "06f02804b89d: Layer already exists\n",
      "54beb86c2dbe: Layer already exists\n",
      "aa57b43dc9e0: Layer already exists\n",
      "ae5c80704277: Layer already exists\n",
      "d1cc4baf7a93: Layer already exists\n",
      "8fd21a588646: Layer already exists\n",
      "9af2b05f2c3b: Layer already exists\n",
      "b470f3b3096a: Layer already exists\n",
      "4cf9aed48cda: Layer already exists\n",
      "c5b9544e7743: Layer already exists\n",
      "57f574ab1503: Layer already exists\n",
      "c3f11d77a5de: Layer already exists\n",
      "15376ad02676: Pushed\n",
      "df9e3fb9a7dc: Pushed\n",
      "latest: digest: sha256:01a5f5165305b6b626fa5de855b1d82d068153d30150410a55ab062383854ab6 size: 15485\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                            STATUS\n",
      "07b08f81-8f13-40eb-b93f-4a6ecaf4006b  2022-10-27T13:37:18+00:00  8M11S     gs://hybrid-vertex_cloudbuild/source/1666877837.763472-feeb9049dfb84553a32a137b46cc7d95.tgz  gcr.io/hybrid-vertex/nvt-preprocessing (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17fd2ca-1003-4c1f-badf-1a78b6e4757c",
   "metadata": {},
   "source": [
    "# Vertex Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b940a83-6d48-4db9-9921-50e081bd142d",
   "metadata": {},
   "source": [
    "### labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74544409-014c-45ff-85f0-538d989a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu_limit': '4',\n",
      " 'gpu_type': 'nvidia_tesla_t4',\n",
      " 'instance_type': 'n1-highmem-64',\n",
      " 'memory_limit': '624g',\n",
      " 'version': 'v1full'}\n"
     ]
    }
   ],
   "source": [
    "LABELS = {\n",
    "    'version': f'{VERSION}',\n",
    "    'gpu_type': f'{GPU_TYPE.lower()}',\n",
    "    'gpu_limit': f'{GPU_LIMIT}',\n",
    "    'memory_limit': f'{MEMORY_LIMIT.lower()}',\n",
    "    'instance_type': f'{INSTANCE_TYPE}',\n",
    "}\n",
    "pprint(LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e34e6-9891-421e-a62d-2e6e49a66a44",
   "metadata": {},
   "source": [
    "## define pipe params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "105398a1-be24-4c85-94d9-fdfa45ee0e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'app': 'spotify',\n",
      " 'bucket_data_output': 'jt-merlin-scaling',\n",
      " 'bucket_data_src': 'spotify-beam-v3',\n",
      " 'file_pattern': '*.snappy.parquet',\n",
      " 'num_output_files_train': 100,\n",
      " 'num_output_files_valid': 10,\n",
      " 'output_path_analyzed_dir': 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed',\n",
      " 'output_path_defined_dir': 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-defined',\n",
      " 'output_path_transformed_dir': 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed',\n",
      " 'shuffle': 'null',\n",
      " 'train_prefix': 'train_data_parquet',\n",
      " 'valid_prefix': 'valid_data_parquet',\n",
      " 'version': 'v1full'}\n"
     ]
    }
   ],
   "source": [
    "# from google.cloud import storage\n",
    "\n",
    "# storage_client = storage.Client()\n",
    "\n",
    "TRAIN_PREFIX = 'train_data_parquet'\n",
    "VALID_PREFIX = 'valid_data_parquet'\n",
    "\n",
    "delimiter = '/'\n",
    "# FILE_PATTERN = \"*.parquet\"                    # full\n",
    "FILE_PATTERN = '*.snappy.parquet'    # subset\n",
    "\n",
    "# trying to achieve avg file size of ~100 mb\n",
    "num_output_files_train = 100 #0 # Number of output Parquet files\n",
    "num_output_files_valid = 10 #2 # Number of output Parquet files\n",
    "\n",
    "# Define output directories\n",
    "OUTPUT_DEFINED_DIR = os.path.join(WORKSPACE, \"nvt-defined\")\n",
    "OUTPUT_WORKFLOW_DIR = os.path.join(WORKSPACE, \"nvt-analyzed\")\n",
    "OUTPUT_TRANSFORMED_DIR = os.path.join(WORKSPACE, \"nvt-processed\")\n",
    "\n",
    "\n",
    "parq_parameter_values = {\n",
    "    'bucket_data_src': BUCKET_source,\n",
    "    'bucket_data_output': BUCKET_destin,\n",
    "    'train_prefix': f'{TRAIN_PREFIX}',\n",
    "    'valid_prefix': f'{VALID_PREFIX}',\n",
    "    'file_pattern': f'{FILE_PATTERN}',\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid,\n",
    "    'output_path_defined_dir': f'{OUTPUT_DEFINED_DIR}',\n",
    "    'output_path_analyzed_dir': f'{OUTPUT_WORKFLOW_DIR}',\n",
    "    'output_path_transformed_dir': f'{OUTPUT_TRANSFORMED_DIR}',\n",
    "    'version':f'{VERSION}',\n",
    "    'shuffle': json.dumps(None), # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "    'app':f'{APP}',\n",
    "}\n",
    "\n",
    "pprint(parq_parameter_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6527bc-5ad8-4bea-9694-3e1b2763e290",
   "metadata": {},
   "source": [
    "## compile pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75150bb6-c0ad-4e8c-9a08-bc707d2dce67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin/src'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list the current work dir\n",
    "os.chdir('/home/jupyter/spotify-merlin/src')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d07797b-3541-4f2b-869b-bc1e7184fc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile.hugectr\t     cloudbuild.yaml\n",
      "Dockerfile.merlintf\t     hugectr\n",
      "Dockerfile.merlintf-22_07    nvt-parquet-pipeline-v1-subset.json\n",
      "Dockerfile.merlintf-22_09    nvt-parquet-pipeline.json\n",
      "Dockerfile.merlintf-22_10    nvt-parquet-v3sub.json\n",
      "Dockerfile.merlintf-nightly  pipelines\n",
      "Dockerfile.nvt-133\t     pipes\n",
      "Dockerfile.nvtabular\t     preprocessing\n",
      "Dockerfile.nvtabular-133     preprocessor\n",
      "Dockerfile.nvtabular-160     trainer\n",
      "Dockerfile.triton\t     training\n",
      "Untitled.ipynb\t\t     wip_hold\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c34709df-46f9-4f2a-9422-e196cf287345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipes.preproc_pipelines import preprocessing_parquet\n",
    "\n",
    "_compiled_pipeline_path = f'{PREPROCESS_PARQUET_PIPELINE_NAME}.json'\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_parquet,\n",
    "       package_path=_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c41f7b-5a55-4af0-8331-4e61f0fabadf",
   "metadata": {},
   "source": [
    "## submit pipeline to Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "18a23dfe-2f1e-4863-9868-ea62644b204f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nvt-parquet-v1full'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREPROCESS_PARQUET_PIPELINE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5305c2e3-d1bd-4349-9e83-e5539c11b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "job_name = f'{PREPROCESS_PARQUET_PIPELINE_NAME}_{TIMESTAMP}' #{TIMESTAMP}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parq_parameter_values,\n",
    "    labels=LABELS,\n",
    ")\n",
    "\n",
    "pipeline_job.submit(service_account=VERTEX_SA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ead4c-47ec-45ec-9b67-cb158f0f5f2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Local Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4417394b-922a-4452-9502-b7cf5fef2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_raw = nvt.Dataset(f\"gs://{BUCKET_source}/{TRAIN_SRC_DIR}/*.snappy.parquet\")\n",
    "# valid_raw = nvt.Dataset(f\"gs://{BUCKET_source}/{VALID_SRC_DIR}/*.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deab76-3e86-41e1-8953-be1b7e854787",
   "metadata": {},
   "source": [
    "### Define the NVTabular preprocessing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c986af4-9b1e-4ee2-a143-971b9b7f82a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = [\"track_uri_can\"] >> Categorify(dtype=\"int32\") >> TagAsItemID() \n",
    "playlist_id = [\"pid\"] >> Categorify(dtype=\"int32\") >> TagAsUserID() \n",
    "\n",
    "\n",
    "item_features_cat = ['artist_name_can',\n",
    "        'track_name_can',\n",
    "        'artist_genres_can',\n",
    "    ]\n",
    "\n",
    "item_features_cont = [\n",
    "        'duration_ms_can',\n",
    "        'track_pop_can',\n",
    "        'artist_pop_can',\n",
    "        'artist_followers_can',\n",
    "    ]\n",
    "\n",
    "playlist_features_cat = [\n",
    "        'description_pl',\n",
    "        'name',\n",
    "        'collaborative',\n",
    "    ]\n",
    "\n",
    "playlist_features_cont = [\n",
    "        'duration_ms_seed_pl',\n",
    "        'n_songs_pl',\n",
    "        'num_artists_pl',\n",
    "        'num_albums_pl',\n",
    "    ]\n",
    "\n",
    "seq_feats_cat = [\n",
    "        'artist_name_pl',\n",
    "        'track_uri_pl',\n",
    "        'track_name_pl',\n",
    "        'album_name_pl',\n",
    "        'artist_genres_pl',\n",
    "    ]\n",
    "\n",
    "CAT = playlist_features_cat + item_features_cat\n",
    "CONT = item_features_cont + playlist_features_cont\n",
    "\n",
    "item_feature_cat_node = item_features_cat >> nvt.ops.FillMissing()>> Categorify(dtype=\"int32\") >> TagAsItemFeatures()\n",
    "\n",
    "item_feature_cont_node =  item_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsItemFeatures()\n",
    "\n",
    "playlist_feature_cat_node = playlist_features_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() \n",
    "\n",
    "playlist_feature_cont_node = playlist_features_cont >> nvt.ops.FillMissing() >>  nvt.ops.Normalize() >> TagAsUserFeatures()\n",
    "\n",
    "playlist_feature_cat_seq_node = seq_feats_cat >> nvt.ops.FillMissing() >> Categorify(dtype=\"int32\") >> TagAsUserFeatures() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e2a3d3-8c66-404b-9ceb-976577e3c066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset_definition(\n",
    "    # data_paths,\n",
    "    # recursive,\n",
    "    # col_dtypes,\n",
    "    frac_size,\n",
    "    bucket_name,\n",
    "    data_prefix,\n",
    "    file_pattern,\n",
    "    # sep='\\t'\n",
    "):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
