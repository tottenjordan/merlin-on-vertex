{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20fb7941-cf7c-4ff4-aab3-86f68879e618",
   "metadata": {},
   "source": [
    "# Train and Deploy Merlin models with Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a796242f-4f0e-454e-aba0-5eb6ba87f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kfp\n",
    "# !pip install google-cloud-pipeline-components --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5337f359-59e6-4bff-a1cd-3504e500c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "# ! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\"\n",
    "# ! python3 -c \"import google.cloud.aiplatform; print('aiplatform SDK version: {}'.format(google.cloud.aiplatform.__version__))\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 141,
=======
   "execution_count": 1,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "550909fb-6604-4907-8cb9-14395fd2b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 142,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 2,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "e184436e-2656-4d6e-bb31-bc2d1147b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 143,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 3,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "b42c134e-2fef-49e9-aaa2-c33906673e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "\n",
    "# Pipelines\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "\n",
    "# Kubeflow SDK\n",
    "# TODO: fix these\n",
    "from kfp.v2 import dsl\n",
    "import kfp\n",
    "import kfp.v2.dsl\n",
    "from kfp.v2.google import client as pipelines_client\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component)\n",
    "\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "\n",
    "vertex_ai.init(project=PROJECT_ID,location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 144,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 4,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "6abff2a0-3022-4d0e-9423-4cc067da146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "PIPELINES_SUB_DIR = 'train_pipes'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 145,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 5,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "3e50ebd1-4f72-41b1-89e1-3be35d62bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2d38e-459a-4ccd-9ae8-928a86ed2474",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 146,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 6,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "cd5c12ae-40ef-485a-aa02-c0fc116ca33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/merlin-on-vertex'"
      ]
     },
<<<<<<< HEAD
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 146,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
     "execution_count": 6,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98a7ed-ec46-43b6-aad0-18e4edc040fb",
   "metadata": {},
   "source": [
    "## Build Custom Image"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 147,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 7,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "55c8639e-d29a-4441-a315-1ce0b1c33ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/build_custom_image.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/build_custom_image.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"gcr.io/google.com/cloudsdktool/cloud-sdk:latest\",\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-build\"\n",
    "    ],\n",
    ")\n",
    "def build_custom_image(\n",
    "    project: str,\n",
    "    artifact_gcs_path: str,\n",
    "    docker_name: str,\n",
    "    app_dir_name: str,\n",
    "    custom_image_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('custom_image_uri', str),\n",
    "]):\n",
    "    # TODO: make output Artifact for image_uri\n",
    "    \"\"\"\n",
    "    custom pipeline component to build custom image using\n",
    "    Cloud Build, the training/serving application code, and dependencies\n",
    "    defined in the Dockerfile\n",
    "    \"\"\"\n",
    "    \n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud.devtools import cloudbuild_v1 as cloudbuild\n",
    "    from google.protobuf.duration_pb2 import Duration\n",
    "\n",
    "    # initialize client for cloud build\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    build_client = cloudbuild.services.cloud_build.CloudBuildClient()\n",
    "    \n",
    "    # parse step inputs to get path to Dockerfile and training application code\n",
    "    _gcs_dockerfile_path = os.path.join(artifact_gcs_path, f\"{docker_name}\") # Dockerfile.XXXXX\n",
    "    _gcs_script_dir_path = os.path.join(artifact_gcs_path, f\"{app_dir_name}/\") # \"trainer/\"\n",
    "    \n",
    "    logging.info(f\"_gcs_dockerfile_path: {_gcs_dockerfile_path}\")\n",
    "    logging.info(f\"_gcs_script_dir_path: {_gcs_script_dir_path}\")\n",
    "    \n",
    "    # define build steps to pull the training code and Dockerfile\n",
    "    # and build/push the custom training container image\n",
    "    build = cloudbuild.Build()\n",
    "    build.steps = [\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", \"-r\", _gcs_script_dir_path, \".\"],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/gsutil\",\n",
    "            \"args\": [\"cp\", _gcs_dockerfile_path, \"Dockerfile\"],\n",
    "        },\n",
    "        # enabling Kaniko cache in a Docker build that caches intermediate\n",
    "        # layers and pushes image automatically to Container Registry\n",
    "        # https://cloud.google.com/build/docs/kaniko-cache\n",
    "        # {\n",
    "        #     \"name\": \"gcr.io/kaniko-project/executor:latest\",\n",
    "        #     # \"name\": \"gcr.io/kaniko-project/executor:v1.8.0\",        # TODO; downgraded to avoid error in build\n",
    "        #     # \"args\": [f\"--destination={training_image_uri}\", \"--cache=true\"],\n",
    "        #     \"args\": [f\"--destination={training_image_uri}\", \"--cache=false\"],\n",
    "        # },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['build','-t', f'{custom_image_uri}', '.'],\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"gcr.io/cloud-builders/docker\",\n",
    "            \"args\": ['push', f'{custom_image_uri}'], \n",
    "        },\n",
    "    ]\n",
    "    # override default timeout of 10min\n",
    "    timeout = Duration()\n",
    "    timeout.seconds = 7200\n",
    "    build.timeout = timeout\n",
    "\n",
    "    # create build\n",
    "    operation = build_client.create_build(project_id=project, build=build)\n",
    "    logging.info(\"IN PROGRESS:\")\n",
    "    logging.info(operation.metadata)\n",
    "\n",
    "    # get build status\n",
    "    result = operation.result()\n",
    "    logging.info(\"RESULT:\", result.status)\n",
    "\n",
    "    # return step outputs\n",
    "    return (\n",
    "        custom_image_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad9cd2-58a1-4b75-a608-cbc1888ad921",
   "metadata": {},
   "source": [
    "## Train Job"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 148,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 8,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "0d882a90-2445-463e-9005-f83a87f78682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/train_merlin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/train_merlin.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def train_merlin(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    train_image_uri: str,     # TODO: Artifact\n",
    "    tb_resource: str,\n",
    "    batch_size: int, \n",
    "    train_epochs: int,\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    service_account: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('merlin_model_gcs_dir', str),\n",
    "    ('query_tower_gcs_dir', str),\n",
    "    ('candidate_tower_gcs_uri', str),\n",
    "    ('candidate_embeddings_gcs_uri', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    # ====================================================\n",
    "    # Helper function for workerpool specs\n",
    "    # ====================================================\n",
    "    def prepare_worker_pool_specs(\n",
    "        image_uri,\n",
    "        # args,\n",
    "        cmd,\n",
    "        replica_count=1,\n",
    "        machine_type=\"n1-standard-16\",\n",
    "        accelerator_count=1,\n",
    "        accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "        reduction_server_count=0,\n",
    "        reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "        reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "    ):\n",
    "\n",
    "        if accelerator_count > 0:\n",
    "            machine_spec = {\n",
    "                \"machine_type\": machine_type,\n",
    "                \"accelerator_type\": accelerator_type,\n",
    "                \"accelerator_count\": accelerator_count,\n",
    "            }\n",
    "        else:\n",
    "            machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "        container_spec = {\n",
    "            \"image_uri\": image_uri,\n",
    "            # \"args\": args,\n",
    "            \"command\": cmd,\n",
    "        }\n",
    "\n",
    "        chief_spec = {\n",
    "            \"replica_count\": 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "\n",
    "        worker_pool_specs = [chief_spec]\n",
    "        if replica_count > 1:\n",
    "            workers_spec = {\n",
    "                \"replica_count\": replica_count - 1,\n",
    "                \"machine_spec\": machine_spec,\n",
    "                \"container_spec\": container_spec,\n",
    "            }\n",
    "            worker_pool_specs.append(workers_spec)\n",
    "        if reduction_server_count > 1:\n",
    "            workers_spec = {\n",
    "                \"replica_count\": reduction_server_count,\n",
    "                \"machine_spec\": {\n",
    "                    \"machine_type\": reduction_server_machine_type,\n",
    "                },\n",
    "                \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "            }\n",
    "            worker_pool_specs.append(workers_spec)\n",
    "\n",
    "        return worker_pool_specs\n",
    "    \n",
    "    # ====================================================\n",
    "    # Define device strategy\n",
    "    # ====================================================\n",
    "    # TODO: parameterize\n",
    "    \n",
    "    WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "    REPLICA_COUNT = 1\n",
    "    ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "    PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "    REDUCTION_SERVER_COUNT = 0                                                      \n",
    "    REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "    DISTRIBUTE_STRATEGY = 'single'\n",
    "    \n",
    "    # ====================================================\n",
    "    # # DEFINE ARGS\n",
    "    # ====================================================\n",
    "    # TODO: parameterize\n",
    "    \n",
    "    BATCH_SIZE = 4096*4      # TODO: `batch_size * 4 ? jw\n",
    "    LEARNING_RATE = 0.001\n",
    "    LAYERS = \"[512, 256, 128]\"\n",
    "    \n",
    "    OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "    \n",
    "    EXPERIMENT_RUN = f'{experiment_run}-{TIMESTAMP}'\n",
    "    \n",
    "    WORKER_CMD = [\n",
    "        'sh',\n",
    "        '-euc',\n",
    "        f'''pip freeze && python -m trainer.train_task --tb_name={tb_resource} --per_gpu_batch_size={batch_size} \\\n",
    "        --train_output_bucket={OUTPUT_BUCKET} --train_dir={train_dir} --valid_dir={valid_dir} --workflow_dir={workflow_dir} \\\n",
    "        --num_epochs={train_epochs} --learning_rate={LEARNING_RATE} --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "        --experiment_name={experiment_name} --experiment_run={EXPERIMENT_RUN} --project={project} --location={location}'''\n",
    "    ]\n",
    "    \n",
    "    WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "        image_uri=train_image_uri,\n",
    "        # args=WORKER_ARGS,\n",
    "        cmd=WORKER_CMD,\n",
    "        replica_count=REPLICA_COUNT,\n",
    "        machine_type=WORKER_MACHINE_TYPE,\n",
    "        accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "        accelerator_type=ACCELERATOR_TYPE,\n",
    "        reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "        reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    "    )\n",
    "    # ==============================================================================\n",
    "    # Submit Train Job \n",
    "    # ==============================================================================\n",
    "    STAGING_BUCKET = f'gs://{OUTPUT_BUCKET}/{experiment_name}'\n",
    "    JOB_NAME = f'train-merlin-retrieval-{version}'\n",
    "    gpu_type = ACCELERATOR_TYPE.lower() # lowercase for labels\n",
    "\n",
    "    job = vertex_ai.CustomJob(\n",
    "        display_name=JOB_NAME,\n",
    "        worker_pool_specs=WORKER_POOL_SPECS,\n",
    "        staging_bucket=STAGING_BUCKET,\n",
    "        labels={\n",
    "            'gpu': f'{gpu_type}',\n",
    "            'gpu_per_replica' : f'{PER_MACHINE_ACCELERATOR_COUNT}',\n",
    "            'replica_cnt' : f'{REPLICA_COUNT}',\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    job.run(\n",
    "        sync=True, \n",
    "        service_account=service_account,\n",
    "        # tensorboard=EXPERIMENT_TB,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    "    )\n",
    "    \n",
    "    # uris set during train script\n",
    "    WORKING_DIR_GCS_URI = f'gs://{OUTPUT_BUCKET}/{experiment_name}/{EXPERIMENT_RUN}'\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model-dir\"\n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query-tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate-tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate-embeddings\"\n",
    "    \n",
    "    logging.info(f'WORKING_DIR_GCS_URI: {WORKING_DIR_GCS_URI}')\n",
    "    logging.info(f'MODEL_DIR: {MODEL_DIR}')\n",
    "    logging.info(f'QUERY_TOWER_PATH: {QUERY_TOWER_PATH}')\n",
    "    logging.info(f'CANDIDATE_TOWER_PATH: {CANDIDATE_TOWER_PATH}')\n",
    "    logging.info(f'EMBEDDINGS_PATH: {EMBEDDINGS_PATH}')\n",
    "    \n",
    "    return (\n",
    "        f'{MODEL_DIR}',\n",
    "        f'{QUERY_TOWER_PATH}',\n",
    "        f'{CANDIDATE_TOWER_PATH}',\n",
    "        f'{EMBEDDINGS_PATH}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b2fbb9-497b-4bcd-9f0c-89b9afee36f1",
   "metadata": {},
   "source": [
    "## Custom Model Upload"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 150,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 9,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "95c88ec3-bca9-4edb-8b43-7f63f5198a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/upload_custom_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/upload_custom_model.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def upload_custom_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    display_name: str,\n",
    "    artifact_uri: str,\n",
    "    unmanaged_container_model: Input[Artifact],\n",
    "    serving_container_image_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('model', Model),\n",
    "    ('model_resource_name', str),\n",
    "]):\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    logging.info(f\" display_name: {display_name}\")\n",
    "    logging.info(f\" artifact_uri: {artifact_uri}\")\n",
    "    logging.info(f\" unmanaged_container_model: {unmanaged_container_model}\")\n",
    "    logging.info(f\" serving_container_image_uri: {serving_container_image_uri}\")\n",
    "    \n",
    "    logging.info(f\"Uploading model to Vertex...\")\n",
    "    model = vertex_ai.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_predict_route='/predict',\n",
    "        serving_container_health_route='/health',\n",
    "        serving_container_command=[\"sh\", \"-c\", \"uvicorn app.main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"],\n",
    "        serving_container_args='--gpus all',\n",
    "        # parent_model=PARENT_MODEL,\n",
    "        sync=True,\n",
    "    )\n",
    "    \n",
    "    MODEL_RESOURCE_NAME = model.resource_name\n",
    "    logging.info(f\" MODEL_RESOURCE_NAME: {MODEL_RESOURCE_NAME}\")\n",
    "    \n",
    "    return (\n",
    "        model,\n",
    "        f'{MODEL_RESOURCE_NAME}',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6c711-328f-4779-bcec-91bcefff76dc",
   "metadata": {},
   "source": [
    "## Create ANN Index"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 151,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 10,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "bf39168b-735f-447a-bcf4-d58474879c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str, \n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    ann_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    leaf_node_embedding_count: int,\n",
    "    leaf_nodes_to_search_percent: int, \n",
    "    ann_index_description: str,\n",
    "    # ann_index_labels: Dict, \n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('ann_index', Artifact),\n",
    "]):\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(f\"ENDPOINT: {ENDPOINT}\")\n",
    "    logging.info(f\"project: {project}\")\n",
    "    logging.info(f\"location: {location}\")\n",
    "    logging.info(f\"INDEX_DIR_GCS: {INDEX_DIR_GCS}\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "        \n",
    "    tree_ah_index = vertex_ai.MatchingEngineIndex.create_tree_ah_index(\n",
    "        display_name=f'{ann_index_display_name}-{TIMESTAMP}',\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}/', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        leaf_node_embedding_count=leaf_node_embedding_count,\n",
    "        leaf_nodes_to_search_percent=leaf_nodes_to_search_percent,\n",
    "        description=ann_index_description,\n",
    "        # labels=ann_index_labels,\n",
    "        # sync=True,\n",
    "    )\n",
    "\n",
    "    end = time.time()\n",
    "    elapsed_time = round((end - start), 2)\n",
    "    logging.info(f'Elapsed time creating index: {elapsed_time} seconds\\n')\n",
    "    \n",
    "    ann_index_resource_uri = tree_ah_index.resource_name\n",
    "    logging.info(\"ann_index_resource_uri:\", ann_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{ann_index_resource_uri}',\n",
    "      tree_ah_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26d57a-354a-4083-86aa-cafdcbe284fa",
   "metadata": {},
   "source": [
    "## Create brute force index"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 152,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 11,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "78389c19-9a24-439d-aa43-ca5a06c3ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_brute_force_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_brute_force_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_brute_force_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    emb_index_gcs_uri: str,\n",
    "    dimensions: int,\n",
    "    brute_force_index_display_name: str,\n",
    "    approximate_neighbors_count: int,\n",
    "    distance_measure_type: str,\n",
    "    brute_force_index_description: str,\n",
    "    # brute_force_index_labels: Dict,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('brute_force_index', Artifact),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    ENDPOINT = \"{}-aiplatform.googleapis.com\".format(location)\n",
    "    NETWORK_NAME = vpc_network_name\n",
    "    INDEX_DIR_GCS = emb_index_gcs_uri\n",
    "    PARENT = \"projects/{}/locations/{}\".format(project, location)\n",
    "\n",
    "    logging.info(\"ENDPOINT: {}\".format(ENDPOINT))\n",
    "    logging.info(\"PROJECT_ID: {}\".format(project))\n",
    "    logging.info(\"REGION: {}\".format(location))\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # Create Index \n",
    "    # ==============================================================================\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    brute_force_index = vertex_ai.MatchingEngineIndex.create_brute_force_index(\n",
    "        display_name=f'{brute_force_index_display_name}-{TIMESTAMP}',\n",
    "        contents_delta_uri=f'{emb_index_gcs_uri}/', # emb_index_gcs_uri,\n",
    "        dimensions=dimensions,\n",
    "        # approximate_neighbors_count=approximate_neighbors_count,\n",
    "        distance_measure_type=distance_measure_type,\n",
    "        description=brute_force_index_description,\n",
    "        # labels=brute_force_index_labels,\n",
    "        # sync=True,\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_force_index.resource_name\n",
    "    print(\"brute_force_index_resource_uri:\",brute_force_index_resource_uri) \n",
    "\n",
    "    return (\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      brute_force_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d56f08-c92a-405f-8432-9866add59005",
   "metadata": {},
   "source": [
    "## Create ANN index endpoint"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 153,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 12,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "cd0c7a4c-6270-43b8-83a9-e11f82aaaea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_ann_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_ann_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_ann_index_endpoint_vpc(\n",
    "    ann_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    ann_index_endpoint_display_name: str,\n",
    "    ann_index_endpoint_description: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('ann_index_endpoint_resource_uri', str),\n",
    "    ('ann_index_endpoint', Artifact),\n",
    "    ('ann_index_endpoint_display_name', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    ann_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{ann_index_endpoint_display_name}',\n",
    "        description=ann_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    ann_index_endpoint_resource_uri = ann_index_endpoint.resource_name\n",
    "    logging.info(f\"ann_index_endpoint_resource_uri: {ann_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "        f'{vpc_network_resource_uri}',\n",
    "        f'{ann_index_endpoint_resource_uri}',\n",
    "        ann_index_endpoint,\n",
    "        f'{ann_index_endpoint_display_name}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12337261-4471-49de-af2d-7c09f5196fa5",
   "metadata": {},
   "source": [
    "## Create brute force index endpoint"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 154,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 13,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "6478a8e7-da31-46f1-9f7b-22b90a38a002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/create_brute_index_endpoint_vpc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/create_brute_index_endpoint_vpc.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def create_brute_index_endpoint_vpc(\n",
    "    bf_index_artifact: Input[Artifact],\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    vpc_network_name: str,\n",
    "    brute_index_endpoint_display_name: str,\n",
    "    brute_index_endpoint_description: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('vpc_network_resource_uri', str),\n",
    "    ('brute_index_endpoint_resource_uri', str),\n",
    "    ('brute_index_endpoint', Artifact),\n",
    "    ('brute_index_endpoint_display_name', str),\n",
    "]):\n",
    "\n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    \n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    vpc_network_resource_uri = f'projects/{project_number}/global/networks/{vpc_network_name}'\n",
    "    logging.info(f\"vpc_network_resource_uri: {vpc_network_resource_uri}\")\n",
    "\n",
    "    brute_index_endpoint = vertex_ai.MatchingEngineIndexEndpoint.create(\n",
    "        display_name=f'{brute_index_endpoint_display_name}',\n",
    "        description=brute_index_endpoint_description,\n",
    "        network=vpc_network_resource_uri,\n",
    "    )\n",
    "    brute_index_endpoint_resource_uri = brute_index_endpoint.resource_name\n",
    "    logging.info(f\"brute_index_endpoint_resource_uri: {brute_index_endpoint_resource_uri}\")\n",
    "\n",
    "    return (\n",
    "      f'{vpc_network_resource_uri}',\n",
    "      f'{brute_index_endpoint_resource_uri}',\n",
    "      brute_index_endpoint,\n",
    "      f'{brute_index_endpoint_display_name}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875fbd6-a4f4-42f0-8186-65a8f4bc9156",
   "metadata": {},
   "source": [
    "## Deploy ANN Index"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 155,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 14,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "6af0825f-68a1-4baa-afce-2a9375c5c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/deploy_ann_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/deploy_ann_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1'\n",
    "    ]\n",
    ")\n",
    "def deploy_ann_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_ann_index_name: str,\n",
    "    ann_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('ann_index_resource_uri', str),\n",
    "    ('deployed_ann_index_name', str),\n",
    "    ('deployed_ann_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    VERSION = version\n",
    "    \n",
    "    ann_index = vertex_ai.MatchingEngineIndex(\n",
    "      index_name=ann_index_resource_uri\n",
    "    )\n",
    "    ann_index_resource_uri = ann_index.resource_name\n",
    "\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(\n",
    "      index_endpoint_resource_uri\n",
    "    )\n",
    "\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "      index=ann_index, \n",
    "      deployed_index_id=f'{deployed_ann_index_name}' #-{TIMESTAMP}'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{ann_index_resource_uri}',\n",
    "      f'{deployed_ann_index_name}',\n",
    "      ann_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376252ba-4728-4406-8e7f-aa54b2a4d240",
   "metadata": {},
   "source": [
    "## Deploy brute force Index"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 16,
=======
   "execution_count": 156,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 15,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "f6d9149f-fbaf-4703-8850-4bc975d266c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train_pipes/deploy_brute_index.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{PIPELINES_SUB_DIR}/deploy_brute_index.py\n",
    "\n",
    "import kfp\n",
    "from typing import Any, Callable, Dict, NamedTuple, Optional, List\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, component, Metrics)\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform==1.18.1',\n",
    "        # 'google-cloud-storage',\n",
    "    ],\n",
    ")\n",
    "def deploy_brute_index(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    version: str,\n",
    "    deployed_brute_force_index_name: str,\n",
    "    brute_force_index_resource_uri: str,\n",
    "    index_endpoint_resource_uri: str,\n",
    ") -> NamedTuple('Outputs', [\n",
    "    ('index_endpoint_resource_uri', str),\n",
    "    ('brute_force_index_resource_uri', str),\n",
    "    ('deployed_brute_force_index_name', str),\n",
    "    ('deployed_brute_force_index', Artifact),\n",
    "]):\n",
    "  \n",
    "    import logging\n",
    "    from google.cloud import aiplatform as vertex_ai\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "\n",
    "    vertex_ai.init(\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "    TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    brute_index = vertex_ai.MatchingEngineIndex(\n",
    "        index_name=brute_force_index_resource_uri\n",
    "    )\n",
    "    brute_force_index_resource_uri = brute_index.resource_name\n",
    "\n",
    "    index_endpoint = vertex_ai.MatchingEngineIndexEndpoint(index_endpoint_resource_uri)\n",
    "\n",
    "    index_endpoint = index_endpoint.deploy_index(\n",
    "        index=brute_index, \n",
    "        deployed_index_id=f'{deployed_brute_force_index_name}', #-{TIMESTAMP}'\n",
    "    )\n",
    "\n",
    "    logging.info(f\"index_endpoint.deployed_indexes: {index_endpoint.deployed_indexes}\")\n",
    "\n",
    "    return (\n",
    "      f'{index_endpoint_resource_uri}',\n",
    "      f'{brute_force_index_resource_uri}',\n",
    "      f'{deployed_brute_force_index_name}', #-{TIMESTAMP}',\n",
    "      brute_index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f6ed0-5fbd-4168-bba8-45b4d5899340",
   "metadata": {},
   "source": [
    "# Build & Compile Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cb337-7015-419e-aa3b-40f8bcda6db7",
   "metadata": {},
   "source": [
    "### pipe configs"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 157,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 17,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "fc7d180f-1158-4769-a583-1b2ca6969b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "PIPELINE_TAG: merlin-train-deploy--v7\n"
=======
      "PIPELINE_TAG: merlin-train-deploy-jtv7\n"
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
      "PIPELINE_TAG: merlin-train-deploy-jtv11\n"
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "PIPELINE_VERSION = 'v7' # pipeline code\n",
    "PIPELINE_TAG = f'merlin-train-deploy--{PIPELINE_VERSION}'\n",
=======
    "PIPELINE_VERSION = 'jtv7' # pipeline code\n",
=======
    "PIPELINE_VERSION = 'jtv11' # pipeline code\n",
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
    "PIPELINE_TAG = f'merlin-train-deploy-{PIPELINE_VERSION}'\n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
    "print(\"PIPELINE_TAG:\", PIPELINE_TAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1268a06a-1214-470b-ad73-3203c0010291",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 18,
   "id": "8e03fdbc-96f7-4185-8d0f-a3e30525c494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-central1-aiplatform.googleapis.com/v1/projects/934903580331/locations/us-central1/endpoints/3308390905559384064/operations/6971859539301761024'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"resources\":[{\"resourceType\":\"VertexLro\",\"resourceUri\":\"https://us-central1-aiplatform.googleapis.com/v1/projects/934903580331/locations/us-central1/endpoints/3308390905559384064/operations/6971859539301761024\"}]}\n",
    "\n",
    "a['resources'][0]['resourceUri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
=======
   "execution_count": 169,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 18,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "cdc191f5-b605-4b53-9f8d-7cf5dc2a9742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_pipes import train_merlin, build_custom_image, upload_custom_model, \\\n",
    "                            create_ann_index, create_brute_force_index, create_ann_index_endpoint_vpc, \\\n",
    "                            create_brute_index_endpoint_vpc, deploy_ann_index, deploy_brute_index\n",
    "\n",
    "@kfp.v2.dsl.pipeline(\n",
    "    name=f'{PIPELINE_VERSION}-{PIPELINE_TAG}'.replace('_', '-')\n",
    ")\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    version:str,\n",
    "    vpc_network_name: str,\n",
    "    pipe_gcs_path: str,\n",
    "    training_image_uri: str,\n",
    "    serving_image_uri: str,\n",
    "    train_docker_name: str,\n",
    "    serving_docker_name: str,\n",
    "    tb_resource: str,\n",
    "    batch_size: int,\n",
    "    train_epochs: int,\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    experiment_name: str,\n",
    "    experiment_run: str,\n",
    "    service_account: str,\n",
    "):\n",
    "    \n",
    "    from kfp.v2.components import importer_node\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    # ========================================================================\n",
    "    # TODO: data processing steps\n",
    "    # ========================================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build TRAIN Image\n",
    "    # ========================================================================\n",
    "    build_custom_train_image_op = (\n",
    "        build_custom_image.build_custom_image(\n",
    "            project=project,\n",
    "            artifact_gcs_path=f'{pipe_gcs_path}',\n",
    "            app_dir_name='trainer',\n",
    "            docker_name=train_docker_name,\n",
    "            custom_image_uri=training_image_uri,\n",
    "        )\n",
    "        .set_display_name(\"Build Train Image\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Train Merlin Towers\n",
    "    # ========================================================================\n",
    "    \n",
    "    train_merlin_op = (\n",
    "        train_merlin.train_merlin(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            train_image_uri=build_custom_train_image_op.outputs['custom_image_uri'],\n",
    "            tb_resource=tb_resource,\n",
    "            batch_size=batch_size,\n",
    "            train_epochs=train_epochs,\n",
    "            train_dir=train_dir,\n",
    "            valid_dir=valid_dir,\n",
    "            workflow_dir=workflow_dir,\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_run=experiment_run,\n",
    "            service_account=service_account,\n",
    "        )\n",
    "        .set_display_name(\"Train Merlin Towers\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Build SERVING Image\n",
    "    # ========================================================================\n",
    "    build_custom_serving_image_op = (\n",
    "        build_custom_image.build_custom_image(\n",
    "            project=project,\n",
    "            artifact_gcs_path=f'{pipe_gcs_path}',\n",
    "            app_dir_name='serving',\n",
    "            docker_name=serving_docker_name,\n",
    "            custom_image_uri=serving_image_uri,\n",
    "        )\n",
    "        .set_display_name(\"Build Serving Image\")\n",
    "        # .after(build_custom_train_image_op)\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "       \n",
    "    # ========================================================================\n",
    "    # Import Trained Towers to Pipeline DAG\n",
    "    # ========================================================================\n",
    "    import_query_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=train_merlin_op.outputs['merlin_model_gcs_dir'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel, #https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ModelContainerSpec\n",
    "            metadata={'containerSpec':{'imageUri': build_custom_serving_image_op.outputs[\"custom_image_uri\"],\n",
    "                                      'command': [\"sh\", \"-c\", \"uvicorn app.main:app --host 0.0.0.0 --port $AIP_HTTP_PORT\"]},\n",
    "                                      'healthRoute': '/health',\n",
    "                                      'predictRoute': '/predict',\n",
    "                                      'args': ['--gpus all'],\n",
    "                                      'env': [{'name': 'WORKFLOW_URI', 'value': workflow_dir}]\n",
    "                     }\n",
    "        )\n",
    "        .set_display_name(\"Import Query Tower\")\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "    # os.path.join('gs://jt-merlin-scaling', experiment_name, experiment_run, 'model-dir', 'query-tower')\n",
    "    # WORKING_DIR_GCS_URI = f'gs://{OUTPUT_BUCKET}/{experiment_name}/{EXPERIMENT_RUN}'\n",
    "    # MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model-dir\"\n",
    "    # QUERY_TOWER_PATH = f\"{MODEL_DIR}/query-tower\"\n",
    "\n",
    "    \n",
    "    import_candidate_model_task = (\n",
    "        importer_node.importer(\n",
    "            artifact_uri=train_merlin_op.outputs['candidate_tower_gcs_uri'],\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        )\n",
    "        .set_display_name(\"Import Candidate Tower\")\n",
    "        .set_caching_options(False)\n",
    "    )\n",
    "\n",
    "    # ========================================================================\n",
    "    # Upload Models to Vertex AI Model Registry\n",
    "    # ========================================================================\n",
    "    \n",
<<<<<<< HEAD
    "    # query_model_upload_op = (\n",
    "    #     upload_custom_model.upload_custom_model(\n",
    "    #         project=project,\n",
    "    #         location=location,\n",
    "    #         version=version,\n",
    "    #         display_name=f'merlin-query-tower-{version}',\n",
    "    #         artifact_uri=train_merlin_op.outputs[\"query_tower_gcs_dir\"],\n",
    "    #         unmanaged_container_model=import_query_model_task.outputs[\"artifact\"],\n",
    "    #         serving_container_image_uri=build_custom_serving_image_op.outputs[\"custom_image_uri\"],\n",
    "    #     )\n",
    "    #     # .after(xxx)\n",
    "    #     .set_display_name(\"Register Query Tower\")\n",
    "    #     .set_caching_options(False)\n",
    "    # )\n",
    "    ###alternate do this:\n",
    "    \n",
    "    query_model_upload_op = gcc_aip.ModelUploadOp(project=project, \n",
    "                                                  location=location, \n",
    "                                                  unmanaged_container_model=import_query_model_task.outputs[\"artifact\"],\n",
    "                                                  display_name=f'merlin-query-tower-{version}',\n",
    "                                                 ).set_display_name(\"Register Query Tower\").set_caching_options(False)\n",
    "    \n",
=======
    "    query_model_upload_op = (\n",
    "        upload_custom_model.upload_custom_model(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            display_name=f'merlin-query-tower-{version}',\n",
    "            artifact_uri=train_merlin_op.outputs[\"query_tower_gcs_dir\"],\n",
    "            unmanaged_container_model=import_query_model_task.outputs[\"artifact\"],\n",
    "            serving_container_image_uri=build_custom_serving_image_op.outputs[\"custom_image_uri\"],\n",
    "        )\n",
    "        # .after(xxx)\n",
    "        .set_display_name(\"Register Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    from google_cloud_pipeline_components.experimental.evaluation import \\\n",
    "        GetVertexModelOp\n",
    "    \n",
    "    model = (\n",
    "        GetVertexModelOp(\n",
    "            model_resource_name=query_model_upload_op.outputs['model_resource_name'],\n",
    "        )\n",
    "        .set_display_name(\"Get Vertex Model\")\n",
    "    )\n",
    "\n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
    "    # ========================================================================\n",
    "    # Deploy Model to Endpoint\n",
    "    # ========================================================================\n",
    "    endpoint_create_op = (\n",
    "        gcc_aip.EndpointCreateOp(\n",
    "            project=project,\n",
    "            display_name=f'query-tower-endpoint-{version}'\n",
    "        )\n",
    "        .after(query_model_upload_op)\n",
    "        .set_display_name(\"Create Query Endpoint\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    model_deploy_op = (\n",
    "        gcc_aip.ModelDeployOp(\n",
    "            endpoint=endpoint_create_op.outputs['endpoint'],\n",
<<<<<<< HEAD
    "            # endpoint=vertex_ai.Endpoint(endpoint_create_op.outputs['resourceUri']),\n",
    "            model=query_model_upload_op.outputs['model'],\n",
    "            deployed_model_display_name=f'deployed-query-tower-{version}',\n",
=======
    "            model=model.outputs['model'],\n",
    "            deployed_model_display_name=f'deployed-qtower-{version}',\n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
    "            dedicated_resources_machine_type=\"n1-standard-4\",\n",
    "            dedicated_resources_accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "            dedicated_resources_accelerator_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            dedicated_resources_min_replica_count=1,\n",
<<<<<<< HEAD
=======
    "            service_account=service_account,\n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
    "        )\n",
    "        .set_display_name(\"Deploy Query Tower\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Create ME indexes\n",
    "    # ========================================================================\n",
    "    \n",
    "    create_ann_index_op = (\n",
    "        create_ann_index.create_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=train_merlin_op.outputs['candidate_embeddings_gcs_uri'],\n",
    "            dimensions=128,\n",
    "            ann_index_display_name=f'ann_index_pipeline_test_{version}',\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            leaf_node_embedding_count=500,\n",
    "            leaf_nodes_to_search_percent=7, \n",
    "            ann_index_description=\"testing ann index for Merlin deployment\",\n",
    "            # ann_index_labels=ann_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index\")\n",
    "        # .after(XXXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    create_brute_force_index_op = (\n",
    "        create_brute_force_index.create_brute_force_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            emb_index_gcs_uri=train_merlin_op.outputs['candidate_embeddings_gcs_uri'],\n",
    "            dimensions=128,\n",
    "            brute_force_index_display_name=f'bf_index_pipeline_test_{version}',\n",
    "            approximate_neighbors_count=50,\n",
    "            distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
    "            brute_force_index_description=\"testing bf index for Merlin deployment\",\n",
    "            # brute_force_index_labels=brute_force_index_labels,\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index\")\n",
    "        # .after(XXX)\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Create ME index endpoints\n",
    "    # ========================================================================\n",
    "    \n",
    "    create_ann_index_endpoint_vpc_op = (\n",
    "        create_ann_index_endpoint_vpc.create_ann_index_endpoint_vpc(\n",
    "            ann_index_artifact=create_ann_index_op.outputs['ann_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            ann_index_endpoint_display_name=f'ann-index=endpoint-{version}',\n",
    "            ann_index_endpoint_description='endpoint for ann index',\n",
    "        )\n",
    "        .set_display_name(\"Create ANN Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "        \n",
    "    create_brute_index_endpoint_vpc_op = (\n",
    "        create_brute_index_endpoint_vpc.create_brute_index_endpoint_vpc(\n",
    "            bf_index_artifact=create_brute_force_index_op.outputs['brute_force_index'],\n",
    "            project=project,\n",
    "            project_number=project_number,\n",
    "            version=version,\n",
    "            location=location,\n",
    "            vpc_network_name=vpc_network_name,\n",
    "            brute_index_endpoint_display_name=f'bf-index-endpoint-{version}',\n",
    "            brute_index_endpoint_description='endpoint for brute force index',\n",
    "        )\n",
    "        .set_display_name(\"Create BF Index Endpoint\")\n",
    "        # .after(XXX)\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Deploy Indexes\n",
    "    # ========================================================================\n",
    "\n",
    "    deploy_ann_index_op = (\n",
    "        deploy_ann_index.deploy_ann_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            deployed_ann_index_name=f'deployed_ann_index_{version}',\n",
    "            ann_index_resource_uri=create_ann_index_op.outputs['ann_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_ann_index_endpoint_vpc_op.outputs['ann_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy ANN Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n",
    "\n",
    "    deploy_brute_index_op = (\n",
    "        deploy_brute_index.deploy_brute_index(\n",
    "            project=project,\n",
    "            location=location,\n",
    "            version=version,\n",
    "            deployed_brute_force_index_name=f'deployed_bf_index_{version}',\n",
    "            brute_force_index_resource_uri=create_brute_force_index_op.outputs['brute_force_index_resource_uri'],\n",
    "            index_endpoint_resource_uri=create_brute_index_endpoint_vpc_op.outputs['brute_index_endpoint_resource_uri'],\n",
    "        )\n",
    "        .set_display_name(\"Deploy BF Index\")\n",
    "        .set_caching_options(True)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0277936-8f7e-44f7-bd31-33a6d40d5ff1",
   "metadata": {},
   "source": [
    "## Compile Pipeline"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 170,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 19,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "d4ceb963-5fdc-4dc7-bbda-df85057753ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPE_GCS_DIR: gs://jt-merlin-scaling/pipelines_root/jtv11\n"
     ]
    }
   ],
   "source": [
    "BUCKET = 'jt-merlin-scaling'\n",
    "BUCKET_URI = f'gs://{BUCKET}'\n",
    "PIPELINE_ROOT = 'pipelines_root'\n",
    "PIPE_GCS_DIR = f'{BUCKET_URI}/{PIPELINE_ROOT}/{PIPELINE_VERSION}'\n",
    "print(f\"PIPE_GCS_DIR: {PIPE_GCS_DIR}\")\n",
    "\n",
    "SERVING_DOCKERNAME='merlin-retriever'\n",
    "TRAIN_DOCKERNAME='merlintf-22_09_v2'\n",
    "SERVING_SUB_DIR='serving'\n",
    "TRAIN_SUB_DIR='trainer'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 21,
=======
   "execution_count": 171,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 21,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "f05b2a36-97f1-4a14-b0d9-67b15f9aac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "kfp.v2.compiler.Compiler().compile(\n",
    "  pipeline_func=pipeline, \n",
    "  package_path='custom_container_pipeline_spec.json',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 172,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 22,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "d742c97c-07ea-425f-92d3-e47c533c4a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://custom_container_pipeline_spec.json [Content-Type=application/json]...\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "/ [1 files][ 87.7 KiB/ 87.7 KiB]                                                \n",
      "Operation completed over 1 objects/87.7 KiB.                                     \n"
=======
      "/ [1 files][ 86.4 KiB/ 86.4 KiB]                                                \n",
      "Operation completed over 1 objects/86.4 KiB.                                     \n"
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
      "/ [1 files][ 86.5 KiB/ 86.5 KiB]                                                \n",
      "Operation completed over 1 objects/86.5 KiB.                                     \n"
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
     ]
    }
   ],
   "source": [
    "!gsutil cp custom_container_pipeline_spec.json $PIPE_GCS_DIR/pipeline_spec.json"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 23,
=======
   "execution_count": 164,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 23,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "3b1966bf-ec3b-4c14-aae2-8d7a580d9f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./src/Dockerfile.merlin-retriever [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  270.0 B/  270.0 B]                                                \n",
      "Operation completed over 1 objects/270.0 B.                                      \n",
      "Copying file://./src/Dockerfile.merlintf-22_09_v2 [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  353.0 B/  353.0 B]                                                \n",
      "Operation completed over 1 objects/353.0 B.                                      \n",
      "Copying file://./src/serving/requirements.txt [Content-Type=text/plain]...\n",
      "Copying file://./src/serving/instances.json [Content-Type=application/json]...  \n",
      "Copying file://./src/serving/app/__init__.py [Content-Type=text/x-python]...    \n",
      "Copying file://./src/serving/app/main.py [Content-Type=text/x-python]...        \n",
      "Copying file://./src/serving/app/predictor.py [Content-Type=text/x-python]...   \n",
      "Copying file://./src/serving/app/.ipynb_checkpoints/predictor-checkpoint.py [Content-Type=text/x-python]...\n",
      "Copying file://./src/serving/app/prestart.sh [Content-Type=text/x-sh]...        \n",
<<<<<<< HEAD
<<<<<<< HEAD
      "Copying file://./src/serving/app/.ipynb_checkpoints/main-checkpoint.py [Content-Type=text/x-python]...\n",
      "/ [8/8 files][ 18.2 KiB/ 18.2 KiB] 100% Done                                    \n",
      "Operation completed over 8 objects/18.2 KiB.                                     \n",
      "Copying file://./src/trainer/two_tower_model.py [Content-Type=text/x-python]...\n",
=======
      "/ [6/6 files][  8.3 KiB/  8.3 KiB] 100% Done                                    \n",
      "Operation completed over 6 objects/8.3 KiB.                                      \n",
=======
      "Copying file://./src/serving/app/.ipynb_checkpoints/__init__-checkpoint.py [Content-Type=text/x-python]...\n",
      "/ [7/7 files][  8.5 KiB/  8.5 KiB] 100% Done                                    \n",
      "Operation completed over 7 objects/8.5 KiB.                                      \n",
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
      "Copying file://./src/trainer/train_task.py [Content-Type=text/x-python]...\n",
      "Copying file://./src/trainer/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file://./src/trainer/two_tower_model.py [Content-Type=text/x-python]... \n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
      "Copying file://./src/trainer/__init__.py [Content-Type=text/x-python]...        \n",
      "Copying file://./src/trainer/train_task.py [Content-Type=text/x-python]...      \n",
      "Copying file://./src/trainer/interactive_train.py [Content-Type=text/x-python]...\n",
      "Copying file://./src/trainer/.ipynb_checkpoints/train_task-checkpoint.py [Content-Type=text/x-python]...\n",
      "/ [5/5 files][ 39.1 KiB/ 39.1 KiB] 100% Done                                    \n",
      "Operation completed over 5 objects/39.1 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp ./$REPO_DOCKER_PATH_PREFIX/Dockerfile.$SERVING_DOCKERNAME $PIPE_GCS_DIR/\n",
    "!gsutil cp ./$REPO_DOCKER_PATH_PREFIX/Dockerfile.$TRAIN_DOCKERNAME $PIPE_GCS_DIR/\n",
    "\n",
    "!gsutil -m cp -r ./$REPO_DOCKER_PATH_PREFIX/$SERVING_SUB_DIR $PIPE_GCS_DIR/\n",
    "!gsutil -m cp -r ./$REPO_DOCKER_PATH_PREFIX/$TRAIN_SUB_DIR $PIPE_GCS_DIR/"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 173,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 24,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "3a0bcea9-de27-44b6-bc89-23c33f492c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save and load pipeline definition\n",
    "PIPELINES = {}\n",
    "def save_pipelines():\n",
    "    with open(PIPELINES_FILEPATH, 'w') as f:\n",
    "        json.dump(PIPELINES, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17686d4c-cb8a-43ef-801f-60c842f494a7",
   "metadata": {},
   "source": [
    "## Submit Pipeline Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4b575-ed3b-4535-8fc4-4caae701ed22",
   "metadata": {},
   "source": [
    "## pipe args"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 25,
=======
   "execution_count": 177,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 25,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "3aecbc56-9e69-4965-bf0f-854acb5079f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-v7\n"
=======
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-jtv7\n"
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-jtv11\n"
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'pipes'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-{MODEL_NAME}-{FRAMEWORK}-{PIPELINE_VERSION}'\n",
    "EXPERIMENT_RUN = f'run-v1'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 178,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 26,
   "id": "a5a6f45b-67cf-435a-b227-31116a500c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merlin-retriever'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVING_DOCKERNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "8fae3384-9ec2-4f51-ab52-c4b6ee8bf73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DOCKERFILE_NAME: Dockerfile.merlintf-22_09_v2\n",
      "SERVE_DOCKERFILE_NAME: Dockerfile.merlin-retriever\n",
      "\n",
<<<<<<< HEAD
<<<<<<< HEAD
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-v7\n",
=======
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-jtv7\n",
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
      "EXPERIMENT_NAME: pipes-2tower-merlin-tf-jtv11\n",
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
      "EXPERIMENT_RUN: run-v1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VPC_NETWORK_NAME = 'ucaip-haystack-vpc-network'\n",
    "\n",
    "# TRAIN JOB CONFIG\n",
    "TENSORBOARD_RESOURCE = 'projects/934903580331/locations/us-central1/tensorboards/70659015247396864'\n",
    "BATCH_SIZE = 4096*4\n",
    "EPOCHS = 1\n",
    "\n",
    "# IMAGES\n",
    "TRAIN_IMAGE_URI = f'gcr.io/{PROJECT_ID}/merlin-tf-2tower-training-jtv1-22_09_v2'\n",
    "# SERVING_IMAGE_URI = f'gcr.io/{PROJECT_ID}/merlin-triton-serving-v9'\n",
    "SERVING_IMAGE_URI = f\"gcr.io/hybrid-vertex/merlin-vertex-serv-v11\"\n",
    "\n",
    "# DOCKERFILES\n",
    "TRAIN_DOCKERFILE_NAME = f'Dockerfile.{TRAIN_DOCKERNAME}'\n",
    "SERVE_DOCKERFILE_NAME = f'Dockerfile.{SERVING_DOCKERNAME}'\n",
    "\n",
    "# data and schema from nvtabular pipes\n",
    "DATA_DIR = 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed'\n",
    "TRAIN_DIR = f'{DATA_DIR}/train'\n",
    "VALID_DIR = f'{DATA_DIR}/valid'\n",
    "WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed'\n",
    "\n",
    "print(f\"TRAIN_DOCKERFILE_NAME: {TRAIN_DOCKERFILE_NAME}\")\n",
    "print(f\"SERVE_DOCKERFILE_NAME: {SERVE_DOCKERFILE_NAME}\\n\")\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"EXPERIMENT_RUN: {EXPERIMENT_RUN}\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 179,
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
   "execution_count": 29,
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
   "id": "e428e9b8-4a1c-4dcb-b941-a92d03af9ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/google/client/client.py:173: FutureWarning: AIPlatformClient will be deprecated in v2.0.0. Please use PipelineJob https://googleapis.dev/python/aiplatform/latest/_modules/google/cloud/aiplatform/pipeline_jobs.html in Vertex SDK. Install the SDK using \"pip install google-cloud-aiplatform\"\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
<<<<<<< HEAD
<<<<<<< HEAD
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/v7-merlin-train-deploy--v7-20221112015401?project=hybrid-vertex\" target=\"_blank\" >here</a>."
=======
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/jtv7-merlin-train-deploy-jtv7-20221111140827?project=hybrid-vertex\" target=\"_blank\" >here</a>."
>>>>>>> 59f09107be76251cf04a2e190c9b19fa1f0ec078
=======
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/jtv11-merlin-train-deploy-jtv11-20221111185039?project=hybrid-vertex\" target=\"_blank\" >here</a>."
>>>>>>> 114acfc883e7424cf4ff3089a1e8d2764d43f82b
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overwrite = True\n",
    "# overwrite = False\n",
    "\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "pipeline_client = AIPlatformClient(\n",
    "  project_id=PROJECT_ID,\n",
    "  region=LOCATION,\n",
    ")\n",
    "\n",
    "if not PIPELINES.get('train') or overwrite:\n",
    "    response = pipeline_client.create_run_from_job_spec(\n",
    "        job_spec_path='custom_container_pipeline_spec.json',\n",
    "        network=f'projects/{PROJECT_NUM}/global/networks/{VPC_NETWORK_NAME}', # set to same VPC as index\n",
    "        service_account=VERTEX_SA,\n",
    "        parameter_values={\n",
    "            'project': PROJECT_ID,\n",
    "            'project_number': PROJECT_NUM,\n",
    "            'location': LOCATION,\n",
    "            'vpc_network_name': VPC_NETWORK_NAME,\n",
    "            'version': PIPELINE_VERSION,\n",
    "            'pipe_gcs_path': PIPE_GCS_DIR,\n",
    "            'training_image_uri': TRAIN_IMAGE_URI,\n",
    "            'serving_image_uri': SERVING_IMAGE_URI,\n",
    "            'train_docker_name': TRAIN_DOCKERFILE_NAME,\n",
    "            'serving_docker_name': SERVE_DOCKERFILE_NAME,\n",
    "            'tb_resource': TENSORBOARD_RESOURCE,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'train_epochs': EPOCHS,\n",
    "            'train_dir': TRAIN_DIR,\n",
    "            'valid_dir': VALID_DIR,\n",
    "            'workflow_dir': WORKFLOW_DIR,\n",
    "            'experiment_name': EXPERIMENT_NAME,\n",
    "            'experiment_run': EXPERIMENT_RUN,\n",
    "            'service_account': VERTEX_SA,\n",
    "        },\n",
    "        pipeline_root=f'{PIPE_GCS_DIR}',\n",
    "    )\n",
    "    PIPELINES['train'] = response['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8da0d3-7f41-4267-8ed2-4332bbb4dd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-10.m99",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-10:m99"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
