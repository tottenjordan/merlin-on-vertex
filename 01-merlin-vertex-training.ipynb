{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7483ac1f-bb86-4b51-a296-92f545905cd4",
   "metadata": {},
   "source": [
    "# Train Merlin Two-Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d13046-5cb5-43c7-a6ee-b42a3ee9a1e9",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "867b302d-ff4e-4ae4-ba6a-18b717f6d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nvtabular as nvt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "from nvtabular.ops import (\n",
    "    Categorify,\n",
    "    TagAsUserID,\n",
    "    TagAsItemID,\n",
    "    TagAsItemFeatures,\n",
    "    TagAsUserFeatures,\n",
    "    AddMetadata,\n",
    "    ListSlice\n",
    ")\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "from merlin.schema.tags import Tags\n",
    "\n",
    "import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9140a-8529-489e-9885-8431960e129a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "495ea222-2f9e-4dc5-8cfa-23d1470a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Project definitions\n",
    "PROJECT_ID = 'hybrid-vertex' # Change to your project ID.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "\n",
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' # Change to your service account with Vertex AI Admin permitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc441d2e-b156-4d03-9951-ff943b129fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "BUCKET = 'jt-merlin-scaling' # 'spotify-merlin-v1'\n",
    "\n",
    "VERSION = 'jtv1'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "MODEL_DISPLAY_NAME = f'vertex-{FRAMEWORK}-{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions for training\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# # DOCKERNAME = 'hugectr'\n",
    "# DOCKERNAME = 'merlintf'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49f034-6107-426c-9ca5-78bc8e4bed22",
   "metadata": {},
   "source": [
    "# Training Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2dcb695-6e50-4efd-9715-0eb02e1dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAIN_SUB_DIR = 'trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21cfb1e6-fef6-42b7-ba21-b730ade84cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f47e4461-5b2d-4082-bfd3-88072163d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db637aa-c331-4db0-a7da-5fcceed19fd6",
   "metadata": {},
   "source": [
    "## Interactive Train Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a376724-9ed3-4a8b-9b6f-cc139ccc0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141daa01-b5dc-4757-a92c-aad284dea45b",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5d7ad57-89f5-4c68-9fa4-26644c2541d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/two_tower_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/two_tower_model.py\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "import nvtabular as nvt\n",
    "# # import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_two_tower(\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    layer_sizes: List[Any] = [512, 256, 128],\n",
    "):\n",
    "    \n",
    "    #=========================================\n",
    "    # get workflow details\n",
    "    #=========================================\n",
    "    workflow = nvt.Workflow.load(workflow_dir) # gs://spotify-merlin-v1/nvt-preprocessing-spotify-v24/nvt-analyzed\n",
    "    \n",
    "    schema = workflow.output_schema\n",
    "    # embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    user_schema = schema.select_by_tag(Tags.USER)\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    \n",
    "    #=========================================\n",
    "    # build towers\n",
    "    #=========================================\n",
    "    query = mm.Encoder(user_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    item_schema = schema.select_by_tag(Tags.ITEM)\n",
    "    item_inputs = mm.InputBlockV2(\n",
    "        item_schema,\n",
    "    )\n",
    "    candidate = mm.Encoder(item_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    model = mm.RetrievalModelV2(\n",
    "        query=query,\n",
    "        candidate=candidate,\n",
    "        output=mm.ContrastiveOutput(\n",
    "            to_call=DotProduct(),\n",
    "            negative_samplers=\"in-batch\",\n",
    "            schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "            candidate_name=\"item\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac09-65a6-494c-9b42-7b12a22d650f",
   "metadata": {},
   "source": [
    "## Train task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a431f3c9-7e9c-4399-a541-9b6eb514c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "# os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "\n",
    "# # nvtabular\n",
    "# import nvtabular as nvt\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# merlin\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "# nvtabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# gcp\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "# import hypertune\n",
    "# from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "# repo\n",
    "from .two_tower_model import create_two_tower\n",
    "# import utils\n",
    "\n",
    "# local\n",
    "HYPERTUNE_METRIC_NAME = 'AUC'\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions - TODO: move to utils?\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "    # return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "    # return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "\n",
    "def get_upload_logs_to_manged_tb_command(tb_resource_name, logs_dir, experiment_name, ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(\n",
    "        f\"\"\"tb-gcp-uploader --tensorboard_resource_name={tb_resource_name} \\\n",
    "        --logdir={logs_dir} \\\n",
    "        --experiment_name={experiment_name} \\\n",
    "        --one_shot={oneshot} \\\n",
    "        --event_file_inactive_secs={60*60*ttl_hrs}\"\"\"\n",
    "    )\n",
    "\n",
    "def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name, project):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# TRAINING SCRIPT\n",
    "# ====================================================\n",
    "    \n",
    "def main(args):\n",
    "    \"\"\"Runs a training loop.\"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    # os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "    # os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "    \n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    vertex_ai.init(project=f'{args.project}', location=f'{args.location}')\n",
    "    storage_client = storage.Client(project=args.project)\n",
    "    logging.info(\"vertex_ai initialized...\")\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}-{TIMESTAMP}\" # f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    WORKING_DIR_GCS_URI = f'gs://{args.train_output_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "    logging.info(f\"WORKING_DIR_GCS_URI: {WORKING_DIR_GCS_URI}\")\n",
    "    \n",
    "    TB_RESOURCE_NAME = f'{args.tb_name}'\n",
    "    LOGS_DIR = f'{WORKING_DIR_GCS_URI}/tb_logs'\n",
    "    logging.info(f\"tensorboard LOGS_DIR: {LOGS_DIR}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU Strategy\n",
    "    # ====================================================    \n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.test.is_gpu_available(): # TODO: replace with - tf.config.list_physical_devices('GPU')\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "        \n",
    "    \n",
    "    # set related vars...\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = NUM_WORKERS * args.per_gpu_batch_size\n",
    "    # num_gpus = sum([len(gpus) for gpus in args.gpus])\n",
    "    # GLOBAL_BATCH_SIZE = num_gpus * args.per_gpu_batch_size\n",
    "\n",
    "    logging.info(f'NUM_WORKERS = {NUM_WORKERS}')\n",
    "    # logging.info(f'num_gpus: {num_gpus}')\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE: {GLOBAL_BATCH_SIZE}')\n",
    "    \n",
    "    # set worker vars...\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "        \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'Loading workflow & schema from : {args.workflow_dir}')\n",
    "    \n",
    "    workflow = nvt.Workflow.load(args.workflow_dir) # gs://{BUCKET}/..../nvt-analyzed\n",
    "    schema = workflow.output_schema\n",
    "    embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    train_data = MerlinDataset(os.path.join(args.train_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    valid_data = MerlinDataset(os.path.join(args.valid_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    \n",
    "    # train_data = MerlinDataset(args.train_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    # valid_data = MerlinDataset(args.valid_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Callbacks\n",
    "    # ====================================================\n",
    "    class UploadTBLogsBatchEnd(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            os.system(\n",
    "                get_upload_logs_to_manged_tb_command(\n",
    "                    tb_resource_name=TB_RESOURCE_NAME, \n",
    "                    logs_dir=LOGS_DIR, \n",
    "                    experiment_name=EXPERIMENT_NAME,\n",
    "                    ttl_hrs = 5, \n",
    "                    oneshot=\"true\",\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOGS_DIR,\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        # profile_batch=(20,50)\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Train\n",
    "    # ==================================================== \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "\n",
    "    # with strategy.scope():\n",
    "    model = create_two_tower(\n",
    "        train_dir=args.train_dir,\n",
    "        valid_dir=args.valid_dir,\n",
    "        workflow_dir=args.workflow_dir,\n",
    "        layer_sizes=LAYER_SIZES # args.layer_sizes,\n",
    "    )\n",
    "        \n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "        run_eagerly=False,\n",
    "        metrics=[mm.RecallAt(1), mm.RecallAt(10), mm.NDCGAt(10)],\n",
    "    )\n",
    "    \n",
    "    # cloud_profiler.init() # managed TB profiler\n",
    "        \n",
    "    logging.info('Starting training loop...')\n",
    "    \n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=valid_data, \n",
    "        batch_size=GLOBAL_BATCH_SIZE, \n",
    "        epochs=args.num_epochs,\n",
    "        # steps_per_epoch=20, \n",
    "        callbacks=[\n",
    "            tensorboard_callback, \n",
    "            UploadTBLogsBatchEnd()\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    elapsed_model_fit = end_model_fit - start_model_fit\n",
    "    elapsed_model_fit = round(elapsed_model_fit, 2)\n",
    "    logging.info(f'Elapsed model_fit: {elapsed_model_fit} seconds')\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams & metrics for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging params & metrics for Vertex Experiments')\n",
    "    \n",
    "    # get the metrics for the experiment run\n",
    "    history_keys = model.history.history.keys()\n",
    "    metrics_dict = {}\n",
    "    _ = [metrics_dict.update({key: model.history.history[key][-1]}) for key in history_keys]\n",
    "    metrics_dict[\"elapsed_model_fit\"] = elapsed_model_fit\n",
    "    \n",
    "    logging.info(f'metrics_dict: {metrics_dict}')\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    \n",
    "    logging.info(f'metaparams: {metaparams}')\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"num_gpus\"] = NUM_WORKERS # num_gpus\n",
    "    hyperparams[\"per_gpu_batch_size\"] = args.per_gpu_batch_size\n",
    "    hyperparams[\"global_batch_size\"] = GLOBAL_BATCH_SIZE\n",
    "    hyperparams[\"learning_rate\"] = args.learning_rate\n",
    "    hyperparams['layers'] = f'{args.layer_sizes}'\n",
    "    \n",
    "    logging.info(f'hyperparams: {hyperparams}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Experiments\n",
    "    # ====================================================\n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "    \n",
    "        # Create experiment\n",
    "        vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "        with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "            logging.info(f\"logging metrics_dict\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams\")\n",
    "            my_run.log_params(metaparams)\n",
    "\n",
    "            logging.info(f\"logging hyperparams\")\n",
    "            my_run.log_params(hyperparams)\n",
    "        \n",
    "    # =============================================\n",
    "    # save retrieval (query) tower\n",
    "    # =============================================\n",
    "    # set vars...\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model-dir\"\n",
    "    logging.info(f'Saving towers to {MODEL_DIR}')\n",
    "    \n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query-tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate-tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate-embeddings\"\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        # save query tower\n",
    "        query_tower = model.query_encoder\n",
    "        query_tower.save(QUERY_TOWER_PATH)\n",
    "        logging.info(f'Saved query tower to {QUERY_TOWER_PATH}')\n",
    "        \n",
    "        candidate_tower = model.candidate_encoder\n",
    "        candidate_tower.save(CANDIDATE_TOWER_PATH)\n",
    "        logging.info(f'Saved candidate tower to {CANDIDATE_TOWER_PATH}')\n",
    "    \n",
    "    # =============================================\n",
    "    # save embeddings for ME index\n",
    "    # =============================================\n",
    "    EMBEDDINGS_FILE_NAME = \"candidate_embeddings.json\"\n",
    "    logging.info(f\"Saving {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    def format_for_matching_engine(data) -> None:\n",
    "        emb = [data[i] for i in range(LAYER_SIZES[-1])] # get the embeddings\n",
    "        formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "        with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "            f.write(formatted_emb)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # !rm candidate_embeddings.json > /dev/null \n",
    "    # !touch candidate_embeddings.json\n",
    "    item_data = pd.read_parquet(f'{args.workflow_dir}/categories/unique.track_uri_can.parquet')\n",
    "    lookup_dict = dict(item_data['track_uri_can'])\n",
    "\n",
    "    # item embeds from TRAIN\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features = (unique_rows_by_features(train_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs = model.candidate_embeddings(item_features, index=item_features.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd = item_embs.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd['track_uri_can'] = item_emb_pd['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting TRAIN embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    # item embeds from VALID\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features_val = (unique_rows_by_features(valid_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs_val = model.candidate_embeddings(item_features_val, index=item_features_val.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd_val = item_embs_val.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd_val['track_uri_can'] = item_emb_pd_val['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd_val.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting VALID embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        _upload_blob_gcs(\n",
    "            EMBEDDINGS_PATH, \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\", \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\",\n",
    "            args.project\n",
    "        )\n",
    "    \n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "# ====================================================\n",
    "# arg parser\n",
    "# ====================================================\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--experiment_name',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed-experiment',\n",
    "        help='name of vertex ai experiment'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--experiment_run',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed_run',\n",
    "        help='name of vertex ai experiment run'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--tb_name',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='projects/XXXXXX/locations/us-central1/tensorboards/XXXXXXXX'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--distribute',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='single',\n",
    "        help='training strategy'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_output_bucket',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        # default='single',\n",
    "        help='gcs bucket name'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to saved workflow.pkl e.g., nvt-analyzed'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to training data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--valid_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to validation data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='num_epochs'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--per_gpu_batch_size',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='Per GPU Batch size'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--layer_sizes',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='[512, 256, 128]',\n",
    "        help='layer_sizes'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=.001,\n",
    "        help='learning_rate'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp project'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--location',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp location'\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     '--gpus',\n",
    "    #     type=str,\n",
    "    #     required=False,\n",
    "    #     default='[[0]]',\n",
    "    #     help='GPU devices to use for Preprocessing'\n",
    "    # )\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    # parsed_args.gpus = json.loads(parsed_args.gpus)\n",
    "\n",
    "    # parsed_args.slot_size_array = [\n",
    "    #     int(i) for i in parsed_args.slot_size_array.split(sep=' ')\n",
    "    # ]\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting training')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158e674-6b0b-47ac-ac12-cce0b002bf93",
   "metadata": {},
   "source": [
    "## Training Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa81f3-0912-4f00-ba09-9bb9c40991eb",
   "metadata": {},
   "source": [
    "### versioned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ad9482d-5713-43bc-81db-d269ac4283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker definitions for training\n",
    "MERLIN_VERSION = '22_09_v2'\n",
    "IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}-{MERLIN_VERSION}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = f'merlintf-{MERLIN_VERSION}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212d9ee-4b1b-4092-8094-b6f0b88c42c6",
   "metadata": {},
   "source": [
    "* nvtabular==1.5.0\n",
    "* nvtabular==1.3.3\n",
    "* cloudml-hypertune\n",
    "\n",
    "```\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13146051-f765-4bdf-9e87-d2eb8f05e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5 gcsfs google-cloud-aiplatform[cloud_profiler] kfp\n",
    "# RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "# COPY trainer/* ./\n",
    "\n",
    "# ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0848b1d2-b50a-4040-b0ce-f595750fb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.merlintf-22_09_v2\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# RUN pip install -U pip\n",
    "RUN pip install git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5 gcsfs google-cloud-aiplatform fastapi\n",
    "\n",
    "\n",
    "COPY trainer /trainer\n",
    "# COPY trainer/* ./\n",
    "\n",
    "# RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "# CMD python trainer/task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d7f449cb-78f2-4e8b-a056-eb139469ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/requirements.txt\n",
    "# fastapi\n",
    "# git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5\n",
    "# gsutil\n",
    "# gcsfs\n",
    "# matplotlib\n",
    "# google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e455c73-6ace-40fe-a9a1-308830a80029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/home/jupyter/spotify-merlin/src/trainer\u001b[00m\n",
      "├── __init__.py\n",
      "├── interactive_train.py\n",
      "├── train_task.py\n",
      "└── two_tower_model.py\n",
      "\n",
      "0 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "!tree /home/jupyter/spotify-merlin/{REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740819d7-2a79-46a2-9c79-a1f718da76ba",
   "metadata": {},
   "source": [
    "### nightly image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8476061-b8b1-4fbe-8e56-76d94cf34bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Docker definitions for training\n",
    "# MERLIN_VERSION = 'nightly'\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}-{MERLIN_VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "# DOCKERNAME = f'merlintf-{MERLIN_VERSION}'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10b1d9c9-e31e-4de4-9ea5-f51aeec5c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# # FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:nightly\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git\n",
    "# RUN pip install google-cloud-bigquery gcsfs cloudml-hypertune\n",
    "# RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "# RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "# COPY trainer/* ./\n",
    "\n",
    "# ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f39a5146-1880-44ba-ab66-95045c05cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree /home/jupyter/spotify-merlin/{REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "513735ef-7713-4a59-b5bc-5e82f298ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_URI='gcr.io/hybrid-vertex/merlin-tf-twotower-training-jtv1-nightly'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5134-d7aa-4f6c-882c-5f11d35cbfc2",
   "metadata": {},
   "source": [
    "# Build Train Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42e901-e60c-47ab-ba34-c5add53845c0",
   "metadata": {},
   "source": [
    "### test locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75e03e5f-c0b3-4654-b40f-b33d5aa0f1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e7fb19c0-4201-48e9-b0c8-470424be95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# _LOCATION='us-central1'\n",
    "# _TB_NAME='projects/934903580331/locations/us-central1/tensorboards/5925030667573264384'\n",
    "# _DATA_DIR='gs://spotify-beam-v3/merlin-processed'\n",
    "# _TRAIN_DATA=f'{_DATA_DIR}/valid/'\n",
    "# _VALID_DATA=f'{_DATA_DIR}/valid/'\n",
    "# _WORKFLOW_DIR=f'{_DATA_DIR}/workflow/2t-spotify-workflow'\n",
    "# _OUTPUT_BUCKET='jt-merlin-scaling'\n",
    "# _EXPERIMENT_NAME='local-experiment'\n",
    "# _EXPERIMENT_RUN=f'run-v1-{TIMESTAMP}'\n",
    "# _DISTRIBUTE='single'\n",
    "# _PER_GPU_BATCH_SIZE=4096\n",
    "# _LAYER_SIZES='[256, 128]'\n",
    "# _LEARNING_RATE=0.001\n",
    "# _NUM_EPOCHS=1\n",
    "\n",
    "# # !cd src/trainer; python3 -m trainer.train_task \\\n",
    "# !cd src/trainer; python3 train_task.py \\\n",
    "#     --project=PROJECT_ID --location=$_LOCATION \\\n",
    "#     --train_output_bucket=$_OUTPUT_BUCKET --tb_name=$_TB_NAME \\\n",
    "#     --workflow_dir=$_WORKFLOW_DIR --train_dir=$_TRAIN_DATA --valid_dir=$_VALID_DATA \\\n",
    "#     --experiment_name=$_EXPERIMENT_NAME --experiment_run=$_EXPERIMENT_RUN \\\n",
    "#     --distribute=$_DISTRIBUTE \\\n",
    "#     --num_epochs=$_NUM_EPOCHS \\\n",
    "#     --per_gpu_batch_size=$_PER_GPU_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54394f2-7c65-40ea-9102-ccf0cd3178a5",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "817dc303-b3e6-4427-bfd0-5cd949183a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68e469cf-13ae-47e6-93bb-30aef60ebe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/spotify-merlin'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/home/jupyter/spotify-merlin')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e640822-e85b-4ba2-a192-9c8d790ff2cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 87 file(s) totalling 1.9 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1667920095.497321-5d11b67f530e48d080b9d6f937632f29.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/db4bbe66-4a28-4476-9d69-e8d7ab9e52d7].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/db4bbe66-4a28-4476-9d69-e8d7ab9e52d7?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"db4bbe66-4a28-4476-9d69-e8d7ab9e52d7\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1667920095.497321-5d11b67f530e48d080b9d6f937632f29.tgz#1667920096132128\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1667920095.497321-5d11b67f530e48d080b9d6f937632f29.tgz#1667920096132128...\n",
      "/ [1 files][322.1 KiB/322.1 KiB]                                                \n",
      "Operation completed over 1 objects/322.1 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  515.6kB\n",
      "Step 1/4 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      "22.09: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "3b65ec22a9e9: Pulling fs layer\n",
      "fd80d866e8b2: Pulling fs layer\n",
      "a364ca75fd6d: Pulling fs layer\n",
      "3d4731d03623: Pulling fs layer\n",
      "53a5c2e0251f: Pulling fs layer\n",
      "b00ff40d02d9: Pulling fs layer\n",
      "3036e9b94123: Pulling fs layer\n",
      "453fdcdda788: Pulling fs layer\n",
      "35e12ec5e515: Pulling fs layer\n",
      "11f61a475a23: Pulling fs layer\n",
      "24280cf31c9a: Pulling fs layer\n",
      "79007799e2ed: Pulling fs layer\n",
      "03eb76abf1e5: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "5e9434e8ae41: Pulling fs layer\n",
      "3d4731d03623: Waiting\n",
      "88a3e778b5bf: Pulling fs layer\n",
      "729af2b35d14: Pulling fs layer\n",
      "30e0a3a7e9e5: Pulling fs layer\n",
      "3036e9b94123: Waiting\n",
      "0852b4bd65a1: Pulling fs layer\n",
      "81cb421c2c25: Pulling fs layer\n",
      "53a5c2e0251f: Waiting\n",
      "b00ff40d02d9: Waiting\n",
      "3d6b664afa23: Pulling fs layer\n",
      "3e8f37aba8a2: Pulling fs layer\n",
      "91bba9bd0f1f: Pulling fs layer\n",
      "a422be4dcb08: Pulling fs layer\n",
      "453fdcdda788: Waiting\n",
      "ca10bb6dc143: Pulling fs layer\n",
      "35e12ec5e515: Waiting\n",
      "e48bbfc7d00e: Pulling fs layer\n",
      "860a23551ac0: Pulling fs layer\n",
      "11f61a475a23: Waiting\n",
      "cc78be876588: Pulling fs layer\n",
      "ad6568ad37e5: Pulling fs layer\n",
      "24280cf31c9a: Waiting\n",
      "258a31babfce: Pulling fs layer\n",
      "116ca0069c88: Pulling fs layer\n",
      "79007799e2ed: Waiting\n",
      "ede4a5022f61: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "154d6414dd17: Pulling fs layer\n",
      "e1f68d1c5137: Pulling fs layer\n",
      "03eb76abf1e5: Waiting\n",
      "5e9434e8ae41: Waiting\n",
      "0d4b5cd36c43: Pulling fs layer\n",
      "fc9b6547dc7c: Pulling fs layer\n",
      "de51b5b1b318: Pulling fs layer\n",
      "d684c579871f: Pulling fs layer\n",
      "4a39f6623824: Pulling fs layer\n",
      "729af2b35d14: Waiting\n",
      "30bba30584e3: Pulling fs layer\n",
      "c104fa3a7626: Pulling fs layer\n",
      "0852b4bd65a1: Waiting\n",
      "30e0a3a7e9e5: Waiting\n",
      "22f09e497c63: Pulling fs layer\n",
      "5f66cd739f31: Pulling fs layer\n",
      "c9c9a9e1cad6: Pulling fs layer\n",
      "630fcaa7a158: Pulling fs layer\n",
      "1b8090778700: Pulling fs layer\n",
      "860a23551ac0: Waiting\n",
      "4ca3cacea924: Pulling fs layer\n",
      "4aa043daa566: Pulling fs layer\n",
      "cc78be876588: Waiting\n",
      "c1d86dc35ba1: Pulling fs layer\n",
      "ad6568ad37e5: Waiting\n",
      "3d6b664afa23: Waiting\n",
      "55f4be7d7adc: Pulling fs layer\n",
      "f8e1ddeececb: Pulling fs layer\n",
      "258a31babfce: Waiting\n",
      "bf1f5a5d15b8: Pulling fs layer\n",
      "68d63385def6: Pulling fs layer\n",
      "865f53eecc40: Pulling fs layer\n",
      "70ac11bbf381: Pulling fs layer\n",
      "91a761249212: Pulling fs layer\n",
      "2cb953eac694: Pulling fs layer\n",
      "2e6386dccac0: Pulling fs layer\n",
      "67793606273a: Pulling fs layer\n",
      "bfdaa5a6754f: Pulling fs layer\n",
      "3e0d0b3b5c1c: Pulling fs layer\n",
      "8d6d2c98840c: Pulling fs layer\n",
      "2559630e37d2: Pulling fs layer\n",
      "d0f13d58f7fa: Pulling fs layer\n",
      "929ce3ad5dc9: Pulling fs layer\n",
      "7a42c1978b2e: Pulling fs layer\n",
      "bf1f5a5d15b8: Waiting\n",
      "3e8f37aba8a2: Waiting\n",
      "68d63385def6: Waiting\n",
      "ca10bb6dc143: Waiting\n",
      "865f53eecc40: Waiting\n",
      "e48bbfc7d00e: Waiting\n",
      "91bba9bd0f1f: Waiting\n",
      "c1d86dc35ba1: Waiting\n",
      "c104fa3a7626: Waiting\n",
      "630fcaa7a158: Waiting\n",
      "1b8090778700: Waiting\n",
      "fc9b6547dc7c: Waiting\n",
      "929ce3ad5dc9: Waiting\n",
      "de51b5b1b318: Waiting\n",
      "7a42c1978b2e: Waiting\n",
      "22f09e497c63: Waiting\n",
      "d684c579871f: Waiting\n",
      "4a39f6623824: Waiting\n",
      "30bba30584e3: Waiting\n",
      "2cb953eac694: Waiting\n",
      "ede4a5022f61: Waiting\n",
      "e1f68d1c5137: Waiting\n",
      "0d4b5cd36c43: Waiting\n",
      "2559630e37d2: Waiting\n",
      "d0f13d58f7fa: Waiting\n",
      "67793606273a: Waiting\n",
      "154d6414dd17: Waiting\n",
      "116ca0069c88: Waiting\n",
      "f8e1ddeececb: Waiting\n",
      "a422be4dcb08: Waiting\n",
      "3e0d0b3b5c1c: Waiting\n",
      "8d6d2c98840c: Waiting\n",
      "4aa043daa566: Waiting\n",
      "bfdaa5a6754f: Waiting\n",
      "2e6386dccac0: Waiting\n",
      "3b65ec22a9e9: Verifying Checksum\n",
      "3b65ec22a9e9: Download complete\n",
      "3d4731d03623: Verifying Checksum\n",
      "3d4731d03623: Download complete\n",
      "fd80d866e8b2: Download complete\n",
      "a364ca75fd6d: Verifying Checksum\n",
      "a364ca75fd6d: Download complete\n",
      "b00ff40d02d9: Download complete\n",
      "3036e9b94123: Verifying Checksum\n",
      "3036e9b94123: Download complete\n",
      "453fdcdda788: Verifying Checksum\n",
      "453fdcdda788: Download complete\n",
      "35e12ec5e515: Download complete\n",
      "3b65ec22a9e9: Pull complete\n",
      "11f61a475a23: Verifying Checksum\n",
      "11f61a475a23: Download complete\n",
      "79007799e2ed: Verifying Checksum\n",
      "79007799e2ed: Download complete\n",
      "24280cf31c9a: Verifying Checksum\n",
      "24280cf31c9a: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "5e9434e8ae41: Verifying Checksum\n",
      "5e9434e8ae41: Download complete\n",
      "88a3e778b5bf: Download complete\n",
      "729af2b35d14: Download complete\n",
      "03eb76abf1e5: Verifying Checksum\n",
      "03eb76abf1e5: Download complete\n",
      "0852b4bd65a1: Verifying Checksum\n",
      "0852b4bd65a1: Download complete\n",
      "fd80d866e8b2: Pull complete\n",
      "81cb421c2c25: Verifying Checksum\n",
      "81cb421c2c25: Download complete\n",
      "3d6b664afa23: Verifying Checksum\n",
      "3d6b664afa23: Download complete\n",
      "30e0a3a7e9e5: Verifying Checksum\n",
      "30e0a3a7e9e5: Download complete\n",
      "a364ca75fd6d: Pull complete\n",
      "3d4731d03623: Pull complete\n",
      "91bba9bd0f1f: Verifying Checksum\n",
      "91bba9bd0f1f: Download complete\n",
      "a422be4dcb08: Download complete\n",
      "ca10bb6dc143: Verifying Checksum\n",
      "ca10bb6dc143: Download complete\n",
      "e48bbfc7d00e: Verifying Checksum\n",
      "860a23551ac0: Verifying Checksum\n",
      "860a23551ac0: Download complete\n",
      "cc78be876588: Verifying Checksum\n",
      "cc78be876588: Download complete\n",
      "ad6568ad37e5: Verifying Checksum\n",
      "ad6568ad37e5: Download complete\n",
      "258a31babfce: Verifying Checksum\n",
      "258a31babfce: Download complete\n",
      "116ca0069c88: Download complete\n",
      "ede4a5022f61: Verifying Checksum\n",
      "ede4a5022f61: Download complete\n",
      "154d6414dd17: Verifying Checksum\n",
      "154d6414dd17: Download complete\n",
      "e1f68d1c5137: Verifying Checksum\n",
      "e1f68d1c5137: Download complete\n",
      "0d4b5cd36c43: Verifying Checksum\n",
      "0d4b5cd36c43: Download complete\n",
      "fc9b6547dc7c: Verifying Checksum\n",
      "fc9b6547dc7c: Download complete\n",
      "53a5c2e0251f: Verifying Checksum\n",
      "53a5c2e0251f: Download complete\n",
      "3e8f37aba8a2: Verifying Checksum\n",
      "3e8f37aba8a2: Download complete\n",
      "4a39f6623824: Verifying Checksum\n",
      "4a39f6623824: Download complete\n",
      "30bba30584e3: Verifying Checksum\n",
      "30bba30584e3: Download complete\n",
      "c104fa3a7626: Verifying Checksum\n",
      "c104fa3a7626: Download complete\n",
      "22f09e497c63: Download complete\n",
      "d684c579871f: Verifying Checksum\n",
      "d684c579871f: Download complete\n",
      "5f66cd739f31: Verifying Checksum\n",
      "5f66cd739f31: Download complete\n",
      "c9c9a9e1cad6: Verifying Checksum\n",
      "c9c9a9e1cad6: Download complete\n",
      "630fcaa7a158: Verifying Checksum\n",
      "630fcaa7a158: Download complete\n",
      "1b8090778700: Verifying Checksum\n",
      "1b8090778700: Download complete\n",
      "4ca3cacea924: Verifying Checksum\n",
      "4ca3cacea924: Download complete\n",
      "c1d86dc35ba1: Verifying Checksum\n",
      "c1d86dc35ba1: Download complete\n",
      "4aa043daa566: Verifying Checksum\n",
      "4aa043daa566: Download complete\n",
      "de51b5b1b318: Verifying Checksum\n",
      "de51b5b1b318: Download complete\n",
      "55f4be7d7adc: Verifying Checksum\n",
      "55f4be7d7adc: Download complete\n",
      "bf1f5a5d15b8: Verifying Checksum\n",
      "bf1f5a5d15b8: Download complete\n",
      "f8e1ddeececb: Verifying Checksum\n",
      "f8e1ddeececb: Download complete\n",
      "68d63385def6: Verifying Checksum\n",
      "68d63385def6: Download complete\n",
      "865f53eecc40: Verifying Checksum\n",
      "865f53eecc40: Download complete\n",
      "70ac11bbf381: Verifying Checksum\n",
      "70ac11bbf381: Download complete\n",
      "2e6386dccac0: Verifying Checksum\n",
      "2e6386dccac0: Download complete\n",
      "67793606273a: Download complete\n",
      "bfdaa5a6754f: Verifying Checksum\n",
      "bfdaa5a6754f: Download complete\n",
      "3e0d0b3b5c1c: Verifying Checksum\n",
      "3e0d0b3b5c1c: Download complete\n",
      "8d6d2c98840c: Verifying Checksum\n",
      "8d6d2c98840c: Download complete\n",
      "2559630e37d2: Download complete\n",
      "d0f13d58f7fa: Verifying Checksum\n",
      "d0f13d58f7fa: Download complete\n",
      "91a761249212: Verifying Checksum\n",
      "91a761249212: Download complete\n",
      "7a42c1978b2e: Verifying Checksum\n",
      "7a42c1978b2e: Download complete\n",
      "2cb953eac694: Verifying Checksum\n",
      "2cb953eac694: Download complete\n",
      "929ce3ad5dc9: Verifying Checksum\n",
      "929ce3ad5dc9: Download complete\n",
      "53a5c2e0251f: Pull complete\n",
      "b00ff40d02d9: Pull complete\n",
      "3036e9b94123: Pull complete\n",
      "453fdcdda788: Pull complete\n",
      "35e12ec5e515: Pull complete\n",
      "11f61a475a23: Pull complete\n",
      "24280cf31c9a: Pull complete\n",
      "79007799e2ed: Pull complete\n",
      "03eb76abf1e5: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "5e9434e8ae41: Pull complete\n",
      "88a3e778b5bf: Pull complete\n",
      "729af2b35d14: Pull complete\n",
      "30e0a3a7e9e5: Pull complete\n",
      "0852b4bd65a1: Pull complete\n",
      "81cb421c2c25: Pull complete\n",
      "3d6b664afa23: Pull complete\n",
      "3e8f37aba8a2: Pull complete\n",
      "91bba9bd0f1f: Pull complete\n",
      "a422be4dcb08: Pull complete\n",
      "ca10bb6dc143: Pull complete\n",
      "e48bbfc7d00e: Pull complete\n",
      "860a23551ac0: Pull complete\n",
      "cc78be876588: Pull complete\n",
      "ad6568ad37e5: Pull complete\n",
      "258a31babfce: Pull complete\n",
      "116ca0069c88: Pull complete\n",
      "ede4a5022f61: Pull complete\n",
      "154d6414dd17: Pull complete\n",
      "e1f68d1c5137: Pull complete\n",
      "0d4b5cd36c43: Pull complete\n",
      "fc9b6547dc7c: Pull complete\n",
      "de51b5b1b318: Pull complete\n",
      "d684c579871f: Pull complete\n",
      "4a39f6623824: Pull complete\n",
      "30bba30584e3: Pull complete\n",
      "c104fa3a7626: Pull complete\n",
      "22f09e497c63: Pull complete\n",
      "5f66cd739f31: Pull complete\n",
      "c9c9a9e1cad6: Pull complete\n",
      "630fcaa7a158: Pull complete\n",
      "1b8090778700: Pull complete\n",
      "4ca3cacea924: Pull complete\n",
      "4aa043daa566: Pull complete\n",
      "c1d86dc35ba1: Pull complete\n",
      "55f4be7d7adc: Pull complete\n",
      "f8e1ddeececb: Pull complete\n",
      "bf1f5a5d15b8: Pull complete\n",
      "68d63385def6: Pull complete\n",
      "865f53eecc40: Pull complete\n",
      "70ac11bbf381: Pull complete\n",
      "91a761249212: Pull complete\n",
      "2cb953eac694: Pull complete\n",
      "2e6386dccac0: Pull complete\n",
      "67793606273a: Pull complete\n",
      "bfdaa5a6754f: Pull complete\n",
      "3e0d0b3b5c1c: Pull complete\n",
      "8d6d2c98840c: Pull complete\n",
      "2559630e37d2: Pull complete\n",
      "d0f13d58f7fa: Pull complete\n",
      "929ce3ad5dc9: Pull complete\n",
      "7a42c1978b2e: Pull complete\n",
      "Digest: sha256:2475b7062a16cd7ba0e5eda0ff58f206400714aafd061d4d8a1a1e8aacd59668\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
      " ---> ec90adb8185e\n",
      "Step 2/4 : WORKDIR /\n",
      " ---> Running in 1aa8e7f248bf\n",
      "Removing intermediate container 1aa8e7f248bf\n",
      " ---> 6cbb834f73ad\n",
      "Step 3/4 : RUN pip install git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5 gcsfs google-cloud-aiplatform fastapi\n",
      " ---> Running in 3fb7629a24a9\n",
      "Collecting git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5\n",
      "  Cloning https://github.com/NVIDIA-Merlin/models.git (to revision efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5) to /tmp/pip-req-build-vi5rrw4v\n",
      "\u001b[91m  Running command git clone -q https://github.com/NVIDIA-Merlin/models.git /tmp/pip-req-build-vi5rrw4v\n",
      "\u001b[0m\u001b[91m  Running command git checkout -q efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5\n",
      "\u001b[0m  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2022.10.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.18.3-py2.py3-none-any.whl (2.3 MB)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.86.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-models==0.9.0+2.gefe4bc91c) (0.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.3)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from gcsfs) (2.22.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.6.0-py2.py3-none-any.whl (105 kB)\n",
      "Collecting fsspec==2022.10.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.12.0)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (3.19.5)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "Collecting google-cloud-bigquery<3.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-2.34.4-py2.py3-none-any.whl (206 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "Collecting starlette==0.20.4\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.10.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (4.64.1)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2022.5.1)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2022.5.1)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.3.5)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.2.5)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.10.0)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (0.56.2)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (7.0.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.1.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.1)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.0-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from google-auth>=1.2->gcsfs) (1.14.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.41.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.56.4)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl (14 kB)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.20.4->fastapi) (3.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from starlette==0.20.4->fastapi) (4.3.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.3.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (6.0)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2.2.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (6.2)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.7.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.0.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (8.1.3)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (3.1.2)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (5.9.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.26.12)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.22.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2022.2.1)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (0.4.3)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.2.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.2.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (4.12.0)\n",
      "Requirement already satisfied: setuptools<60 in /usr/lib/python3/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (45.2.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (0.39.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs) (2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.2.1)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi) (1.3.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (2.1.1)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (4.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (3.8.1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models==0.9.0+2.gefe4bc91c) (4.0.0)\n",
      "Building wheels for collected packages: merlin-models\n",
      "  Building wheel for merlin-models (PEP 517): started\n",
      "  Building wheel for merlin-models (PEP 517): finished with status 'done'\n",
      "  Created wheel for merlin-models: filename=merlin_models-0.9.0+2.gefe4bc91c-py3-none-any.whl size=348889 sha256=18215f55ddf8b57de2dd0e8ffd158aeb63f2a67846124d7f3fab7afce996cc79\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/17/d5/d96715750ada098103e83d44bd102282eaceef5c9ed7a4a418\n",
      "Successfully built merlin-models\n",
      "\u001b[91mERROR: merlin-core 0.7.0 has requirement fsspec==2022.5.0, but you'll have fsspec 2022.10.0 which is incompatible.\n",
      "ERROR: grpcio-status 1.50.0 has requirement grpcio>=1.50.0, but you'll have grpcio 1.41.0 which is incompatible.\n",
      "ERROR: grpcio-status 1.50.0 has requirement protobuf>=4.21.6, but you'll have protobuf 3.19.5 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: google-crc32c, google-resumable-media, grpcio-status, google-api-core, google-cloud-core, google-cloud-storage, fsspec, gcsfs, proto-plus, google-cloud-bigquery, grpc-google-iam-v1, google-cloud-resource-manager, google-cloud-aiplatform, starlette, pydantic, fastapi, merlin-models\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "  Attempting uninstall: merlin-models\n",
      "    Found existing installation: merlin-models 0.8.0\n",
      "    Uninstalling merlin-models-0.8.0:\n",
      "      Successfully uninstalled merlin-models-0.8.0\n",
      "Successfully installed fastapi-0.86.0 fsspec-2022.10.0 gcsfs-2022.10.0 google-api-core-2.10.2 google-cloud-aiplatform-1.18.3 google-cloud-bigquery-2.34.4 google-cloud-core-2.3.2 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.6.0 google-crc32c-1.5.0 google-resumable-media-2.4.0 grpc-google-iam-v1-0.12.4 grpcio-status-1.50.0 merlin-models-0.9.0+2.gefe4bc91c proto-plus-1.22.1 pydantic-1.10.2 starlette-0.20.4\n",
      "Removing intermediate container 3fb7629a24a9\n",
      " ---> b0998f294813\n",
      "Step 4/4 : COPY trainer /trainer\n",
      " ---> 68d732e05a76\n",
      "Successfully built 68d732e05a76\n",
      "Successfully tagged gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2\n",
      "The push refers to repository [gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2]\n",
      "b964b48b14cf: Preparing\n",
      "3cef30a8019f: Preparing\n",
      "bd113aa55fbd: Preparing\n",
      "e58f9c500afd: Preparing\n",
      "c3c9ca29c9f2: Preparing\n",
      "e844c8057c95: Preparing\n",
      "0b07555d3a5b: Preparing\n",
      "a1537cf26842: Preparing\n",
      "00679b7c9426: Preparing\n",
      "add0b850bdf8: Preparing\n",
      "c2e412b87e2d: Preparing\n",
      "7730d4c011cd: Preparing\n",
      "f83c32bae622: Preparing\n",
      "86147dce553c: Preparing\n",
      "a86afd489635: Preparing\n",
      "725647e88671: Preparing\n",
      "78e008bc66d2: Preparing\n",
      "e7d002ddb49b: Preparing\n",
      "f2b540bc31be: Preparing\n",
      "442b3d22fc2d: Preparing\n",
      "72f4d03b40d8: Preparing\n",
      "54244453f24a: Preparing\n",
      "c9dfb1d8d420: Preparing\n",
      "c1af80eb8994: Preparing\n",
      "5c0e49e0fefd: Preparing\n",
      "e844c8057c95: Waiting\n",
      "64579a0c8694: Preparing\n",
      "82432f6543d2: Preparing\n",
      "efbb58199899: Preparing\n",
      "a1537cf26842: Waiting\n",
      "f390faf5524c: Preparing\n",
      "daee9dea71d9: Preparing\n",
      "00679b7c9426: Waiting\n",
      "add0b850bdf8: Waiting\n",
      "c2e412b87e2d: Waiting\n",
      "dc74b4fa6312: Preparing\n",
      "7730d4c011cd: Waiting\n",
      "f83c32bae622: Waiting\n",
      "86147dce553c: Waiting\n",
      "a86afd489635: Waiting\n",
      "725647e88671: Waiting\n",
      "0b07555d3a5b: Waiting\n",
      "cf7ec5236059: Preparing\n",
      "2784fc353e53: Preparing\n",
      "4748e1954466: Preparing\n",
      "17dd0d3a32ca: Preparing\n",
      "1a4d9b216faa: Preparing\n",
      "29ec3f10d323: Preparing\n",
      "b103452845b7: Preparing\n",
      "5de10b8eda77: Preparing\n",
      "93a1a17119ba: Preparing\n",
      "0b949eaca829: Preparing\n",
      "76dffad7db12: Preparing\n",
      "103f14ded07d: Preparing\n",
      "05a0d2d578a6: Preparing\n",
      "1f7bd087086a: Preparing\n",
      "a03ce844e2ad: Preparing\n",
      "b5583e44add1: Preparing\n",
      "3ff439c0455c: Preparing\n",
      "2ee8c052052a: Preparing\n",
      "f3154f787b0f: Preparing\n",
      "944a1106424f: Preparing\n",
      "01386fafb257: Preparing\n",
      "8a9d499564b0: Preparing\n",
      "d882bfae03e4: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "78e008bc66d2: Waiting\n",
      "913f47d5362d: Preparing\n",
      "06f02804b89d: Preparing\n",
      "e7d002ddb49b: Waiting\n",
      "54beb86c2dbe: Preparing\n",
      "aa57b43dc9e0: Preparing\n",
      "ae5c80704277: Preparing\n",
      "f2b540bc31be: Waiting\n",
      "d1cc4baf7a93: Preparing\n",
      "8fd21a588646: Preparing\n",
      "442b3d22fc2d: Waiting\n",
      "b470f3b3096a: Preparing\n",
      "72f4d03b40d8: Waiting\n",
      "9af2b05f2c3b: Preparing\n",
      "4cf9aed48cda: Preparing\n",
      "54244453f24a: Waiting\n",
      "57f574ab1503: Preparing\n",
      "c9dfb1d8d420: Waiting\n",
      "c5b9544e7743: Preparing\n",
      "c1af80eb8994: Waiting\n",
      "c3f11d77a5de: Preparing\n",
      "5c0e49e0fefd: Waiting\n",
      "01386fafb257: Waiting\n",
      "1a4d9b216faa: Waiting\n",
      "64579a0c8694: Waiting\n",
      "8a9d499564b0: Waiting\n",
      "29ec3f10d323: Waiting\n",
      "d882bfae03e4: Waiting\n",
      "82432f6543d2: Waiting\n",
      "b103452845b7: Waiting\n",
      "efbb58199899: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "f390faf5524c: Waiting\n",
      "913f47d5362d: Waiting\n",
      "daee9dea71d9: Waiting\n",
      "06f02804b89d: Waiting\n",
      "5de10b8eda77: Waiting\n",
      "54beb86c2dbe: Waiting\n",
      "dc74b4fa6312: Waiting\n",
      "aa57b43dc9e0: Waiting\n",
      "cf7ec5236059: Waiting\n",
      "93a1a17119ba: Waiting\n",
      "ae5c80704277: Waiting\n",
      "2784fc353e53: Waiting\n",
      "0b949eaca829: Waiting\n",
      "d1cc4baf7a93: Waiting\n",
      "4748e1954466: Waiting\n",
      "76dffad7db12: Waiting\n",
      "8fd21a588646: Waiting\n",
      "17dd0d3a32ca: Waiting\n",
      "b470f3b3096a: Waiting\n",
      "103f14ded07d: Waiting\n",
      "9af2b05f2c3b: Waiting\n",
      "05a0d2d578a6: Waiting\n",
      "4cf9aed48cda: Waiting\n",
      "1f7bd087086a: Waiting\n",
      "2ee8c052052a: Waiting\n",
      "57f574ab1503: Waiting\n",
      "a03ce844e2ad: Waiting\n",
      "c5b9544e7743: Waiting\n",
      "b5583e44add1: Waiting\n",
      "c3f11d77a5de: Waiting\n",
      "f3154f787b0f: Waiting\n",
      "3ff439c0455c: Waiting\n",
      "944a1106424f: Waiting\n",
      "c3c9ca29c9f2: Layer already exists\n",
      "bd113aa55fbd: Layer already exists\n",
      "e58f9c500afd: Layer already exists\n",
      "0b07555d3a5b: Layer already exists\n",
      "e844c8057c95: Layer already exists\n",
      "a1537cf26842: Layer already exists\n",
      "00679b7c9426: Layer already exists\n",
      "add0b850bdf8: Layer already exists\n",
      "c2e412b87e2d: Layer already exists\n",
      "7730d4c011cd: Layer already exists\n",
      "86147dce553c: Layer already exists\n",
      "f83c32bae622: Layer already exists\n",
      "a86afd489635: Layer already exists\n",
      "725647e88671: Layer already exists\n",
      "78e008bc66d2: Layer already exists\n",
      "f2b540bc31be: Layer already exists\n",
      "442b3d22fc2d: Layer already exists\n",
      "e7d002ddb49b: Layer already exists\n",
      "72f4d03b40d8: Layer already exists\n",
      "54244453f24a: Layer already exists\n",
      "b964b48b14cf: Pushed\n",
      "c9dfb1d8d420: Layer already exists\n",
      "c1af80eb8994: Layer already exists\n",
      "64579a0c8694: Layer already exists\n",
      "5c0e49e0fefd: Layer already exists\n",
      "82432f6543d2: Layer already exists\n",
      "efbb58199899: Layer already exists\n",
      "f390faf5524c: Layer already exists\n",
      "daee9dea71d9: Layer already exists\n",
      "dc74b4fa6312: Layer already exists\n",
      "cf7ec5236059: Layer already exists\n",
      "2784fc353e53: Layer already exists\n",
      "17dd0d3a32ca: Layer already exists\n",
      "4748e1954466: Layer already exists\n",
      "1a4d9b216faa: Layer already exists\n",
      "29ec3f10d323: Layer already exists\n",
      "b103452845b7: Layer already exists\n",
      "5de10b8eda77: Layer already exists\n",
      "93a1a17119ba: Layer already exists\n",
      "0b949eaca829: Layer already exists\n",
      "76dffad7db12: Layer already exists\n",
      "103f14ded07d: Layer already exists\n",
      "05a0d2d578a6: Layer already exists\n",
      "1f7bd087086a: Layer already exists\n",
      "a03ce844e2ad: Layer already exists\n",
      "b5583e44add1: Layer already exists\n",
      "3ff439c0455c: Layer already exists\n",
      "2ee8c052052a: Layer already exists\n",
      "944a1106424f: Layer already exists\n",
      "f3154f787b0f: Layer already exists\n",
      "01386fafb257: Layer already exists\n",
      "8a9d499564b0: Layer already exists\n",
      "d882bfae03e4: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "913f47d5362d: Layer already exists\n",
      "06f02804b89d: Layer already exists\n",
      "54beb86c2dbe: Layer already exists\n",
      "aa57b43dc9e0: Layer already exists\n",
      "ae5c80704277: Layer already exists\n",
      "d1cc4baf7a93: Layer already exists\n",
      "8fd21a588646: Layer already exists\n",
      "b470f3b3096a: Layer already exists\n",
      "9af2b05f2c3b: Layer already exists\n",
      "4cf9aed48cda: Layer already exists\n",
      "c5b9544e7743: Layer already exists\n",
      "57f574ab1503: Layer already exists\n",
      "c3f11d77a5de: Layer already exists\n",
      "3cef30a8019f: Pushed\n",
      "latest: digest: sha256:61f400d30031b32ecefd15c885e68211d3827079c65d5ee717d786c7c41dff62 size: 14640\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                                  STATUS\n",
      "db4bbe66-4a28-4476-9d69-e8d7ab9e52d7  2022-11-08T15:08:16+00:00  4M38S     gs://hybrid-vertex_cloudbuild/source/1667920095.497321-5d11b67f530e48d080b9d6f937632f29.tgz  gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1473cd1-af0d-4c63-aff0-d1286f096170",
   "metadata": {},
   "source": [
    "# Vertex Train Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cdca6-d053-401d-8f95-5f24d3192d7a",
   "metadata": {},
   "source": [
    "### Prepare `worker_pool_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "59df486c-8698-4528-9b08-dbc19de7c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    # args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        # \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6d7e-959a-4354-85d5-36f7a9a662ed",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81d7050b-e6a8-4314-b3df-27e7abf0d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ====================================================\n",
    "# Single | Single machine, single GPU\n",
    "# ====================================================\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'\n",
    "\n",
    "# ====================================================\n",
    "# Mirrored | Single Machine; multiple GPU\n",
    "# ====================================================\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-2g'           # a2-ultragpu-4g\n",
    "# REPLICA_COUNT = 1\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 2\n",
    "# REDUCTION_SERVER_COUNT = 0                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'mirrored'\n",
    "\n",
    "# ====================================================\n",
    "# Multi-Worker | Multiple Machines, 1 GPU per Machine\n",
    "# ====================================================\n",
    "# WORKER_MACHINE_TYPE = 'n1-standard-16'\n",
    "# REPLICA_COUNT = 10\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 10                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 2\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'\n",
    "\n",
    "# WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "# REPLICA_COUNT = 4\n",
    "# ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "# PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "# REDUCTION_SERVER_COUNT = 4                                                      \n",
    "# REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "# DISTRIBUTE_STRATEGY = 'multiworker'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158e910-f435-40fe-8150-6529191cab5d",
   "metadata": {},
   "source": [
    "## Train Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0383-d688-4fb9-b252-152a4639572e",
   "metadata": {},
   "source": [
    "### Previously defined Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3b9fcc0-f78a-4c43-9f71-aade96b55d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "VERSION: jtv1\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2\n",
      "MODEL_NAME: 2tower\n",
      "FRAMEWORK: merlin-tf\n",
      "MODEL_DISPLAY_NAME: vertex-merlin-tf-2tower-jtv1\n",
      "WORKSPACE: gs://jt-merlin-scaling/vertex-merlin-tf-2tower-jtv1\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT_ID}\")\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "487df262-9e3d-4dd9-b7c5-94ca8da2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: test-2tower-merlin-tf-jtv1\n",
      "RUN_NAME_PREFIX: run-v4\n",
      "TRAIN_DATA: gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed/train\n",
      "VALID_DATA: gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed/valid\n",
      "WORKFLOW_DIR: gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_PREFIX = 'test'\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-{MODEL_NAME}-{FRAMEWORK}-{VERSION}'\n",
    "RUN_NAME_PREFIX = f'run-v4' # timestamp assigned during job\n",
    "\n",
    "# # data and schema from JW \n",
    "# DATA_DIR = 'gs://spotify-beam-v3/merlin-processed'\n",
    "# TRAIN_DATA = f'{DATA_DIR}/train' #/_gcs_file_list.txt'\n",
    "# VALID_DATA = f'{DATA_DIR}/valid' #/_gcs_file_list.txt'\n",
    "# # WORKFLOW_DIR = f'gs://{DATA_DIR}/workflow/2t-spotify'\n",
    "# WORKFLOW_DIR = 'gs://spotify-beam-v3/merlin-processed/workflow/2t-spotify-workflow'\n",
    "\n",
    "# data and schema from nvtabular pipes\n",
    "DATA_DIR = 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed'\n",
    "TRAIN_DATA = f'{DATA_DIR}/train' #/_gcs_file_list.txt'\n",
    "VALID_DATA = f'{DATA_DIR}/valid' #/_gcs_file_list.txt'\n",
    "WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME_PREFIX: {RUN_NAME_PREFIX}\")\n",
    "print(f\"TRAIN_DATA: {TRAIN_DATA}\")\n",
    "print(f\"VALID_DATA: {VALID_DATA}\")\n",
    "print(f\"WORKFLOW_DIR: {WORKFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded705c-b752-4459-a8a3-4cab97f94360",
   "metadata": {},
   "source": [
    "### Managed TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a76672e0-ca2f-4c2c-9028-9d388bfa2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard resource name: projects/934903580331/locations/us-central1/tensorboards/70659015247396864\n",
      "TENSORBOARD_DISPLAY_NAME: tb-test-twotower-merlin-tf-jtv1\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Managed Tensorboard\n",
    "# ====================================================\n",
    "\n",
    "# create new\n",
    "# TENSORBOARD_DISPLAY_NAME = f\"tb-{EXPERIMENT_NAME}\"\n",
    "# tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME)\n",
    "# EXPERIMENT_TB = f'{tensorboard.gca_resource.name}'\n",
    "\n",
    "# use existing\n",
    "EXPERIMENT_TB = 'projects/934903580331/locations/us-central1/tensorboards/70659015247396864'\n",
    "TENSORBOARD_DISPLAY_NAME = 'tb-test-twotower-merlin-tf-jtv1'\n",
    "\n",
    "\n",
    "print(\"TensorBoard resource name:\", EXPERIMENT_TB)\n",
    "print(\"TENSORBOARD_DISPLAY_NAME:\", TENSORBOARD_DISPLAY_NAME)\n",
    "# TENSORBOARD= \"projects/934903580331/locations/us-central1/tensorboards/7439955380509081600\"\n",
    "# tensorboard = vertex_ai.Tensorboard(f'{TENSORBOARD}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58659-7506-4275-b26d-f37bfd3c9c79",
   "metadata": {},
   "source": [
    "### Worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "811a40ec-4829-4013-8820-2ffbfb5de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'command': ['sh',\n",
      "                                 '-euc',\n",
      "                                 'pip freeze && python -m trainer.train_task '\n",
      "                                 '--tb_name=projects/934903580331/locations/us-central1/tensorboards/70659015247396864 '\n",
      "                                 '--per_gpu_batch_size=16384     '\n",
      "                                 '--train_output_bucket=jt-merlin-scaling '\n",
      "                                 '--train_dir=gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed/train '\n",
      "                                 '--valid_dir=gs://jt-merlin-scaling/nvt-last5-v1full/nvt-processed/valid '\n",
      "                                 '--workflow_dir=gs://jt-merlin-scaling/nvt-last5-v1full/nvt-analyzed     '\n",
      "                                 '--num_epochs=1 --learning_rate=0.001 '\n",
      "                                 '--distribute=single     '\n",
      "                                 '--experiment_name=test-2tower-merlin-tf-jtv1 '\n",
      "                                 '--experiment_run=run-v4 '\n",
      "                                 '--project=hybrid-vertex '\n",
      "                                 '--location=us-central1'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv1-22_09_v2'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4096*4      # TODO: `batch_size * 4 ? jw\n",
    "LEARNING_RATE = 0.001\n",
    "LAYERS = \"[512, 256, 128]\"\n",
    "\n",
    "# ACCELERATOR_NUM = REPLICA_COUNT * PER_MACHINE_ACCELERATOR_COUNT\n",
    "# gpus = json.dumps([list(range(ACCELERATOR_NUM))]).replace(' ','')\n",
    "\n",
    "# WORKER_CMD = [\"python\", \"trainer/train_task.py\"]\n",
    "# WORKER_CMD = [\"python\", \"-m\", \"train_task\"]\n",
    "# WORKER_CMD = ['sh','-euc','pip freeze && python -m trainer.train_task']\n",
    "# WORKER_CMD = ['sh','-euc','pip freeze && python trainer/train_task.py']\n",
    "\n",
    "# WORKER_ARGS = [\n",
    "#     f'--project={PROJECT_ID}',\n",
    "#     f'--location={REGION}',\n",
    "#     f'--tb_name={EXPERIMENT_TB}',\n",
    "#     f'--workflow_dir={WORKFLOW_DIR}',\n",
    "#     f'--train_dir={TRAIN_DATA}',\n",
    "#     f'--valid_dir={VALID_DATA}',\n",
    "#     f'--train_output_bucket={OUTPUT_BUCKET}',\n",
    "#     f'--experiment_name={EXPERIMENT_NAME}',\n",
    "#     f'--experiment_run={RUN_NAME_PREFIX}',\n",
    "#     f'--distribute={DISTRIBUTE_STRATEGY}',\n",
    "#     f'--per_gpu_batch_size={BATCH_SIZE}',\n",
    "#     # f'--layer_sizes={LAYERS}',\n",
    "#     f'--learning_rate={LEARNING_RATE}',\n",
    "#     f'--num_epochs={NUM_EPOCHS}',\n",
    "# ]\n",
    "\n",
    "# python trainer/train_task.py    # python: can't open file 'trainer/train_task.py'\n",
    "# python -m train_task            # /usr/bin/python: No module named train_task\n",
    "# python -m trainer.train_task    # /etc/bash.bashrc: line 9: PS1: unbound variable\n",
    "    \n",
    "WORKER_CMD = [\n",
    "    'sh',\n",
    "    '-euc',\n",
    "    f'''pip freeze && python -m trainer.train_task --tb_name={EXPERIMENT_TB} --per_gpu_batch_size={BATCH_SIZE} \\\n",
    "    --train_output_bucket={OUTPUT_BUCKET} --train_dir={TRAIN_DATA} --valid_dir={VALID_DATA} --workflow_dir={WORKFLOW_DIR} \\\n",
    "    --num_epochs={NUM_EPOCHS} --learning_rate={LEARNING_RATE} --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "    --experiment_name={EXPERIMENT_NAME} --experiment_run={RUN_NAME_PREFIX} --project={PROJECT_ID} --location={REGION}'''\n",
    "]\n",
    "    # --layer_sizes={LAYERS} \\\n",
    "\n",
    "# ====================================================\n",
    "# Worker pool specs\n",
    "# ====================================================\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    # args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f5b0-22ab-451d-ba49-09cc445d053f",
   "metadata": {},
   "source": [
    "## Submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bddd12da-d6ef-4ffe-917f-408b7bade0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGING_BUCKET : gs://jt-merlin-scaling/test-2tower-merlin-tf-jtv1\n",
      "JOB_NAME : 2209-tb-train-vertex-merlin-tf-2tower-jtv1\n",
      "\n",
      "gpu_type : nvidia_tesla_a100\n",
      "gpu_per_replica : 1\n",
      "replica_cnt : 1\n"
     ]
    }
   ],
   "source": [
    "STAGING_BUCKET = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "\n",
    "job_prefix = '2209-tb'\n",
    "JOB_NAME = f'{job_prefix}-train-{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# labels for train job\n",
    "gpu_type = ACCELERATOR_TYPE.lower()\n",
    "gpu_per_replica = PER_MACHINE_ACCELERATOR_COUNT\n",
    "replica_cnt = REPLICA_COUNT\n",
    "\n",
    "print(f'STAGING_BUCKET : {STAGING_BUCKET}')\n",
    "print(f'JOB_NAME : {JOB_NAME}\\n')\n",
    "print(f'gpu_type : {gpu_type}')\n",
    "print(f'gpu_per_replica : {gpu_per_replica}')\n",
    "print(f'replica_cnt : {replica_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041217fa-0cf5-477a-b1a8-e5cfb58712bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    "    labels={\n",
    "        # 'mm_image' : 'nightly',\n",
    "        'gpu' : f'{gpu_type}',\n",
    "        'gpu_per_replica' : f'{gpu_per_replica}',\n",
    "        'replica_cnt' : f'{replica_cnt}',\n",
    "    }\n",
    ")\n",
    "job.run(sync=True, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=EXPERIMENT_TB,\n",
    "        restart_job_on_worker_restart=False,\n",
    "        enable_web_access=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b978-1a64-40e8-b8c4-149a7a40bca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
