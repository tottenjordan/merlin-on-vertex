{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7483ac1f-bb86-4b51-a296-92f545905cd4",
   "metadata": {},
   "source": [
    "# Train Merlin Two-Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d13046-5cb5-43c7-a6ee-b42a3ee9a1e9",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867b302d-ff4e-4ae4-ba6a-18b717f6d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import nvtabular as nvt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# from nvtabular.ops import (\n",
    "#     Categorify,\n",
    "#     TagAsUserID,\n",
    "#     TagAsItemID,\n",
    "#     TagAsItemFeatures,\n",
    "#     TagAsUserFeatures,\n",
    "#     AddMetadata,\n",
    "#     ListSlice\n",
    "# )\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.schema.tags import Tags\n",
    "\n",
    "# import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "# from merlin.io.dataset import Dataset as MerlinDataset\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "# import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "# from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9140a-8529-489e-9885-8431960e129a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48f540a-ccd4-4e15-96a8-25f4812f1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495ea222-2f9e-4dc5-8cfa-23d1470a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' # Change to your service account with Vertex AI Admin permitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc441d2e-b156-4d03-9951-ff943b129fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "BUCKET = 'jt-merlin-scaling' # 'spotify-merlin-v1'\n",
    "\n",
    "VERSION = 'jtv16'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "MODEL_DISPLAY_NAME = f'vertex-{FRAMEWORK}-{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions for training\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# # DOCKERNAME = 'hugectr'\n",
    "# DOCKERNAME = 'merlintf'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49f034-6107-426c-9ca5-78bc8e4bed22",
   "metadata": {},
   "source": [
    "# Training Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2dcb695-6e50-4efd-9715-0eb02e1dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAIN_SUB_DIR = 'trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cfb1e6-fef6-42b7-ba21-b730ade84cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f47e4461-5b2d-4082-bfd3-88072163d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db637aa-c331-4db0-a7da-5fcceed19fd6",
   "metadata": {},
   "source": [
    "## Interactive Train Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a376724-9ed3-4a8b-9b6f-cc139ccc0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141daa01-b5dc-4757-a92c-aad284dea45b",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5d7ad57-89f5-4c68-9fa4-26644c2541d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/two_tower_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/two_tower_model.py\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "import nvtabular as nvt\n",
    "# # import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_two_tower(\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    layer_sizes: List[Any] = [512, 256, 128],\n",
    "):\n",
    "    \n",
    "    #=========================================\n",
    "    # get workflow details\n",
    "    #=========================================\n",
    "    workflow = nvt.Workflow.load(workflow_dir) # gs://spotify-merlin-v1/nvt-preprocessing-spotify-v24/nvt-analyzed\n",
    "    \n",
    "    schema = workflow.output_schema\n",
    "    # embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    user_schema = schema.select_by_tag(Tags.USER)\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    \n",
    "    #=========================================\n",
    "    # build towers\n",
    "    #=========================================\n",
    "    query = mm.Encoder(user_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    item_schema = schema.select_by_tag(Tags.ITEM)\n",
    "    item_inputs = mm.InputBlockV2(\n",
    "        item_schema,\n",
    "    )\n",
    "    candidate = mm.Encoder(item_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    model = mm.TwoTowerModelV2(\n",
    "        query_tower=query,\n",
    "        candidate_tower=candidate,\n",
    "        # output=mm.ContrastiveOutput(\n",
    "        #     to_call=DotProduct(),\n",
    "        #     negative_samplers=\"in-batch\",\n",
    "        #     schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "        #     candidate_name=\"item\",\n",
    "        # )\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac09-65a6-494c-9b42-7b12a22d650f",
   "metadata": {},
   "source": [
    "## Train task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a431f3c9-7e9c-4399-a541-9b6eb514c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "# os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "\n",
    "# # nvtabular\n",
    "# import nvtabular as nvt\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# merlin\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "# nvtabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# gcp\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "# import hypertune\n",
    "# from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "# repo\n",
    "from .two_tower_model import create_two_tower\n",
    "# import utils\n",
    "\n",
    "# local\n",
    "HYPERTUNE_METRIC_NAME = 'AUC'\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions - TODO: move to utils?\n",
    "# ====================================================\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "    # return (task_type == 'chief') or (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "    # return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "\n",
    "def get_upload_logs_to_manged_tb_command(tb_resource_name, logs_dir, experiment_name, ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(\n",
    "        f\"\"\"tb-gcp-uploader --tensorboard_resource_name={tb_resource_name} \\\n",
    "        --logdir={logs_dir} \\\n",
    "        --experiment_name={experiment_name} \\\n",
    "        --one_shot={oneshot} \\\n",
    "        --event_file_inactive_secs={60*60*ttl_hrs}\"\"\"\n",
    "    )\n",
    "\n",
    "def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name, project):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "# ====================================================\n",
    "# TRAINING SCRIPT\n",
    "# ====================================================\n",
    "    \n",
    "def main(args):\n",
    "    \"\"\"Runs a training loop.\"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    # os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "    # os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "    \n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    vertex_ai.init(project=f'{args.project}', location=f'{args.location}')\n",
    "    storage_client = storage.Client(project=args.project)\n",
    "    logging.info(\"vertex_ai initialized...\")\n",
    "    \n",
    "    EXPERIMENT_NAME = f\"{args.experiment_name}\"\n",
    "    RUN_NAME = f\"{args.experiment_run}\" #-{TIMESTAMP}\" # f\"{args.experiment_run}\"\n",
    "    logging.info(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\\n RUN_NAME: {RUN_NAME}\")\n",
    "    \n",
    "    WORKING_DIR_GCS_URI = f'gs://{args.train_output_bucket}/{EXPERIMENT_NAME}/{RUN_NAME}'\n",
    "    logging.info(f\"WORKING_DIR_GCS_URI: {WORKING_DIR_GCS_URI}\")\n",
    "    \n",
    "    TB_RESOURCE_NAME = f'{args.tb_name}'\n",
    "    LOGS_DIR = f'{WORKING_DIR_GCS_URI}/tb_logs'\n",
    "    logging.info(f\"tensorboard LOGS_DIR: {LOGS_DIR}\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU Strategy\n",
    "    # ====================================================    \n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.test.is_gpu_available(): # TODO: replace with - tf.config.list_physical_devices('GPU')\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "        \n",
    "    \n",
    "    # set related vars...\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = NUM_WORKERS * args.per_gpu_batch_size\n",
    "    # num_gpus = sum([len(gpus) for gpus in args.gpus])\n",
    "    # GLOBAL_BATCH_SIZE = num_gpus * args.per_gpu_batch_size\n",
    "\n",
    "    logging.info(f'NUM_WORKERS = {NUM_WORKERS}')\n",
    "    # logging.info(f'num_gpus: {num_gpus}')\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE: {GLOBAL_BATCH_SIZE}')\n",
    "    \n",
    "    # set worker vars...\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "        \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'Loading workflow & schema from : {args.workflow_dir}')\n",
    "    \n",
    "    workflow = nvt.Workflow.load(args.workflow_dir) # gs://{BUCKET}/..../nvt-analyzed\n",
    "    schema = workflow.output_schema\n",
    "    # embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    train_data = MerlinDataset(os.path.join(args.train_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    valid_data = MerlinDataset(os.path.join(args.valid_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    \n",
    "    # train_data = MerlinDataset(args.train_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    # valid_data = MerlinDataset(args.valid_dir + \"*.parquet\", part_size=\"1GB\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Callbacks\n",
    "    # ====================================================\n",
    "    class UploadTBLogsBatchEnd(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            os.system(\n",
    "                get_upload_logs_to_manged_tb_command(\n",
    "                    tb_resource_name=TB_RESOURCE_NAME, \n",
    "                    logs_dir=LOGS_DIR, \n",
    "                    experiment_name=EXPERIMENT_NAME,\n",
    "                    ttl_hrs = 5, \n",
    "                    oneshot=\"true\",\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=LOGS_DIR,\n",
    "        histogram_freq=0, \n",
    "        write_graph=True, \n",
    "        # profile_batch=(20,50)\n",
    "    )\n",
    "    \n",
    "    # ====================================================\n",
    "    # Train\n",
    "    # ==================================================== \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "\n",
    "    # with strategy.scope():\n",
    "    model = create_two_tower(\n",
    "        train_dir=args.train_dir,\n",
    "        valid_dir=args.valid_dir,\n",
    "        workflow_dir=args.workflow_dir,\n",
    "        layer_sizes=LAYER_SIZES # args.layer_sizes,\n",
    "    )\n",
    "        \n",
    "        \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "        run_eagerly=False,\n",
    "        metrics=[mm.RecallAt(1), mm.RecallAt(10), mm.NDCGAt(10)],\n",
    "    )\n",
    "    \n",
    "    # cloud_profiler.init() # managed TB profiler\n",
    "        \n",
    "    logging.info('Starting training loop...')\n",
    "    \n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=valid_data, \n",
    "        batch_size=GLOBAL_BATCH_SIZE, \n",
    "        epochs=args.num_epochs,\n",
    "        # steps_per_epoch=20, \n",
    "        callbacks=[\n",
    "            tensorboard_callback, \n",
    "            UploadTBLogsBatchEnd()\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    elapsed_model_fit = end_model_fit - start_model_fit\n",
    "    elapsed_model_fit = round(elapsed_model_fit, 2)\n",
    "    logging.info(f'Elapsed model_fit: {elapsed_model_fit} seconds')\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams & metrics for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging params & metrics for Vertex Experiments')\n",
    "    \n",
    "    # get the metrics for the experiment run\n",
    "    history_keys = model.history.history.keys()\n",
    "    metrics_dict = {}\n",
    "    _ = [metrics_dict.update({key: model.history.history[key][-1]}) for key in history_keys]\n",
    "    metrics_dict[\"elapsed_model_fit\"] = elapsed_model_fit\n",
    "    \n",
    "    logging.info(f'metrics_dict: {metrics_dict}')\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{EXPERIMENT_NAME}'\n",
    "    metaparams[\"experiment_run\"] = f\"{RUN_NAME}\"\n",
    "    \n",
    "    logging.info(f'metaparams: {metaparams}')\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"num_gpus\"] = NUM_WORKERS # num_gpus\n",
    "    hyperparams[\"per_gpu_batch_size\"] = args.per_gpu_batch_size\n",
    "    hyperparams[\"global_batch_size\"] = GLOBAL_BATCH_SIZE\n",
    "    hyperparams[\"learning_rate\"] = args.learning_rate\n",
    "    hyperparams['layers'] = f'{args.layer_sizes}'\n",
    "    \n",
    "    logging.info(f'hyperparams: {hyperparams}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Experiments\n",
    "    # ====================================================\n",
    "    logging.info(f\"Creating run: {RUN_NAME}; for experiment: {EXPERIMENT_NAME}\")\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "    \n",
    "        # Create experiment\n",
    "        vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "\n",
    "        with vertex_ai.start_run(RUN_NAME) as my_run:\n",
    "            logging.info(f\"logging metrics_dict\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams\")\n",
    "            my_run.log_params(metaparams)\n",
    "\n",
    "            logging.info(f\"logging hyperparams\")\n",
    "            my_run.log_params(hyperparams)\n",
    "        \n",
    "    # =============================================\n",
    "    # save retrieval (query) tower\n",
    "    # =============================================\n",
    "    # set vars...\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model-dir\"\n",
    "    logging.info(f'Saving towers to {MODEL_DIR}')\n",
    "    \n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query-tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate-tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate-embeddings\"\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        # save query tower\n",
    "        query_tower = model.query_encoder\n",
    "        query_tower.save(QUERY_TOWER_PATH)\n",
    "        logging.info(f'Saved query tower to {QUERY_TOWER_PATH}')\n",
    "        \n",
    "        candidate_tower = model.candidate_encoder\n",
    "        candidate_tower.save(CANDIDATE_TOWER_PATH)\n",
    "        logging.info(f'Saved candidate tower to {CANDIDATE_TOWER_PATH}')\n",
    "    \n",
    "    # =============================================\n",
    "    # save embeddings for ME index\n",
    "    # =============================================\n",
    "    EMBEDDINGS_FILE_NAME = \"candidate_embeddings.json\"\n",
    "    logging.info(f\"Saving {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    # def format_for_matching_engine(data) -> None:\n",
    "    #     emb = [data[i] for i in range(LAYER_SIZES[-1])] # get the embeddings\n",
    "    #     formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "    #     with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "    #         f.write(formatted_emb)\n",
    "    #         f.write(\"\\n\")\n",
    "    \n",
    "    def format_for_matching_engine(data) -> None:\n",
    "        cols = [str(i) for i in range(LAYER_SIZES[-1])]      # ensure we are only pulling 0-EMBEDDING_DIM cols\n",
    "        emb = [data[col] for col in cols]                    # get the embeddings\n",
    "        formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "        with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "            f.write(formatted_emb)\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # !rm candidate_embeddings.json > /dev/null \n",
    "    # !touch candidate_embeddings.json\n",
    "    item_data = pd.read_parquet(f'{args.workflow_dir}/categories/unique.track_uri_can.parquet')\n",
    "    lookup_dict = dict(item_data['track_uri_can'])\n",
    "\n",
    "    # item embeds from TRAIN\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features = (unique_rows_by_features(train_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs = model.candidate_embeddings(item_features, index=item_features.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd = item_embs.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd['track_uri_can'] = item_emb_pd['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting TRAIN embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    # item embeds from VALID\n",
    "    start_embeds = time.time()\n",
    "    \n",
    "    item_features_val = (unique_rows_by_features(valid_data, Tags.ITEM, Tags.ID))\n",
    "    item_embs_val = model.candidate_embeddings(item_features_val, index=item_features_val.schema['track_uri_can'], batch_size=10000)\n",
    "    item_emb_pd_val = item_embs_val.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "    item_emb_pd_val['track_uri_can'] = item_emb_pd_val['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "    _ = item_emb_pd_val.apply(format_for_matching_engine, axis=1)\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_embeds = time.time()\n",
    "    elapsed_time = end_embeds - start_embeds\n",
    "    elapsed_time = round(elapsed_time, 2)\n",
    "    logging.info(f'Elapsed time writting VALID embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        _upload_blob_gcs(\n",
    "            EMBEDDINGS_PATH, \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\", \n",
    "            f\"{EMBEDDINGS_FILE_NAME}\",\n",
    "            args.project\n",
    "        )\n",
    "    \n",
    "    logging.info('All done - model saved') #all done\n",
    "    \n",
    "# ====================================================\n",
    "# arg parser\n",
    "# ====================================================\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--experiment_name',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed-experiment',\n",
    "        help='name of vertex ai experiment'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--experiment_run',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='unnamed_run',\n",
    "        help='name of vertex ai experiment run'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--tb_name',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='projects/XXXXXX/locations/us-central1/tensorboards/XXXXXXXX'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--distribute',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='single',\n",
    "        help='training strategy'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_output_bucket',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        # default='single',\n",
    "        help='gcs bucket name'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workflow_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to saved workflow.pkl e.g., nvt-analyzed'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--train_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to training data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--valid_dir',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='Path to validation data _file_list.txt'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--num_epochs',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='num_epochs'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--per_gpu_batch_size',\n",
    "        type=int,\n",
    "        required=True,\n",
    "        help='Per GPU Batch size'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--layer_sizes',\n",
    "        type=str,\n",
    "        required=False,\n",
    "        default='[512, 256, 128]',\n",
    "        help='layer_sizes'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        required=False,\n",
    "        default=.001,\n",
    "        help='learning_rate'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--project',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp project'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--location',\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help='gcp location'\n",
    "    )\n",
    "    # parser.add_argument(\n",
    "    #     '--gpus',\n",
    "    #     type=str,\n",
    "    #     required=False,\n",
    "    #     default='[[0]]',\n",
    "    #     help='GPU devices to use for Preprocessing'\n",
    "    # )\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    # parsed_args.gpus = json.loads(parsed_args.gpus)\n",
    "\n",
    "    # parsed_args.slot_size_array = [\n",
    "    #     int(i) for i in parsed_args.slot_size_array.split(sep=' ')\n",
    "    # ]\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting training')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158e674-6b0b-47ac-ac12-cce0b002bf93",
   "metadata": {},
   "source": [
    "## Training Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa81f3-0912-4f00-ba09-9bb9c40991eb",
   "metadata": {},
   "source": [
    "### versioned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad9482d-5713-43bc-81db-d269ac4283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker definitions for training\n",
    "MERLIN_VERSION = '22_12_v4'\n",
    "IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}-{MERLIN_VERSION}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = f'merlintf-{MERLIN_VERSION}'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212d9ee-4b1b-4092-8094-b6f0b88c42c6",
   "metadata": {},
   "source": [
    "* nvtabular==1.5.0\n",
    "* nvtabular==1.3.3\n",
    "* cloudml-hypertune\n",
    "\n",
    "```\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13146051-f765-4bdf-9e87-d2eb8f05e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "# FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.09\n",
    "\n",
    "# WORKDIR /src\n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5 gcsfs google-cloud-aiplatform[cloud_profiler] kfp\n",
    "# RUN echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main\" | tee -a /etc/apt/sources.list.d/google-cloud-sdk.list && curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key --keyring /usr/share/keyrings/cloud.google.gpg  add - && apt-get update -y && apt-get install google-cloud-sdk -y\n",
    "\n",
    "# COPY trainer/* ./\n",
    "\n",
    "# ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/hugectr/lib:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib:/opt/tritonserver/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e11fdda-b3be-44f9-98df-6ee962f68cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN pip install git+https://github.com/NVIDIA-Merlin/models.git@efe4bc91cc7e161f6e1c6dab3ff2a8ef04fd84b5 gcsfs google-cloud-aiplatform fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0848b1d2-b50a-4040-b0ce-f595750fb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/Dockerfile.merlintf-22_12_v4\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# RUN pip install -U pip\n",
    "RUN pip install merlin-models gcsfs google-cloud-aiplatform fastapi\n",
    "\n",
    "\n",
    "COPY trainer /trainer\n",
    "\n",
    "RUN apt update && apt -y install nvtop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7f449cb-78f2-4e8b-a056-eb139469ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/requirements.txt\n",
    "# fastapi\n",
    "# merlin-models\n",
    "# gcsfs\n",
    "# google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5134-d7aa-4f6c-882c-5f11d35cbfc2",
   "metadata": {},
   "source": [
    "# Build Train Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54394f2-7c65-40ea-9102-ccf0cd3178a5",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "817dc303-b3e6-4427-bfd0-5cd949183a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68e469cf-13ae-47e6-93bb-30aef60ebe3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir('/home/jupyter/jt-merlin/merlin-on-vertex')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e640822-e85b-4ba2-a192-9c8d790ff2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1473cd1-af0d-4c63-aff0-d1286f096170",
   "metadata": {},
   "source": [
    "# Vertex Train Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cdca6-d053-401d-8f95-5f24d3192d7a",
   "metadata": {},
   "source": [
    "### Prepare `worker_pool_specs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59df486c-8698-4528-9b08-dbc19de7c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    # args,\n",
    "    cmd,\n",
    "    replica_count=1,\n",
    "    machine_type=\"n1-standard-16\",\n",
    "    accelerator_count=1,\n",
    "    accelerator_type=\"ACCELERATOR_TYPE_UNSPECIFIED\",\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type=\"n1-highcpu-16\",\n",
    "    reduction_server_image_uri=\"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\",\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            \"machine_type\": machine_type,\n",
    "            \"accelerator_type\": accelerator_type,\n",
    "            \"accelerator_count\": accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\"machine_type\": machine_type}\n",
    "\n",
    "    container_spec = {\n",
    "        \"image_uri\": image_uri,\n",
    "        # \"args\": args,\n",
    "        \"command\": cmd,\n",
    "    }\n",
    "\n",
    "    chief_spec = {\n",
    "        \"replica_count\": 1,\n",
    "        \"machine_spec\": machine_spec,\n",
    "        \"container_spec\": container_spec,\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": replica_count - 1,\n",
    "            \"machine_spec\": machine_spec,\n",
    "            \"container_spec\": container_spec,\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            \"replica_count\": reduction_server_count,\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": reduction_server_machine_type,\n",
    "            },\n",
    "            \"container_spec\": {\"image_uri\": reduction_server_image_uri},\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "\n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6d7e-959a-4354-85d5-36f7a9a662ed",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81d7050b-e6a8-4314-b3df-27e7abf0d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ====================================================\n",
    "# Single | Single machine, single GPU\n",
    "# ====================================================\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158e910-f435-40fe-8150-6529191cab5d",
   "metadata": {},
   "source": [
    "## Train Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0383-d688-4fb9-b252-152a4639572e",
   "metadata": {},
   "source": [
    "### Previously defined Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3b9fcc0-f78a-4c43-9f71-aade96b55d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "VERSION: jtv16\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv16-22_12_v4\n",
      "MODEL_NAME: 2tower\n",
      "FRAMEWORK: merlin-tf\n",
      "MODEL_DISPLAY_NAME: vertex-merlin-tf-2tower-jtv16\n",
      "WORKSPACE: gs://jt-merlin-scaling/vertex-merlin-tf-2tower-jtv16\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv16-22_12_v4\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT_ID}\")\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "487df262-9e3d-4dd9-b7c5-94ca8da2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: latest-2tower-merlin-tf-jtv16\n",
      "RUN_NAME_PREFIX: run-20230223-222603\n",
      "TRAIN_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train/\n",
      "VALID_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid/\n",
      "WORKFLOW_DIR: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXPERIMENT_PREFIX = 'latest'\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-{MODEL_NAME}-{FRAMEWORK}-{VERSION}'\n",
    "RUN_NAME_PREFIX = f'run-{TIMESTAMP}' # timestamp assigned during job\n",
    "\n",
    "# data and schema from nvtabular pipes\n",
    "DATA_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed'\n",
    "# DATA_DIR = 'gs://spotify-beam-v3/merlin-processed' #/train\n",
    "\n",
    "TRAIN_DATA = f'{DATA_DIR}/train/' #/_gcs_file_list.txt'\n",
    "VALID_DATA = f'{DATA_DIR}/valid/' #/_gcs_file_list.txt'\n",
    "\n",
    "WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed'\n",
    "# WORKFLOW_DIR = 'gs://spotify-beam-v3/merlin-processed/workflow/2t-spotify-workflow'\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME_PREFIX: {RUN_NAME_PREFIX}\")\n",
    "print(f\"TRAIN_DATA: {TRAIN_DATA}\")\n",
    "print(f\"VALID_DATA: {VALID_DATA}\")\n",
    "print(f\"WORKFLOW_DIR: {WORKFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded705c-b752-4459-a8a3-4cab97f94360",
   "metadata": {},
   "source": [
    "### Managed TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a76672e0-ca2f-4c2c-9028-9d388bfa2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/2559566312439283712\n",
      "TB display name: latest-2tower-merlin-tf-jtv16-v1\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Managed Tensorboard\n",
    "# ====================================================\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-v1\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=LOCATION)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58659-7506-4275-b26d-f37bfd3c9c79",
   "metadata": {},
   "source": [
    "### Worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "811a40ec-4829-4013-8820-2ffbfb5de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'command': ['sh',\n",
      "                                 '-euc',\n",
      "                                 'pip freeze && python -m trainer.train_task '\n",
      "                                 '--tb_name=projects/934903580331/locations/us-central1/tensorboards/2559566312439283712 '\n",
      "                                 '--per_gpu_batch_size=16384     '\n",
      "                                 '--train_output_bucket=jt-merlin-scaling '\n",
      "                                 '--train_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train/ '\n",
      "                                 '--valid_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid/ '\n",
      "                                 '--workflow_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed     '\n",
      "                                 '--num_epochs=1 --learning_rate=0.001 '\n",
      "                                 '--distribute=single     '\n",
      "                                 '--experiment_name=latest-2tower-merlin-tf-jtv16 '\n",
      "                                 '--experiment_run=run-20230223-222603 '\n",
      "                                 '--project=hybrid-vertex '\n",
      "                                 '--location=us-central1'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/merlin-tf-2tower-training-jtv16-22_12_v4'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 4096*4      # TODO: `batch_size * 4 ? jw\n",
    "LEARNING_RATE = 0.001\n",
    "LAYERS = \"[512, 256, 128]\"\n",
    "\n",
    "# python trainer/train_task.py    # python: can't open file 'trainer/train_task.py'\n",
    "# python -m train_task            # /usr/bin/python: No module named train_task\n",
    "# python -m trainer.train_task    # /etc/bash.bashrc: line 9: PS1: unbound variable\n",
    "    \n",
    "WORKER_CMD = [\n",
    "    'sh',\n",
    "    '-euc',\n",
    "    f'''pip freeze && python -m trainer.train_task --tb_name={TB_RESOURCE_NAME} --per_gpu_batch_size={BATCH_SIZE} \\\n",
    "    --train_output_bucket={OUTPUT_BUCKET} --train_dir={TRAIN_DATA} --valid_dir={VALID_DATA} --workflow_dir={WORKFLOW_DIR} \\\n",
    "    --num_epochs={NUM_EPOCHS} --learning_rate={LEARNING_RATE} --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "    --experiment_name={EXPERIMENT_NAME} --experiment_run={RUN_NAME_PREFIX} --project={PROJECT_ID} --location={LOCATION}'''\n",
    "]\n",
    "    # --layer_sizes={LAYERS} \\\n",
    "\n",
    "# ====================================================\n",
    "# Worker pool specs\n",
    "# ====================================================\n",
    "    \n",
    "WORKER_POOL_SPECS = prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    # args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)\n",
    "# jt-merlin-scaling/nvt-last5-latest-12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f5b0-22ab-451d-ba49-09cc445d053f",
   "metadata": {},
   "source": [
    "## Submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bddd12da-d6ef-4ffe-917f-408b7bade0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_OUTPUT_DIR : gs://jt-merlin-scaling/latest-2tower-merlin-tf-jtv16\n",
      "JOB_NAME : 2212-tb-train-vertex-merlin-tf-2tower-jtv16\n",
      "\n",
      "gpu_type : nvidia_tesla_a100\n",
      "gpu_per_replica : 1\n",
      "replica_cnt : 1\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    # experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "job_prefix = '2212-tb'\n",
    "JOB_NAME = f'{job_prefix}-train-{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# labels for train job\n",
    "gpu_type = ACCELERATOR_TYPE.lower()\n",
    "gpu_per_replica = PER_MACHINE_ACCELERATOR_COUNT\n",
    "replica_cnt = REPLICA_COUNT\n",
    "\n",
    "print(f'BASE_OUTPUT_DIR : {BASE_OUTPUT_DIR}')\n",
    "print(f'JOB_NAME : {JOB_NAME}\\n')\n",
    "print(f'gpu_type : {gpu_type}')\n",
    "print(f'gpu_per_replica : {gpu_per_replica}')\n",
    "print(f'replica_cnt : {replica_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "041217fa-0cf5-477a-b1a8-e5cfb58712bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    base_output_dir=BASE_OUTPUT_DIR,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    labels={\n",
    "        # 'mm_image' : 'nightly',\n",
    "        'gpu' : f'{gpu_type}',\n",
    "        'gpu_per_replica' : f'{gpu_per_replica}',\n",
    "        'replica_cnt' : f'{replica_cnt}',\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b978-1a64-40e8-b8c4-149a7a40bca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
