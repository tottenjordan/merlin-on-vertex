{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7483ac1f-bb86-4b51-a296-92f545905cd4",
   "metadata": {},
   "source": [
    "# Train Merlin Two-Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d13046-5cb5-43c7-a6ee-b42a3ee9a1e9",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867b302d-ff4e-4ae4-ba6a-18b717f6d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import nvtabular as nvt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# from nvtabular.ops import (\n",
    "#     Categorify,\n",
    "#     TagAsUserID,\n",
    "#     TagAsItemID,\n",
    "#     TagAsItemFeatures,\n",
    "#     TagAsUserFeatures,\n",
    "#     AddMetadata,\n",
    "#     ListSlice\n",
    "# )\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.schema.tags import Tags\n",
    "\n",
    "# import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "# from merlin.io.dataset import Dataset as MerlinDataset\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "# import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "# from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9140a-8529-489e-9885-8431960e129a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48f540a-ccd4-4e15-96a8-25f4812f1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495ea222-2f9e-4dc5-8cfa-23d1470a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' # Change to your service account with Vertex AI Admin permitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc441d2e-b156-4d03-9951-ff943b129fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "BUCKET = 'jt-merlin-scaling' # 'spotify-merlin-v1'\n",
    "\n",
    "VERSION = 'jtv34'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "MODEL_DISPLAY_NAME = f'vertex-{FRAMEWORK}-{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions for training\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# # DOCKERNAME = 'hugectr'\n",
    "# DOCKERNAME = 'merlintf'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49f034-6107-426c-9ca5-78bc8e4bed22",
   "metadata": {},
   "source": [
    "# Training Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2dcb695-6e50-4efd-9715-0eb02e1dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAIN_SUB_DIR = 'trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cfb1e6-fef6-42b7-ba21-b730ade84cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f47e4461-5b2d-4082-bfd3-88072163d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db637aa-c331-4db0-a7da-5fcceed19fd6",
   "metadata": {},
   "source": [
    "## Interactive Train Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a376724-9ed3-4a8b-9b6f-cc139ccc0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141daa01-b5dc-4757-a92c-aad284dea45b",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5d7ad57-89f5-4c68-9fa4-26644c2541d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/two_tower_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/two_tower_model.py\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "import nvtabular as nvt\n",
    "# # import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_two_tower(\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    layer_sizes: List[Any] = [512, 256, 128],\n",
    "):\n",
    "    \n",
    "    #=========================================\n",
    "    # get workflow details\n",
    "    #=========================================\n",
    "    workflow = nvt.Workflow.load(workflow_dir) # gs://spotify-merlin-v1/nvt-preprocessing-spotify-v24/nvt-analyzed\n",
    "    \n",
    "    schema = workflow.output_schema\n",
    "    # embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    user_schema = schema.select_by_tag(Tags.USER)\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    \n",
    "    #=========================================\n",
    "    # build towers\n",
    "    #=========================================\n",
    "    query = mm.Encoder(user_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    item_schema = schema.select_by_tag(Tags.ITEM)\n",
    "    item_inputs = mm.InputBlockV2(\n",
    "        item_schema,\n",
    "    )\n",
    "    candidate = mm.Encoder(item_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    model = mm.RetrievalModelV2(\n",
    "        query=query,\n",
    "        candidate=candidate,\n",
    "        output=mm.ContrastiveOutput(\n",
    "            to_call=DotProduct(),\n",
    "            negative_samplers=\"in-batch\",\n",
    "            schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model = mm.TwoTowerModelV2(\n",
    "    #     query_tower=query,\n",
    "    #     candidate_tower=candidate,\n",
    "    #     # output=mm.ContrastiveOutput(\n",
    "    #     #     to_call=DotProduct(),\n",
    "    #     #     negative_samplers=\"in-batch\",\n",
    "    #     #     schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "    #     #     candidate_name=\"item\",\n",
    "    #     # )\n",
    "    # )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafc15a-2eb7-4c19-94e2-a42484996f75",
   "metadata": {},
   "source": [
    "## Trainer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83db1eb-8f3d-4d05-a63e-e90769dc6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_utils.py\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions - moved from train_task.py\n",
    "# ====================================================\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# GCS_CLIENT = storage.Client()\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "\n",
    "def get_upload_logs_to_manged_tb_command(tb_resource_name, logs_dir, experiment_name, ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(\n",
    "        f\"\"\"tb-gcp-uploader --tensorboard_resource_name={tb_resource_name} \\\n",
    "        --logdir={logs_dir} \\\n",
    "        --experiment_name={experiment_name} \\\n",
    "        --one_shot={oneshot} \\\n",
    "        --event_file_inactive_secs={60*60*ttl_hrs}\"\"\"\n",
    "    )\n",
    "\n",
    "def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name, project):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    storage_client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = storage_client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "def upload_from_directory(\n",
    "    directory_path: str, \n",
    "    dest_bucket_name: str, \n",
    "    dest_blob_name: str,\n",
    "    project: str,\n",
    "):\n",
    "    storage_client = storage.Client(project=project)\n",
    "    rel_paths = glob.glob(directory_path + '/**', recursive=True)\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    \n",
    "    for local_file in rel_paths:\n",
    "        remote_path = f'{dest_blob_name}/{\"/\".join(local_file.split(os.sep)[1:])}'\n",
    "        if os.path.isfile(local_file):\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac09-65a6-494c-9b42-7b12a22d650f",
   "metadata": {},
   "source": [
    "## Train task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a431f3c9-7e9c-4399-a541-9b6eb514c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "# os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "\n",
    "# merlin\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "# nvtabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# gcp\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "# import hypertune\n",
    "import traceback\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "# repo\n",
    "from .two_tower_model import create_two_tower\n",
    "from .train_utils import (\n",
    "    get_upload_logs_to_manged_tb_command, \n",
    "    get_arch_from_string, \n",
    "    _upload_blob_gcs, \n",
    "    upload_from_directory\n",
    ")\n",
    "\n",
    "# local\n",
    "HYPERTUNE_METRIC_NAME = 'AUC'\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "# ====================================================\n",
    "# arg parser\n",
    "# ====================================================\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command line arguments\n",
    "    \n",
    "    type: int, float, str\n",
    "          bool() converts empty strings to `False` and non-empty strings to `True`\n",
    "          see more details here: https://docs.python.org/3/library/argparse.html#type\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--experiment_name',type=str,required=False,default='unnamed-experiment')\n",
    "    parser.add_argument('--experiment_run', type=str, required=False, default='unnamed_run')\n",
    "    parser.add_argument('--tb_name', type=str, required=False)\n",
    "    parser.add_argument('--distribute', type=str, required=False, default='single')\n",
    "    parser.add_argument('--train_output_bucket', type=str, required=True) # default='single',)\n",
    "    parser.add_argument('--workflow_dir', type=str, required=True)\n",
    "    parser.add_argument('--train_dir', type=str, required=True)\n",
    "    parser.add_argument('--valid_dir', type=str, required=True)\n",
    "    parser.add_argument('--num_epochs', type=int, required=True)\n",
    "    parser.add_argument('--per_gpu_batch_size', type=int, required=True)\n",
    "    parser.add_argument('--layer_sizes', type=str, required=False, default='[512, 256, 128]')\n",
    "    parser.add_argument('--learning_rate', type=float, required=False, default=.001)\n",
    "    parser.add_argument('--project', type=str, required=True)\n",
    "    parser.add_argument('--location', type=str, required=True)\n",
    "    parser.add_argument('--valid_frequency', type=int, required=False)\n",
    "    parser.add_argument('--epoch_steps', type=int, required=False)\n",
    "    parser.add_argument('--valid_steps', type=int, required=False)\n",
    "    parser.add_argument('--chkpt_freq', required=True) # type=int | TODO: value could be int or string\n",
    "    parser.add_argument(\"--profiler\", action='store_true', help=\"include for True; ommit for False\")\n",
    "    parser.add_argument(\"--write_embeddings\", action='store_true', help=\"include for True; ommit for False\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "                        \n",
    "# ====================================================\n",
    "# TRAINING SCRIPT\n",
    "# ====================================================\n",
    "    \n",
    "def main(args):\n",
    "    \"\"\"Runs a training loop.\"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    # os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "    # os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # handle train job restarts for experiment runs (no duplicates)\n",
    "    # ====================================================\n",
    "    logging.info(f\"EXPERIMENT_NAME: {args.experiment_name}\\n RUN_NAME: {args.experiment_run}\")\n",
    "    \n",
    "    SESSION_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
    "    EXPERIMENT_RUN_THIS = f'{args.experiment_run}-{SESSION_id}'\n",
    "    \n",
    "    logging.info(f\"Changing: {args.experiment_run} to: {EXPERIMENT_RUN_THIS} to handle job restarts\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set directories\n",
    "    # ====================================================\n",
    "    WORKING_DIR_GCS_URI = f'gs://{args.train_output_bucket}/{args.experiment_name}/{args.experiment_run}'\n",
    "    logging.info(f\"WORKING_DIR_GCS_URI: {WORKING_DIR_GCS_URI}\")\n",
    "    \n",
    "    LOGS_DIR = f'{WORKING_DIR_GCS_URI}/tb_logs'\n",
    "    if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "        LOGS_DIR=os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "    logging.info(f'TensorBoard LOGS_DIR: {LOGS_DIR}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # log variables\n",
    "    # ====================================================\n",
    "    logging.info(f'TIMESTAMP: {TIMESTAMP}')\n",
    "    logging.info(f'EXPERIMENT_NAME: {args.experiment_name}')\n",
    "    logging.info(f'RUN_NAME: {args.experiment_run}')\n",
    "    logging.info(f'EXPERIMENT_RUN_THIS: {EXPERIMENT_RUN_THIS}')\n",
    "    logging.info(f'NUM_EPOCHS: {args.num_epochs}')\n",
    "    logging.info(f'TB_RESOURCE_NAME tb_name: {args.tb_name}')\n",
    "    logging.info(f'distribute: {args.distribute}')\n",
    "    logging.info(f'train_output_bucket: {args.train_output_bucket}')\n",
    "    logging.info(f'workflow_dir: {args.workflow_dir}')\n",
    "    logging.info(f'train_dir: {args.train_dir}')\n",
    "    logging.info(f'valid_dir: {args.valid_dir}')\n",
    "    logging.info(f'num_epochs: {args.num_epochs}')\n",
    "    logging.info(f'per_gpu_batch_size: {args.per_gpu_batch_size}')\n",
    "    logging.info(f'layer_sizes: {args.layer_sizes}')\n",
    "    logging.info(f'learning_rate: {args.learning_rate}')\n",
    "    logging.info(f'project: {args.project}')\n",
    "    logging.info(f'location: {args.location}')\n",
    "    logging.info(f'valid_frequency: {args.valid_frequency}')\n",
    "    logging.info(f'epoch_steps: {args.epoch_steps}')\n",
    "    logging.info(f'valid_steps: {args.valid_steps}')\n",
    "    logging.info(f'chkpt_freq: {args.chkpt_freq}')\n",
    "    logging.info(f'profiler: {args.profiler}')\n",
    "    logging.info(f'write_embeddings: {args.write_embeddings}')\n",
    "    \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Init Clients\n",
    "    # ====================================================\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "                        \n",
    "    storage_client = storage.Client(project=f'{args.project}')\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=f'{args.project}',\n",
    "        location=f'{args.location}',\n",
    "        experiment=f'{args.experiment_name}',\n",
    "    )\n",
    "    \n",
    "    logging.info(\"vertex_ai initialized...\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU Strategy\n",
    "    # ====================================================    \n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.test.is_gpu_available(): # TODO: replace with - tf.config.list_physical_devices('GPU')\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "        \n",
    "    \n",
    "    # set related vars...\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = NUM_WORKERS * args.per_gpu_batch_size\n",
    "    logging.info(f'NUM_WORKERS = {NUM_WORKERS}')\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE: {GLOBAL_BATCH_SIZE}')\n",
    "    \n",
    "    # set worker vars...\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "        \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'Loading workflow & schema from : {args.workflow_dir}')\n",
    "    \n",
    "    workflow = nvt.Workflow.load(args.workflow_dir)\n",
    "    schema = workflow.output_schema\n",
    "    \n",
    "    train_data = MerlinDataset(os.path.join(args.train_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    valid_data = MerlinDataset(os.path.join(args.valid_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Callbacks\n",
    "    # ====================================================            \n",
    "    checkpoint_dir=os.environ['AIP_CHECKPOINT_DIR']\n",
    "    logging.info(f'Saving model checkpoints to {checkpoint_dir}')\n",
    "    \n",
    "    # model checkpoints - ModelCheckpoint | BackupAndRestore\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + \"/cp-{epoch:03d}-loss={loss:.2f}.ckpt\", # cp-{epoch:04d}.ckpt\" cp-{epoch:04d}.ckpt\"\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='total_loss',\n",
    "        mode='min',\n",
    "        save_freq=args.chkpt_freq,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    if args.profiler:\n",
    "        #TODO\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOGS_DIR,\n",
    "            # histogram_freq=args.hist_frequency, \n",
    "            write_graph=True,\n",
    "            # embeddings_freq=args.embed_frequency,\n",
    "            profile_batch=(25, 30),\n",
    "            update_freq='epoch',     # TODO: JT updated\n",
    "        )\n",
    "        logging.info(f'Tensorboard callback should profile batches...')\n",
    "        \n",
    "    else:\n",
    "        # TODO\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOGS_DIR,\n",
    "            # histogram_freq=args.hist_frequency, \n",
    "            write_graph=True,\n",
    "            # embeddings_freq=args.embed_frequency,\n",
    "        )\n",
    "        logging.info(f'Tensorboard callback NOT profiling batches...')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Train\n",
    "    # ==================================================== \n",
    "    \n",
    "    # Initialize profiler\n",
    "    logging.info('Initializing profiler ...')\n",
    "    \n",
    "    try:\n",
    "        cloud_profiler.init()\n",
    "    except:\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "        print(\"*** Unexpected:\", ex_type.__name__, ex_value)\n",
    "        traceback.print_tb(ex_traceback, limit=10, file=sys.stdout)\n",
    "        \n",
    "    logging.info('The profiler initiated...')\n",
    "\n",
    "    # with strategy.scope():\n",
    "        # here\n",
    "    model = create_two_tower(\n",
    "        train_dir=args.train_dir,\n",
    "        valid_dir=args.valid_dir,\n",
    "        workflow_dir=args.workflow_dir,\n",
    "        layer_sizes=LAYER_SIZES # args.layer_sizes,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "        run_eagerly=False,\n",
    "        metrics=[\n",
    "            mm.RecallAt(10), \n",
    "            mm.RecallAt(20), \n",
    "            mm.NDCGAt(10)\n",
    "        ],\n",
    "    )\n",
    "    logging.info('model compiled...')\n",
    "    \n",
    "    # cloud_profiler.init() # managed TB profiler\n",
    "        \n",
    "    logging.info('Starting training loop...')\n",
    "    \n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=valid_data,\n",
    "        validation_freq=args.valid_frequency,\n",
    "        batch_size=GLOBAL_BATCH_SIZE, \n",
    "        epochs=args.num_epochs,\n",
    "        steps_per_epoch=args.epoch_steps,\n",
    "        validation_steps=args.valid_steps, # 100,\n",
    "        callbacks=[\n",
    "            tensorboard_callback, \n",
    "            # UploadTBLogsBatchEnd(),\n",
    "            model_checkpoint_callback\n",
    "        ],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    \n",
    "    total_train_time = int((end_model_fit - start_model_fit) / 60)\n",
    "    logging.info(f'Elapsed total_train_time: {total_train_time}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams & metrics for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging params & metrics for Vertex Experiments')\n",
    "    \n",
    "    # get the metrics for the experiment run\n",
    "    history_keys = model.history.history.keys()\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    _ = [metrics_dict.update({key: model.history.history[key][-1]}) for key in history_keys]\n",
    "    metrics_dict[\"total_train_time\"] = total_train_time \n",
    "    \n",
    "    logging.info(f'metrics_dict: {metrics_dict}')\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{args.experiment_name}'\n",
    "    metaparams[\"experiment_run\"] = f\"{args.experiment_run}\"\n",
    "    logging.info(f'metaparams: {metaparams}')\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"num_gpus\"] = NUM_WORKERS # num_gpus\n",
    "    hyperparams[\"per_gpu_batch_size\"] = args.per_gpu_batch_size\n",
    "    hyperparams[\"global_batch_size\"] = GLOBAL_BATCH_SIZE\n",
    "    hyperparams[\"learning_rate\"] = args.learning_rate\n",
    "    hyperparams['layers'] = f'{args.layer_sizes}'\n",
    "    logging.info(f'hyperparams: {hyperparams}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Experiments\n",
    "    # ====================================================\n",
    "    logging.info(f\"Creating run: {EXPERIMENT_RUN_THIS}; for experiment: {args.experiment_name}\")\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "        logging.info(f\" logging data to experiment run: {EXPERIMENT_RUN_THIS}\")\n",
    "    \n",
    "        # Create experiment\n",
    "        vertex_ai.init(experiment=args.experiment_name)\n",
    "\n",
    "        with vertex_ai.start_run(args.experiment_run) as my_run:\n",
    "            logging.info(f\"logging metrics_dict\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams\")\n",
    "            my_run.log_params(metaparams)\n",
    "\n",
    "            logging.info(f\"logging hyperparams\")\n",
    "            my_run.log_params(hyperparams)\n",
    "            \n",
    "            vertex_ai.end_run()\n",
    "            logging.info(f\"experiment run: {EXPERIMENT_RUN_THIS} has ended\")\n",
    "        \n",
    "    # =============================================\n",
    "    # save retrieval (query) tower\n",
    "    # =============================================\n",
    "    QUERY_TOWER_LOCAL_DIR = 'query_tower'\n",
    "    CANDIDATE_TOWER_LOCAL_DIR = 'candidate_tower'\n",
    "    # set vars...\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model_dir\"\n",
    "    logging.info(f'Saving towers to {MODEL_DIR}')\n",
    "    \n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query_tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate_tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate_embeddings\"\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        \n",
    "        # save query tower\n",
    "        query_tower = model.query_encoder\n",
    "        query_tower.save(f'{QUERY_TOWER_LOCAL_DIR}/')\n",
    "        logging.info(f'Saved query tower locally to {QUERY_TOWER_LOCAL_DIR}')\n",
    "        upload_from_directory(f'./{QUERY_TOWER_LOCAL_DIR}', args.train_output_bucket, f'{args.experiment_name}/{args.experiment_run}/model_dir', f'{args.project}')\n",
    "        logging.info(f'Saved query tower to {QUERY_TOWER_PATH}')\n",
    "        \n",
    "        candidate_tower = model.candidate_encoder\n",
    "        candidate_tower.save(f'{CANDIDATE_TOWER_LOCAL_DIR}')\n",
    "        logging.info(f'Saved candidate tower locally to {CANDIDATE_TOWER_LOCAL_DIR}')\n",
    "        upload_from_directory(f'./{CANDIDATE_TOWER_LOCAL_DIR}', args.train_output_bucket, f'{args.experiment_name}/{args.experiment_run}/model_dir', f'{args.project}')\n",
    "        logging.info(f'Saved candidate tower to {CANDIDATE_TOWER_PATH}')\n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save embeddings\n",
    "    # ====================================================\n",
    "    \n",
    "    if args.write_embeddings:\n",
    "        # TODO: \n",
    "        logging.info('Saving candidate embeddings...')\n",
    "        EMBEDDINGS_FILE_NAME = \"candidate_embeddings.json\"\n",
    "        logging.info(f\"Saving {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "        # helper function\n",
    "        def format_for_matching_engine(data) -> None:\n",
    "            cols = [str(i) for i in range(LAYER_SIZES[-1])]      # ensure we are only pulling 0-EMBEDDING_DIM cols\n",
    "            emb = [data[col] for col in cols]                    # get the embeddings\n",
    "            formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "            with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "                f.write(formatted_emb)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        item_data = pd.read_parquet(f'{args.workflow_dir}/categories/unique.track_uri_can.parquet')\n",
    "        lookup_dict = dict(item_data['track_uri_can'])\n",
    "\n",
    "        # item embeds from TRAIN\n",
    "        start_embeds = time.time()\n",
    "\n",
    "        item_features = (\n",
    "            unique_rows_by_features(train_data, Tags.ITEM, Tags.ID)\n",
    "        )\n",
    "        item_embs = model.candidate_embeddings(\n",
    "            item_features, \n",
    "            index=item_features.schema['track_uri_can'], \n",
    "            batch_size=10000\n",
    "        )\n",
    "        item_emb_pd = item_embs.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "        item_emb_pd['track_uri_can'] = item_emb_pd['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "        _ = item_emb_pd.apply(format_for_matching_engine, axis=1)\n",
    "\n",
    "        # capture elapsed time\n",
    "        end_embeds = time.time()\n",
    "        elapsed_time = end_embeds - start_embeds\n",
    "        elapsed_time = round(elapsed_time, 2)\n",
    "        logging.info(f'Elapsed time writting TRAIN embeddings: {elapsed_time} seconds')\n",
    "\n",
    "        # item embeds from VALID\n",
    "        start_embeds = time.time()\n",
    "\n",
    "        item_features_val = (\n",
    "            unique_rows_by_features(valid_data, Tags.ITEM, Tags.ID)\n",
    "        )\n",
    "        item_embs_val = model.candidate_embeddings(\n",
    "            item_features_val, \n",
    "            index=item_features_val.schema['track_uri_can'], \n",
    "            batch_size=10000\n",
    "        )\n",
    "        item_emb_pd_val = item_embs_val.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "        item_emb_pd_val['track_uri_can'] = item_emb_pd_val['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "        _ = item_emb_pd_val.apply(format_for_matching_engine, axis=1)\n",
    "\n",
    "        # capture elapsed time\n",
    "        end_embeds = time.time()\n",
    "        elapsed_time = end_embeds - start_embeds\n",
    "        elapsed_time = round(elapsed_time, 2)\n",
    "        logging.info(f'Elapsed time writting VALID embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "        if task_type == 'chief':\n",
    "            _upload_blob_gcs(\n",
    "                EMBEDDINGS_PATH, \n",
    "                f\"{EMBEDDINGS_FILE_NAME}\", \n",
    "                f\"{EMBEDDINGS_FILE_NAME}\",\n",
    "                args.project\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Saved {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "            \n",
    "    else:\n",
    "        logging.info(f\"Did not write embeddings JSON...\")\n",
    "    \n",
    "    logging.info('All done - model saved') #all done\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting training')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcc35c-aff2-4503-aa06-7efaa2332b8f",
   "metadata": {},
   "source": [
    "### train requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f449cb-78f2-4e8b-a056-eb139469ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/requirements.txt\n",
    "merlin-models \n",
    "gcsfs \n",
    "google-cloud-aiplatform>=1.23.0 \n",
    "fastapi\n",
    "tensorboard-plugin-profile==2.11.1\n",
    "google-cloud-aiplatform[cloud_profiler]>=1.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158e674-6b0b-47ac-ac12-cce0b002bf93",
   "metadata": {},
   "source": [
    "## Training Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa81f3-0912-4f00-ba09-9bb9c40991eb",
   "metadata": {},
   "source": [
    "### versioned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad9482d-5713-43bc-81db-d269ac4283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker definitions for training\n",
    "MERLIN_VERSION = '2212v16'\n",
    "IMAGE_NAME = f'train-{MERLIN_VERSION}-{MODEL_DISPLAY_NAME}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = f'train'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212d9ee-4b1b-4092-8094-b6f0b88c42c6",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "```\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0848b1d2-b50a-4040-b0ce-f595750fb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.train\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer/* trainer/ \n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install merlin-models gcsfs google-cloud-aiplatform fastapi\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "RUN apt update && apt -y install nvtop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5134-d7aa-4f6c-882c-5f11d35cbfc2",
   "metadata": {},
   "source": [
    "# Build Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "838d6c3c-4170-4ca2-b0fa-e24c3f4be538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc772e5-906b-44cb-b753-587b2573b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b85024e1-c7f1-4dbf-8ec3-ef4f1590f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/archive/*\n",
    "/imgs/*\n",
    "/mm_src/*\n",
    "/src/serving/*\n",
    "/src/train_pipes/*\n",
    "/src/process_pipes/*\n",
    "/src/preprocessor/*\n",
    "/test_app/*\n",
    "/local_workflow/\n",
    "README.md\n",
    "*.pkl\n",
    "*.png\n",
    "*.ipynb\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "*cpython-37.pyc\n",
    "pip_freeze.txt\n",
    "custom_container_pipeline_spec.json\n",
    "# *.json\n",
    "Dockerfile.triton-cpr\n",
    "Dockerfile.merlin-retriever\n",
    "Dockerfile.merlintf-22_12_v4\n",
    "Dockerfile.nvt\n",
    "Dockerfile.nvt-133\n",
    "Dockerfile\n",
    "src/Dockerfile.mm-query-serve\n",
    "nvt-parquet-full-1a100.json\n",
    "nvt-parquet-latest-12.json\n",
    "nvt-parquet-full-4t4.json\n",
    "nvt-parquet-full-2a100.json\n",
    "custom_pipeline_spec.json\n",
    "spotipy_secret_creds.py\n",
    "sp_utils.py\n",
    ".gitignore\n",
    ".cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dddd6d1d-81ee-452d-8849-092b2ca79f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/train_utils.py\n",
      "src/cloudbuild.yaml\n",
      "src/Dockerfile.train\n",
      "src/trainer/train_task.py\n",
      "src/trainer/interactive_train.py\n",
      "src/trainer/requirements.txt\n",
      "src/trainer/two_tower_model.py\n",
      "src/trainer/__init__.py\n",
      "src/trainer/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "!gcloud meta list-files-for-upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54394f2-7c65-40ea-9102-ccf0cd3178a5",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "817dc303-b3e6-4427-bfd0-5cd949183a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68e469cf-13ae-47e6-93bb-30aef60ebe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=train\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34\n",
      "export FILE_LOCATION=./src\n",
      "export MACHINE_TYPE=e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('/home/jupyter/jt-merlin/merlin-on-vertex')\n",
    "# os.getcwd()\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e640822-e85b-4ba2-a192-9c8d790ff2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1473cd1-af0d-4c63-aff0-d1286f096170",
   "metadata": {},
   "source": [
    "# Vertex Train Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cdca6-d053-401d-8f95-5f24d3192d7a",
   "metadata": {},
   "source": [
    "### Prepare `worker_pool_specs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6d7e-959a-4354-85d5-36f7a9a662ed",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81d7050b-e6a8-4314-b3df-27e7abf0d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ====================================================\n",
    "# Single | Single machine, single GPU\n",
    "# ====================================================\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158e910-f435-40fe-8150-6529191cab5d",
   "metadata": {},
   "source": [
    "## Train Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0383-d688-4fb9-b252-152a4639572e",
   "metadata": {},
   "source": [
    "### Previously defined Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3b9fcc0-f78a-4c43-9f71-aade96b55d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "VERSION: jtv34\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34\n",
      "MODEL_NAME: 2tower\n",
      "FRAMEWORK: merlin-tf\n",
      "MODEL_DISPLAY_NAME: vertex-merlin-tf-2tower-jtv34\n",
      "WORKSPACE: gs://jt-merlin-scaling/vertex-merlin-tf-2tower-jtv34\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT_ID}\")\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74892c-4a1d-4faf-9d10-d724af181704",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21d5946-d679-4802-9e16-e45796defb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: mm-vertex-tf-2tower-jtv34\n",
      "RUN_NAME_PREFIX: run-20230321-095142\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXPERIMENT_PREFIX = 'mm-vertex'\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-tf-{MODEL_NAME}-{VERSION}'\n",
    "RUN_NAME_PREFIX = f'run-{TIMESTAMP}' # timestamp assigned during job\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME_PREFIX: {RUN_NAME_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c242ad-e3df-4ea3-9092-bd6a606fa6d4",
   "metadata": {},
   "source": [
    "### Data dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "487df262-9e3d-4dd9-b7c5-94ca8da2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train\n",
      "VALID_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid\n",
      "WORKFLOW_DIR: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow\n"
     ]
    }
   ],
   "source": [
    "# data and schema from nvtabular pipes\n",
    "DATA_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed'\n",
    "TRAIN_DATA = f'{DATA_DIR}/train'\n",
    "VALID_DATA = f'{DATA_DIR}/valid' \n",
    "\n",
    "# WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed'\n",
    "# WORKFLOW_DIR = 'gs://spotify-beam-v3/merlin-processed/workflow/2t-spotify-workflow'\n",
    "WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow'\n",
    "\n",
    "print(f\"TRAIN_DATA: {TRAIN_DATA}\")\n",
    "print(f\"VALID_DATA: {VALID_DATA}\")\n",
    "print(f\"WORKFLOW_DIR: {WORKFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded705c-b752-4459-a8a3-4cab97f94360",
   "metadata": {},
   "source": [
    "### Managed TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a76672e0-ca2f-4c2c-9028-9d388bfa2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/387846129628217344\n",
      "TB display name: mm-vertex-tf-2tower-jtv34-v1\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Managed Tensorboard\n",
    "# ====================================================\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-v1\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=LOCATION)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58659-7506-4275-b26d-f37bfd3c9c79",
   "metadata": {},
   "source": [
    "### Worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "811a40ec-4829-4013-8820-2ffbfb5de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'command': ['sh',\n",
      "                                 '-euc',\n",
      "                                 'pip freeze && python -m '\n",
      "                                 'trainer.train_task     '\n",
      "                                 '--per_gpu_batch_size=16384     '\n",
      "                                 '--train_output_bucket=jt-merlin-scaling     '\n",
      "                                 '--train_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train     '\n",
      "                                 '--valid_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid     '\n",
      "                                 '--workflow_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow     '\n",
      "                                 '--num_epochs=4     --learning_rate=0.001     '\n",
      "                                 '--distribute=single     '\n",
      "                                 '--experiment_name=mm-vertex-tf-2tower-jtv34     '\n",
      "                                 '--experiment_run=run-20230321-095142     '\n",
      "                                 '--project=hybrid-vertex     '\n",
      "                                 '--location=us-central1     '\n",
      "                                 '--valid_frequency=20     '\n",
      "                                 '--epoch_steps=500     --valid_steps=5     '\n",
      "                                 \"--layer_sizes='[512, 256, 128]'     \"\n",
      "                                 '--chkpt_freq=epoch     '\n",
      "                                 '--write_embeddings     --profiler'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from utils import train_utils\n",
    "\n",
    "# gcs bucket\n",
    "OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "\n",
    "# data size\n",
    "train_sample_cnt = 8_205_265 # 8_205_265\n",
    "valid_samples_cnt = 82_959\n",
    "\n",
    "# train config\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 4096*4 \n",
    "LEARNING_RATE = 0.001\n",
    "VALID_FREQUENCY = 20\n",
    "VALID_STEPS = valid_samples_cnt // BATCH_SIZE\n",
    "EPOCH_STEPS = train_sample_cnt // BATCH_SIZE\n",
    "CHECKPOINT_FREQ='epoch'\n",
    "\n",
    "# model\n",
    "LAYERS = \"[512, 256, 128]\"\n",
    "\n",
    "    \n",
    "WORKER_CMD = [\n",
    "    'sh',\n",
    "    '-euc',\n",
    "    f\"\"\"pip freeze && python -m trainer.train_task \\\n",
    "    --per_gpu_batch_size={BATCH_SIZE} \\\n",
    "    --train_output_bucket={OUTPUT_BUCKET} \\\n",
    "    --train_dir={TRAIN_DATA} \\\n",
    "    --valid_dir={VALID_DATA} \\\n",
    "    --workflow_dir={WORKFLOW_DIR} \\\n",
    "    --num_epochs={NUM_EPOCHS} \\\n",
    "    --learning_rate={LEARNING_RATE} \\\n",
    "    --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "    --experiment_name={EXPERIMENT_NAME} \\\n",
    "    --experiment_run={RUN_NAME_PREFIX} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --location={LOCATION} \\\n",
    "    --valid_frequency={VALID_FREQUENCY} \\\n",
    "    --epoch_steps={EPOCH_STEPS} \\\n",
    "    --valid_steps={VALID_STEPS} \\\n",
    "    --layer_sizes=\\'{LAYERS}\\' \\\n",
    "    --chkpt_freq={CHECKPOINT_FREQ} \\\n",
    "    --write_embeddings \\\n",
    "    --profiler\"\"\"\n",
    "    # --write_embeddings\n",
    "    # '''\n",
    "    # --tb_name={TB_RESOURCE_NAME} \\\n",
    "]\n",
    "\n",
    "# ====================================================\n",
    "# Worker pool specs\n",
    "# ====================================================\n",
    "    \n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    # args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)\n",
    "# jt-merlin-scaling/nvt-last5-latest-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a990b1e-107e-428c-9591-cddd62c00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKER_POOL_SPECS[0]['container_spec']['command']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f5b0-22ab-451d-ba49-09cc445d053f",
   "metadata": {},
   "source": [
    "## Submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bddd12da-d6ef-4ffe-917f-408b7bade0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_OUTPUT_DIR : gs://jt-merlin-scaling/mm-vertex-tf-2tower-jtv34/run-20230321-095142\n",
      "JOB_NAME : train-vertex-merlin-tf-2tower-jtv34\n",
      "\n",
      "gpu_type : nvidia_tesla_a100\n",
      "gpu_per_replica : 1\n",
      "replica_cnt : 1\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME_PREFIX}'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    # experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "JOB_NAME = f'train-{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# labels for train job\n",
    "gpu_type = ACCELERATOR_TYPE.lower()\n",
    "gpu_per_replica = PER_MACHINE_ACCELERATOR_COUNT\n",
    "replica_cnt = REPLICA_COUNT\n",
    "\n",
    "print(f'BASE_OUTPUT_DIR : {BASE_OUTPUT_DIR}')\n",
    "print(f'JOB_NAME : {JOB_NAME}\\n')\n",
    "print(f'gpu_type : {gpu_type}')\n",
    "print(f'gpu_per_replica : {gpu_per_replica}')\n",
    "print(f'replica_cnt : {replica_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "041217fa-0cf5-477a-b1a8-e5cfb58712bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    base_output_dir=BASE_OUTPUT_DIR,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    labels={\n",
    "        # 'mm_image' : 'nightly',\n",
    "        'gpu' : f'{gpu_type}',\n",
    "        'gpu_per_replica' : f'{gpu_per_replica}',\n",
    "        'replica_cnt' : f'{replica_cnt}',\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b978-1a64-40e8-b8c4-149a7a40bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: 'gs:/jt-merlin-scaling/latest-2tower-merlin-tf-jtv16/run-20230223-221213/model-dir/query-tower/.merlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fe04d7a8-2ba9-42ec-95ea-002adedbb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TB_LOGS_PATH = f'{BASE_OUTPUT_DIR}/logs' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5629edd9-b8ff-4246-bf2c-ce3820b74438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20e0b96c-65aa-424a-805d-4fe512865fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-26c38028c2f19b4e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-26c38028c2f19b4e\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$TB_LOGS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcf9e5-5bbc-43f5-ae2f-209ef9540395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
