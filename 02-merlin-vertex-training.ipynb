{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7483ac1f-bb86-4b51-a296-92f545905cd4",
   "metadata": {},
   "source": [
    "# Train Merlin Two-Towers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d13046-5cb5-43c7-a6ee-b42a3ee9a1e9",
   "metadata": {},
   "source": [
    "### pip & package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867b302d-ff4e-4ae4-ba6a-18b717f6d6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import nvtabular as nvt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "# disable INFO and DEBUG logging everywhere\n",
    "import logging\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# from nvtabular.ops import (\n",
    "#     Categorify,\n",
    "#     TagAsUserID,\n",
    "#     TagAsItemID,\n",
    "#     TagAsItemFeatures,\n",
    "#     TagAsUserFeatures,\n",
    "#     AddMetadata,\n",
    "#     ListSlice\n",
    "# )\n",
    "# import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.schema.tags import Tags\n",
    "\n",
    "# import merlin.models.tf as mm\n",
    "# from merlin.io.dataset import Dataset\n",
    "# from merlin.io.dataset import Dataset as MerlinDataset\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "# import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "# from google.cloud.aiplatform import hyperparameter_tuning as hpt\n",
    "\n",
    "# for running this example on CPU, comment out the line below\n",
    "# os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9140a-8529-489e-9885-8431960e129a",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48f540a-ccd4-4e15-96a8-25f4812f1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID: hybrid-vertex\n",
      "PROJECT_NUM: 934903580331\n",
      "LOCATION: us-central1\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECTS = !gcloud config get-value project\n",
    "PROJECT_ID = GCP_PROJECTS[0]\n",
    "PROJECT_NUM = !gcloud projects list --filter=\"$PROJECT_ID\" --format=\"value(PROJECT_NUMBER)\"\n",
    "PROJECT_NUM = PROJECT_NUM[0]\n",
    "LOCATION = 'us-central1'\n",
    "\n",
    "print(f\"PROJECT_ID: {PROJECT_ID}\")\n",
    "print(f\"PROJECT_NUM: {PROJECT_NUM}\")\n",
    "print(f\"LOCATION: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "495ea222-2f9e-4dc5-8cfa-23d1470a15c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Service Account address\n",
    "VERTEX_SA = '934903580331-compute@developer.gserviceaccount.com' # Change to your service account with Vertex AI Admin permitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc441d2e-b156-4d03-9951-ff943b129fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket definitions\n",
    "BUCKET = 'jt-merlin-scaling' # 'spotify-merlin-v1'\n",
    "\n",
    "VERSION = 'jtv34'\n",
    "MODEL_NAME = '2tower'\n",
    "FRAMEWORK = 'merlin-tf'\n",
    "MODEL_DISPLAY_NAME = f'vertex-{FRAMEWORK}-{MODEL_NAME}-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# # Docker definitions for training\n",
    "# IMAGE_NAME = f'{FRAMEWORK}-{MODEL_NAME}-training-{VERSION}'\n",
    "# IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "# # DOCKERNAME = 'hugectr'\n",
    "# DOCKERNAME = 'merlintf'\n",
    "# MACHINE_TYPE ='e2-highcpu-32'\n",
    "# FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49f034-6107-426c-9ca5-78bc8e4bed22",
   "metadata": {},
   "source": [
    "# Training Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2dcb695-6e50-4efd-9715-0eb02e1dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_DOCKER_PATH_PREFIX = 'src'\n",
    "TRAIN_SUB_DIR = 'trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cfb1e6-fef6-42b7-ba21-b730ade84cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the training subfolder\n",
    "! rm -rf {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}\n",
    "! mkdir {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f47e4461-5b2d-4082-bfd3-88072163d495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/__init__.py\n",
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db637aa-c331-4db0-a7da-5fcceed19fd6",
   "metadata": {},
   "source": [
    "## Interactive Train Shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a376724-9ed3-4a8b-9b6f-cc139ccc0e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/interactive_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/interactive_train.py\n",
    "\n",
    "import time\n",
    "\n",
    "while(True):\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141daa01-b5dc-4757-a92c-aad284dea45b",
   "metadata": {},
   "source": [
    "## Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d7ad57-89f5-4c68-9fa4-26644c2541d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/two_tower_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/two_tower_model.py\n",
    "\n",
    "from typing import List, Any\n",
    "\n",
    "import nvtabular as nvt\n",
    "# # import nvtabular.ops as ops\n",
    "\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def create_two_tower(\n",
    "    train_dir: str,\n",
    "    valid_dir: str,\n",
    "    workflow_dir: str,\n",
    "    layer_sizes: List[Any] = [512, 256, 128],\n",
    "):\n",
    "    \n",
    "    #=========================================\n",
    "    # get workflow details\n",
    "    #=========================================\n",
    "    workflow = nvt.Workflow.load(workflow_dir) # gs://spotify-merlin-v1/nvt-preprocessing-spotify-v24/nvt-analyzed\n",
    "    \n",
    "    schema = workflow.output_schema\n",
    "    # embeddings = ops.get_embedding_sizes(workflow)\n",
    "    \n",
    "    user_schema = schema.select_by_tag(Tags.USER)\n",
    "    user_inputs = mm.InputBlockV2(user_schema)\n",
    "    \n",
    "    #=========================================\n",
    "    # build towers\n",
    "    #=========================================\n",
    "    query = mm.Encoder(user_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    item_schema = schema.select_by_tag(Tags.ITEM)\n",
    "    item_inputs = mm.InputBlockV2(\n",
    "        item_schema,\n",
    "    )\n",
    "    candidate = mm.Encoder(item_inputs, mm.MLPBlock(layer_sizes))\n",
    "    \n",
    "    model = mm.RetrievalModelV2(\n",
    "        query=query,\n",
    "        candidate=candidate,\n",
    "        output=mm.ContrastiveOutput(\n",
    "            to_call=DotProduct(),\n",
    "            negative_samplers=\"in-batch\",\n",
    "            schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # model = mm.TwoTowerModelV2(\n",
    "    #     query_tower=query,\n",
    "    #     candidate_tower=candidate,\n",
    "    #     # output=mm.ContrastiveOutput(\n",
    "    #     #     to_call=DotProduct(),\n",
    "    #     #     negative_samplers=\"in-batch\",\n",
    "    #     #     schema=item_schema.select_by_tag(Tags.ITEM_ID),\n",
    "    #     #     candidate_name=\"item\",\n",
    "    #     # )\n",
    "    # )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafc15a-2eb7-4c19-94e2-a42484996f75",
   "metadata": {},
   "source": [
    "## Trainer utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83db1eb-8f3d-4d05-a63e-e90769dc6504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_utils.py\n",
    "\n",
    "# ====================================================\n",
    "# Helper functions - moved from train_task.py\n",
    "# ====================================================\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# GCS_CLIENT = storage.Client()\n",
    "\n",
    "def _is_chief(task_type, task_id): \n",
    "    ''' Check for primary if multiworker training\n",
    "    '''\n",
    "    if task_type == 'chief':\n",
    "        results = 'chief'\n",
    "    else:\n",
    "        results = None\n",
    "    return results\n",
    "\n",
    "def get_upload_logs_to_manged_tb_command(tb_resource_name, logs_dir, experiment_name, ttl_hrs, oneshot=\"false\"):\n",
    "    \"\"\"\n",
    "    Run this and copy/paste the command into terminal to have \n",
    "    upload the tensorboard logs from this machine to the managed tb instance\n",
    "    Note that the log dir is at the granularity of the run to help select the proper\n",
    "    timestamped run in Tensorboard\n",
    "    You can also run this in one-shot mode after training is done \n",
    "    to upload all tb objects at once\n",
    "    \"\"\"\n",
    "    return(\n",
    "        f\"\"\"tb-gcp-uploader --tensorboard_resource_name={tb_resource_name} \\\n",
    "        --logdir={logs_dir} \\\n",
    "        --experiment_name={experiment_name} \\\n",
    "        --one_shot={oneshot} \\\n",
    "        --event_file_inactive_secs={60*60*ttl_hrs}\"\"\"\n",
    "    )\n",
    "\n",
    "def _upload_blob_gcs(gcs_uri, source_file_name, destination_blob_name, project):\n",
    "    \"\"\"Uploads a file to GCS bucket\"\"\"\n",
    "    storage_client = storage.Client(project=project)\n",
    "    blob = Blob.from_string(os.path.join(gcs_uri, destination_blob_name))\n",
    "    blob.bucket._client = storage_client\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    \n",
    "def get_arch_from_string(arch_string):\n",
    "    q = arch_string.replace(']', '')\n",
    "    q = q.replace('[', '')\n",
    "    q = q.replace(\" \", \"\")\n",
    "    return [int(x) for x in q.split(',')]\n",
    "\n",
    "def upload_from_directory(\n",
    "    directory_path: str, \n",
    "    dest_bucket_name: str, \n",
    "    dest_blob_name: str,\n",
    "    project: str,\n",
    "):\n",
    "    storage_client = storage.Client(project=project)\n",
    "    rel_paths = glob.glob(directory_path + '/**', recursive=True)\n",
    "    bucket = storage_client.get_bucket(dest_bucket_name)\n",
    "    \n",
    "    for local_file in rel_paths:\n",
    "        remote_path = f'{dest_blob_name}/{\"/\".join(local_file.split(os.sep)[1:])}'\n",
    "        if os.path.isfile(local_file):\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4520ac09-65a6-494c-9b42-7b12a22d650f",
   "metadata": {},
   "source": [
    "## Train task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a431f3c9-7e9c-4399-a541-9b6eb514c8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/train_task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/train_task.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import string\n",
    "\n",
    "# we can control how much memory to give tensorflow with this environment variable\n",
    "# IMPORTANT: make sure you do this before you initialize TF's runtime, otherwise\n",
    "# TF will have claimed all free GPU memory\n",
    "# os.environ[\"TF_MEMORY_ALLOCATION\"] = \"0.3\"  # fraction of free memory\n",
    "\n",
    "# merlin\n",
    "# from merlin.models.utils.example_utils import workflow_fit_transform\n",
    "from merlin.io.dataset import Dataset as MerlinDataset\n",
    "from merlin.models.tf.outputs.base import DotProduct, MetricsFn, ModelOutput\n",
    "from merlin.schema.tags import Tags\n",
    "import merlin.models.tf as mm\n",
    "\n",
    "from merlin.models.utils.dataset import unique_rows_by_features\n",
    "\n",
    "# nvtabular\n",
    "import nvtabular as nvt\n",
    "import nvtabular.ops as ops\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# gcp\n",
    "import google.cloud.aiplatform as vertex_ai\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.bucket import Bucket\n",
    "from google.cloud.storage.blob import Blob\n",
    "# import hypertune\n",
    "import traceback\n",
    "from google.cloud.aiplatform.training_utils import cloud_profiler\n",
    "\n",
    "# repo\n",
    "from .two_tower_model import create_two_tower\n",
    "from .train_utils import (\n",
    "    get_upload_logs_to_manged_tb_command, \n",
    "    get_arch_from_string, \n",
    "    _upload_blob_gcs, \n",
    "    upload_from_directory\n",
    ")\n",
    "\n",
    "# local\n",
    "HYPERTUNE_METRIC_NAME = 'AUC'\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "\n",
    "# ====================================================\n",
    "# arg parser\n",
    "# ====================================================\n",
    "    \n",
    "def parse_args():\n",
    "    \"\"\"\n",
    "    Parses command line arguments\n",
    "    \n",
    "    type: int, float, str\n",
    "          bool() converts empty strings to `False` and non-empty strings to `True`\n",
    "          see more details here: https://docs.python.org/3/library/argparse.html#type\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--experiment_name',type=str,required=False,default='unnamed-experiment')\n",
    "    parser.add_argument('--experiment_run', type=str, required=False, default='unnamed_run')\n",
    "    parser.add_argument('--tb_name', type=str, required=False)\n",
    "    parser.add_argument('--distribute', type=str, required=False, default='single')\n",
    "    parser.add_argument('--train_output_bucket', type=str, required=True) # default='single',)\n",
    "    parser.add_argument('--workflow_dir', type=str, required=True)\n",
    "    parser.add_argument('--train_dir', type=str, required=True)\n",
    "    parser.add_argument('--valid_dir', type=str, required=True)\n",
    "    parser.add_argument('--num_epochs', type=int, required=True)\n",
    "    parser.add_argument('--per_gpu_batch_size', type=int, required=True)\n",
    "    parser.add_argument('--layer_sizes', type=str, required=False, default='[512, 256, 128]')\n",
    "    parser.add_argument('--learning_rate', type=float, required=False, default=.001)\n",
    "    parser.add_argument('--project', type=str, required=True)\n",
    "    parser.add_argument('--location', type=str, required=True)\n",
    "    parser.add_argument('--valid_frequency', type=int, required=False)\n",
    "    parser.add_argument('--epoch_steps', type=int, required=False)\n",
    "    parser.add_argument('--valid_steps', type=int, required=False)\n",
    "    parser.add_argument('--chkpt_freq', required=True) # type=int | TODO: value could be int or string\n",
    "    parser.add_argument(\"--profiler\", action='store_true', help=\"include for True; ommit for False\")\n",
    "    parser.add_argument(\"--write_embeddings\", action='store_true', help=\"include for True; ommit for False\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "                        \n",
    "# ====================================================\n",
    "# TRAINING SCRIPT\n",
    "# ====================================================\n",
    "    \n",
    "def main(args):\n",
    "    \"\"\"Runs a training loop.\"\"\"\n",
    "    \n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "    # tf.debugging.set_log_device_placement(True) # logs all tf ops and their device placement;\n",
    "    # os.environ['TF_GPU_THREAD_MODE']='gpu_private'\n",
    "    # os.environ['TF_GPU_THREAD_COUNT']='1'\n",
    "    os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "    TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # handle train job restarts for experiment runs (no duplicates)\n",
    "    # ====================================================\n",
    "    logging.info(f\"EXPERIMENT_NAME: {args.experiment_name}\\n RUN_NAME: {args.experiment_run}\")\n",
    "    \n",
    "    SESSION_id = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=3))\n",
    "    EXPERIMENT_RUN_THIS = f'{args.experiment_run}-{SESSION_id}'\n",
    "    \n",
    "    logging.info(f\"Changing: {args.experiment_run} to: {EXPERIMENT_RUN_THIS} to handle job restarts\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set directories\n",
    "    # ====================================================\n",
    "    # WORKING_DIR_GCS_URI = f'gs://{args.train_output_bucket}/{args.experiment_name}/{args.experiment_run}'\n",
    "    WORKING_DIR_GCS_URI = f'/gcs/{args.train_output_bucket}/{args.experiment_name}/{args.experiment_run}'\n",
    "    logging.info(f\"WORKING_DIR_GCS_URI: {WORKING_DIR_GCS_URI}\")\n",
    "    \n",
    "    LOGS_DIR = f'{WORKING_DIR_GCS_URI}/tb_logs'\n",
    "    if 'AIP_TENSORBOARD_LOG_DIR' in os.environ:\n",
    "        LOGS_DIR=os.environ['AIP_TENSORBOARD_LOG_DIR']\n",
    "        if LOGS_DIR[0:5] == 'gs://':\n",
    "            LOGS_DIR = LOGS_DIR.replace('gs://', '/gcs/')\n",
    "        logging.info(f'AIP_TENSORBOARD_LOG_DIR: {LOGS_DIR}')\n",
    "        \n",
    "    logging.info(f'TensorBoard LOGS_DIR: {LOGS_DIR}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # log variables\n",
    "    # ====================================================\n",
    "    logging.info(f'TIMESTAMP: {TIMESTAMP}')\n",
    "    logging.info(f'EXPERIMENT_NAME: {args.experiment_name}')\n",
    "    logging.info(f'RUN_NAME: {args.experiment_run}')\n",
    "    logging.info(f'EXPERIMENT_RUN_THIS: {EXPERIMENT_RUN_THIS}')\n",
    "    logging.info(f'NUM_EPOCHS: {args.num_epochs}')\n",
    "    logging.info(f'TB_RESOURCE_NAME tb_name: {args.tb_name}')\n",
    "    logging.info(f'distribute: {args.distribute}')\n",
    "    logging.info(f'train_output_bucket: {args.train_output_bucket}')\n",
    "    logging.info(f'workflow_dir: {args.workflow_dir}')\n",
    "    logging.info(f'train_dir: {args.train_dir}')\n",
    "    logging.info(f'valid_dir: {args.valid_dir}')\n",
    "    logging.info(f'num_epochs: {args.num_epochs}')\n",
    "    logging.info(f'per_gpu_batch_size: {args.per_gpu_batch_size}')\n",
    "    logging.info(f'layer_sizes: {args.layer_sizes}')\n",
    "    logging.info(f'learning_rate: {args.learning_rate}')\n",
    "    logging.info(f'project: {args.project}')\n",
    "    logging.info(f'location: {args.location}')\n",
    "    logging.info(f'valid_frequency: {args.valid_frequency}')\n",
    "    logging.info(f'epoch_steps: {args.epoch_steps}')\n",
    "    logging.info(f'valid_steps: {args.valid_steps}')\n",
    "    logging.info(f'chkpt_freq: {args.chkpt_freq}')\n",
    "    logging.info(f'profiler: {args.profiler}')\n",
    "    logging.info(f'write_embeddings: {args.write_embeddings}')\n",
    "    \n",
    "    LAYER_SIZES = get_arch_from_string(args.layer_sizes)\n",
    "    logging.info(f'LAYER_SIZES: {LAYER_SIZES}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Init Clients\n",
    "    # ====================================================\n",
    "    project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "                        \n",
    "    storage_client = storage.Client(project=f'{args.project}')\n",
    "    \n",
    "    vertex_ai.init(\n",
    "        project=f'{args.project}',\n",
    "        location=f'{args.location}',\n",
    "        experiment=f'{args.experiment_name}',\n",
    "    )\n",
    "    \n",
    "    logging.info(\"vertex_ai initialized...\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Set Device / GPU Strategy\n",
    "    # ====================================================    \n",
    "    logging.info(\"Detecting devices....\")\n",
    "    logging.info(f'Detected Devices {str(device_lib.list_local_devices())}')\n",
    "    \n",
    "    logging.info(\"Setting device strategy...\")\n",
    "    \n",
    "    # Single Machine, single compute device\n",
    "    if args.distribute == 'single':\n",
    "        if tf.test.is_gpu_available(): # TODO: replace with - tf.config.list_physical_devices('GPU')\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "        else:\n",
    "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "        logging.info(\"Single device training\")\n",
    "    \n",
    "    # Single Machine, multiple compute device\n",
    "    elif args.distribute == 'mirrored':\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        logging.info(\"Mirrored Strategy distributed training\")\n",
    "\n",
    "    # Multi Machine, multiple compute device\n",
    "    elif args.distribute == 'multiworker':\n",
    "        strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "        logging.info(\"Multi-worker Strategy distributed training\")\n",
    "        logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "        \n",
    "    \n",
    "    # set related vars...\n",
    "    NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "    GLOBAL_BATCH_SIZE = NUM_WORKERS * args.per_gpu_batch_size\n",
    "    logging.info(f'NUM_WORKERS = {NUM_WORKERS}')\n",
    "    logging.info(f'GLOBAL_BATCH_SIZE: {GLOBAL_BATCH_SIZE}')\n",
    "    \n",
    "    # set worker vars...\n",
    "    logging.info(f'Setting task_type and task_id...')\n",
    "    if args.distribute == 'multiworker':\n",
    "        task_type, task_id = (\n",
    "            strategy.cluster_resolver.task_type,\n",
    "            strategy.cluster_resolver.task_id\n",
    "        )\n",
    "    else:\n",
    "        task_type, task_id = 'chief', None\n",
    "    \n",
    "    logging.info(f'task_type = {task_type}')\n",
    "    logging.info(f'task_id = {task_id}')\n",
    "        \n",
    "    # ====================================================\n",
    "    # Prepare Train and Valid Data\n",
    "    # ====================================================\n",
    "    logging.info(f'Loading workflow & schema from : {args.workflow_dir}')\n",
    "    \n",
    "    workflow = nvt.Workflow.load(args.workflow_dir)\n",
    "    schema = workflow.output_schema\n",
    "    \n",
    "    train_data = MerlinDataset(os.path.join(args.train_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    valid_data = MerlinDataset(os.path.join(args.valid_dir, \"*.parquet\"), schema=schema, part_size=\"1GB\")\n",
    "    \n",
    "    # ====================================================\n",
    "    # Callbacks\n",
    "    # ====================================================            \n",
    "    checkpoint_dir=os.environ['AIP_CHECKPOINT_DIR']\n",
    "    logging.info(f'Saving model checkpoints to {checkpoint_dir}')\n",
    "    \n",
    "    # model checkpoints - ModelCheckpoint | BackupAndRestore\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_dir + \"/cp-{epoch:03d}-loss={loss:.2f}.ckpt\", # cp-{epoch:04d}.ckpt\" cp-{epoch:04d}.ckpt\"\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='total_loss',\n",
    "        mode='min',\n",
    "        save_freq=args.chkpt_freq,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    if args.profiler:\n",
    "        #TODO\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOGS_DIR,\n",
    "            # histogram_freq=args.hist_frequency, \n",
    "            write_graph=True,\n",
    "            # embeddings_freq=args.embed_frequency,\n",
    "            profile_batch=(25, 30),\n",
    "            update_freq='epoch',     # TODO: JT updated\n",
    "        )\n",
    "        logging.info(f'Tensorboard callback should profile batches...')\n",
    "        \n",
    "    else:\n",
    "        # TODO\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=LOGS_DIR,\n",
    "            # histogram_freq=args.hist_frequency, \n",
    "            write_graph=True,\n",
    "            # embeddings_freq=args.embed_frequency,\n",
    "        )\n",
    "        logging.info(f'Tensorboard callback NOT profiling batches...')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Train\n",
    "    # ==================================================== \n",
    "    \n",
    "    # Initialize profiler\n",
    "    logging.info('Initializing profiler ...')\n",
    "    \n",
    "    try:\n",
    "        cloud_profiler.init()\n",
    "    except:\n",
    "        ex_type, ex_value, ex_traceback = sys.exc_info()\n",
    "        print(\"*** Unexpected:\", ex_type.__name__, ex_value)\n",
    "        traceback.print_tb(ex_traceback, limit=10, file=sys.stdout)\n",
    "        \n",
    "    logging.info('The profiler initiated...')\n",
    "\n",
    "    # with strategy.scope():\n",
    "        # here\n",
    "    model = create_two_tower(\n",
    "        train_dir=args.train_dir,\n",
    "        valid_dir=args.valid_dir,\n",
    "        workflow_dir=args.workflow_dir,\n",
    "        layer_sizes=LAYER_SIZES # args.layer_sizes,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adagrad(args.learning_rate),\n",
    "        run_eagerly=False,\n",
    "        metrics=[\n",
    "            mm.RecallAt(1), \n",
    "            mm.RecallAt(10), \n",
    "            mm.NDCGAt(10)\n",
    "        ],\n",
    "    )\n",
    "    logging.info('model compiled...')\n",
    "    \n",
    "    # cloud_profiler.init() # managed TB profiler\n",
    "        \n",
    "    logging.info('Starting training loop...')\n",
    "    \n",
    "    start_model_fit = time.time()\n",
    "    \n",
    "    model.fit(\n",
    "        train_data, \n",
    "        validation_data=valid_data,\n",
    "        validation_freq=args.valid_frequency,\n",
    "        batch_size=GLOBAL_BATCH_SIZE, \n",
    "        epochs=args.num_epochs,\n",
    "        steps_per_epoch=args.epoch_steps,\n",
    "        validation_steps=args.valid_steps, # 100,\n",
    "        callbacks=[\n",
    "            tensorboard_callback, \n",
    "            # UploadTBLogsBatchEnd(),\n",
    "            model_checkpoint_callback\n",
    "        ],\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # capture elapsed time\n",
    "    end_model_fit = time.time()\n",
    "    \n",
    "    total_train_time = int((end_model_fit - start_model_fit) / 60)\n",
    "    logging.info(f'Elapsed total_train_time: {total_train_time}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # metaparams & metrics for Vertex Ai Experiments\n",
    "    # ====================================================\n",
    "    logging.info('Logging params & metrics for Vertex Experiments')\n",
    "    \n",
    "    # get the metrics for the experiment run\n",
    "    history_keys = model.history.history.keys()\n",
    "    \n",
    "    metrics_dict = {}\n",
    "    _ = [metrics_dict.update({key: model.history.history[key][-1]}) for key in history_keys]\n",
    "    metrics_dict[\"total_train_time\"] = total_train_time \n",
    "    \n",
    "    logging.info(f'metrics_dict: {metrics_dict}')\n",
    "    \n",
    "    metaparams = {}\n",
    "    metaparams[\"experiment_name\"] = f'{args.experiment_name}'\n",
    "    metaparams[\"experiment_run\"] = f\"{args.experiment_run}\"\n",
    "    logging.info(f'metaparams: {metaparams}')\n",
    "    \n",
    "    hyperparams = {}\n",
    "    hyperparams[\"epochs\"] = int(args.num_epochs)\n",
    "    hyperparams[\"num_gpus\"] = NUM_WORKERS # num_gpus\n",
    "    hyperparams[\"per_gpu_batch_size\"] = args.per_gpu_batch_size\n",
    "    hyperparams[\"global_batch_size\"] = GLOBAL_BATCH_SIZE\n",
    "    hyperparams[\"learning_rate\"] = args.learning_rate\n",
    "    hyperparams['layers'] = f'{args.layer_sizes}'\n",
    "    logging.info(f'hyperparams: {hyperparams}')\n",
    "    \n",
    "    # ====================================================\n",
    "    # Experiments\n",
    "    # ====================================================\n",
    "    logging.info(f\"Creating run: {EXPERIMENT_RUN_THIS}; for experiment: {args.experiment_name}\")\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        logging.info(f\" task_type logging experiments: {task_type}\")\n",
    "        logging.info(f\" task_id logging experiments: {task_id}\")\n",
    "        logging.info(f\" logging data to experiment run: {EXPERIMENT_RUN_THIS}\")\n",
    "    \n",
    "        # Create experiment\n",
    "        vertex_ai.init(experiment=args.experiment_name)\n",
    "\n",
    "        with vertex_ai.start_run(args.experiment_run) as my_run:\n",
    "            logging.info(f\"logging metrics_dict\")\n",
    "            my_run.log_metrics(metrics_dict)\n",
    "\n",
    "            logging.info(f\"logging metaparams\")\n",
    "            my_run.log_params(metaparams)\n",
    "\n",
    "            logging.info(f\"logging hyperparams\")\n",
    "            my_run.log_params(hyperparams)\n",
    "            \n",
    "            vertex_ai.end_run()\n",
    "            logging.info(f\"experiment run: {EXPERIMENT_RUN_THIS} has ended\")\n",
    "        \n",
    "    # =============================================\n",
    "    # save retrieval (query) tower\n",
    "    # =============================================\n",
    "    QUERY_TOWER_LOCAL_DIR = 'query_tower'\n",
    "    CANDIDATE_TOWER_LOCAL_DIR = 'candidate_tower'\n",
    "    # set vars...\n",
    "    MODEL_DIR = f\"{WORKING_DIR_GCS_URI}/model_dir\"\n",
    "    logging.info(f'Saving towers to {MODEL_DIR}')\n",
    "    \n",
    "    QUERY_TOWER_PATH = f\"{MODEL_DIR}/query_tower\"\n",
    "    CANDIDATE_TOWER_PATH = f\"{MODEL_DIR}/candidate_tower\"\n",
    "    EMBEDDINGS_PATH = f\"{MODEL_DIR}/candidate_embeddings\"\n",
    "    \n",
    "    if task_type == 'chief':\n",
    "        \n",
    "        # save query tower\n",
    "        query_tower = model.query_encoder\n",
    "        query_tower.save(f'{QUERY_TOWER_LOCAL_DIR}/')\n",
    "        logging.info(f'Saved query tower locally to {QUERY_TOWER_LOCAL_DIR}')\n",
    "        upload_from_directory(f'./{QUERY_TOWER_LOCAL_DIR}', args.train_output_bucket, f'{args.experiment_name}/{args.experiment_run}/model_dir', f'{args.project}')\n",
    "        logging.info(f'Saved query tower to {QUERY_TOWER_PATH}')\n",
    "        \n",
    "        candidate_tower = model.candidate_encoder\n",
    "        candidate_tower.save(f'{CANDIDATE_TOWER_LOCAL_DIR}')\n",
    "        logging.info(f'Saved candidate tower locally to {CANDIDATE_TOWER_LOCAL_DIR}')\n",
    "        upload_from_directory(f'./{CANDIDATE_TOWER_LOCAL_DIR}', args.train_output_bucket, f'{args.experiment_name}/{args.experiment_run}/model_dir', f'{args.project}')\n",
    "        logging.info(f'Saved candidate tower to {CANDIDATE_TOWER_PATH}')\n",
    "\n",
    "    \n",
    "    # ====================================================\n",
    "    # Save embeddings\n",
    "    # ====================================================\n",
    "    \n",
    "    if args.write_embeddings:\n",
    "        # TODO: \n",
    "        logging.info('Saving candidate embeddings...')\n",
    "        EMBEDDINGS_FILE_NAME = \"candidate_embeddings.json\"\n",
    "        logging.info(f\"Saving {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "        # helper function\n",
    "        def format_for_matching_engine(data) -> None:\n",
    "            cols = [str(i) for i in range(LAYER_SIZES[-1])]      # ensure we are only pulling 0-EMBEDDING_DIM cols\n",
    "            emb = [data[col] for col in cols]                    # get the embeddings\n",
    "            formatted_emb = '{\"id\":\"' + str(data['track_uri_can']) + '\",\"embedding\":[' + \",\".join(str(x) for x in list(emb)) + ']}'\n",
    "            with open(f\"{EMBEDDINGS_FILE_NAME}\", 'a') as f:\n",
    "                f.write(formatted_emb)\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        item_data = pd.read_parquet(f'{args.workflow_dir}/categories/unique.track_uri_can.parquet')\n",
    "        lookup_dict = dict(item_data['track_uri_can'])\n",
    "\n",
    "        # item embeds from TRAIN\n",
    "        start_embeds = time.time()\n",
    "\n",
    "        item_features = (\n",
    "            unique_rows_by_features(train_data, Tags.ITEM, Tags.ID)\n",
    "        )\n",
    "        item_embs = model.candidate_embeddings(\n",
    "            item_features, \n",
    "            index=item_features.schema['track_uri_can'], \n",
    "            batch_size=10000\n",
    "        )\n",
    "        item_emb_pd = item_embs.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "        item_emb_pd['track_uri_can'] = item_emb_pd['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "        _ = item_emb_pd.apply(format_for_matching_engine, axis=1)\n",
    "\n",
    "        # capture elapsed time\n",
    "        end_embeds = time.time()\n",
    "        elapsed_time = end_embeds - start_embeds\n",
    "        elapsed_time = round(elapsed_time, 2)\n",
    "        logging.info(f'Elapsed time writting TRAIN embeddings: {elapsed_time} seconds')\n",
    "\n",
    "        # item embeds from VALID\n",
    "        start_embeds = time.time()\n",
    "\n",
    "        item_features_val = (\n",
    "            unique_rows_by_features(valid_data, Tags.ITEM, Tags.ID)\n",
    "        )\n",
    "        item_embs_val = model.candidate_embeddings(\n",
    "            item_features_val, \n",
    "            index=item_features_val.schema['track_uri_can'], \n",
    "            batch_size=10000\n",
    "        )\n",
    "        item_emb_pd_val = item_embs_val.compute().to_pandas().fillna(1e-10).reset_index() #filling blanks with an epsilon value\n",
    "        item_emb_pd_val['track_uri_can'] = item_emb_pd_val['track_uri_can'].apply(lambda l: lookup_dict[l])\n",
    "        _ = item_emb_pd_val.apply(format_for_matching_engine, axis=1)\n",
    "\n",
    "        # capture elapsed time\n",
    "        end_embeds = time.time()\n",
    "        elapsed_time = end_embeds - start_embeds\n",
    "        elapsed_time = round(elapsed_time, 2)\n",
    "        logging.info(f'Elapsed time writting VALID embeddings: {elapsed_time} seconds')\n",
    "    \n",
    "        if task_type == 'chief':\n",
    "            _upload_blob_gcs(\n",
    "                EMBEDDINGS_PATH, \n",
    "                f\"{EMBEDDINGS_FILE_NAME}\", \n",
    "                f\"{EMBEDDINGS_FILE_NAME}\",\n",
    "                args.project\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Saved {EMBEDDINGS_FILE_NAME} to {EMBEDDINGS_PATH}\")\n",
    "            \n",
    "    else:\n",
    "        logging.info(f\"Did not write embeddings JSON...\")\n",
    "    \n",
    "    logging.info('All done - model saved') #all done\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s - %(message)s',\n",
    "        level=logging.INFO, \n",
    "        datefmt='%d-%m-%y %H:%M:%S',\n",
    "        stream=sys.stdout\n",
    "    )\n",
    "\n",
    "    parsed_args = parse_args()\n",
    "\n",
    "    logging.info('Args: %s', parsed_args)\n",
    "    start_time = time.time()\n",
    "    logging.info('Starting training')\n",
    "\n",
    "    main(parsed_args)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    logging.info('Training completed. Elapsed time: %s', elapsed_time )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcc35c-aff2-4503-aa06-7efaa2332b8f",
   "metadata": {},
   "source": [
    "### train requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f449cb-78f2-4e8b-a056-eb139469ae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/trainer/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/{TRAIN_SUB_DIR}/requirements.txt\n",
    "merlin-models \n",
    "gcsfs \n",
    "google-cloud-aiplatform>=1.23.0 \n",
    "fastapi\n",
    "tensorboard-plugin-profile==2.11.1\n",
    "google-cloud-aiplatform[cloud_profiler]>=1.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158e674-6b0b-47ac-ac12-cce0b002bf93",
   "metadata": {},
   "source": [
    "## Training Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5aa81f3-0912-4f00-ba09-9bb9c40991eb",
   "metadata": {},
   "source": [
    "### versioned image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ad9482d-5713-43bc-81db-d269ac4283a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker definitions for training\n",
    "MERLIN_VERSION = '2212v16'\n",
    "IMAGE_NAME = f'train-{MERLIN_VERSION}-{MODEL_DISPLAY_NAME}'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "\n",
    "DOCKERNAME = f'train'\n",
    "MACHINE_TYPE ='e2-highcpu-32'\n",
    "FILE_LOCATION = './src'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212d9ee-4b1b-4092-8094-b6f0b88c42c6",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "```\n",
    "RUN pip install google-cloud-bigquery gcsfs\n",
    "RUN pip install google-cloud-aiplatform[cloud_profiler] kfp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0848b1d2-b50a-4040-b0ce-f595750fb12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/Dockerfile.train\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/Dockerfile.{DOCKERNAME}\n",
    "\n",
    "FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
    "\n",
    "WORKDIR /src\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer/* trainer/ \n",
    "\n",
    "# RUN pip install -U pip\n",
    "# RUN pip install merlin-models gcsfs google-cloud-aiplatform fastapi\n",
    "RUN pip install -r trainer/requirements.txt\n",
    "\n",
    "RUN apt update && apt -y install nvtop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09e5134-d7aa-4f6c-882c-5f11d35cbfc2",
   "metadata": {},
   "source": [
    "# Build Train Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838d6c3c-4170-4ca2-b0fa-e24c3f4be538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/merlin-on-vertex-ORIGINAL/merlin-on-vertex\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbc772e5-906b-44cb-b753-587b2573b5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [gcloudignore/enabled].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set gcloudignore/enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b85024e1-c7f1-4dbf-8ec3-ef4f1590f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .gcloudignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .gcloudignore\n",
    ".gcloudignore\n",
    "/archive/*\n",
    "/imgs/*\n",
    "/mm_src/*\n",
    "/src/serving/*\n",
    "/src/train_pipes/*\n",
    "/src/process_pipes/*\n",
    "/src/preprocessor/*\n",
    "/app/*\n",
    "/local_workflow/\n",
    "README.md\n",
    "*.pkl\n",
    "*.png\n",
    "*.ipynb\n",
    ".git\n",
    ".github\n",
    ".ipynb_checkpoints/*\n",
    "*__pycache__\n",
    "*cpython-37.pyc\n",
    "pip_freeze.txt\n",
    "custom_container_pipeline_spec.json\n",
    "# *.json\n",
    "Dockerfile.triton-cpr\n",
    "Dockerfile.merlin-retriever\n",
    "Dockerfile.merlintf-22_12_v4\n",
    "Dockerfile.nvt\n",
    "Dockerfile.nvt-133\n",
    "Dockerfile\n",
    "src/Dockerfile.mm-query-serve\n",
    "nvt-parquet-full-1a100.json\n",
    "nvt-parquet-latest-12.json\n",
    "nvt-parquet-full-4t4.json\n",
    "nvt-parquet-full-2a100.json\n",
    "custom_pipeline_spec.json\n",
    "spotipy_secret_creds.py\n",
    "sp_utils.py\n",
    ".gitignore\n",
    ".cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dddd6d1d-81ee-452d-8849-092b2ca79f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils/train_utils.py\n",
      "src/cloudbuild.yaml\n",
      "src/Dockerfile.train\n",
      "src/trainer/train_task.py\n",
      "src/trainer/interactive_train.py\n",
      "src/trainer/requirements.txt\n",
      "src/trainer/two_tower_model.py\n",
      "src/trainer/__init__.py\n",
      "src/trainer/train_utils.py\n"
     ]
    }
   ],
   "source": [
    "!gcloud meta list-files-for-upload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54394f2-7c65-40ea-9102-ccf0cd3178a5",
   "metadata": {},
   "source": [
    "### `cloudbuild.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "817dc303-b3e6-4427-bfd0-5cd949183a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {REPO_DOCKER_PATH_PREFIX}/cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', '$_IMAGE_URI', '$_FILE_LOCATION', '-f', '$_FILE_LOCATION/Dockerfile.$_DOCKERNAME']\n",
    "images:\n",
    "- '$_IMAGE_URI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68e469cf-13ae-47e6-93bb-30aef60ebe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export DOCKERNAME=train\n",
      "export IMAGE_URI=gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34\n",
      "export FILE_LOCATION=./src\n",
      "export MACHINE_TYPE=e2-highcpu-32\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('/home/jupyter/jt-merlin/merlin-on-vertex')\n",
    "# os.getcwd()\n",
    "print(f\"export DOCKERNAME={DOCKERNAME}\")\n",
    "print(f\"export IMAGE_URI={IMAGE_URI}\")\n",
    "print(f\"export FILE_LOCATION={FILE_LOCATION}\")\n",
    "print(f\"export MACHINE_TYPE={MACHINE_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e640822-e85b-4ba2-a192-9c8d790ff2cb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 27.6 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://hybrid-vertex_cloudbuild/source/1679371751.278726-2ff2c36e11104e49aa61599ad2cc964f.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/hybrid-vertex/locations/global/builds/39d033a4-a70e-4e33-bcbb-62f1c0f93bff].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/39d033a4-a70e-4e33-bcbb-62f1c0f93bff?project=934903580331 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"39d033a4-a70e-4e33-bcbb-62f1c0f93bff\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://hybrid-vertex_cloudbuild/source/1679371751.278726-2ff2c36e11104e49aa61599ad2cc964f.tgz#1679371751641778\n",
      "Copying gs://hybrid-vertex_cloudbuild/source/1679371751.278726-2ff2c36e11104e49aa61599ad2cc964f.tgz#1679371751641778...\n",
      "/ [1 files][  8.9 KiB/  8.9 KiB]                                                \n",
      "Operation completed over 1 objects/8.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  37.38kB\n",
      "Step 1/5 : FROM nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      "22.12: Pulling from nvidia/merlin/merlin-tensorflow\n",
      "eaead16dc43b: Pulling fs layer\n",
      "d86e6ecee9ab: Pulling fs layer\n",
      "6b08e0981273: Pulling fs layer\n",
      "54de03fa67ca: Pulling fs layer\n",
      "d0067f8e4744: Pulling fs layer\n",
      "9e8473502f86: Pulling fs layer\n",
      "62cbcf4cbe13: Pulling fs layer\n",
      "9054d495b912: Pulling fs layer\n",
      "363a6cd12433: Pulling fs layer\n",
      "69633d945bde: Pulling fs layer\n",
      "030b5d54675a: Pulling fs layer\n",
      "282a25844090: Pulling fs layer\n",
      "51caa25c21ff: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "77bc21e94f24: Pulling fs layer\n",
      "784e1b6fd612: Pulling fs layer\n",
      "af1ee39433f4: Pulling fs layer\n",
      "52321cba559e: Pulling fs layer\n",
      "af00054fc370: Pulling fs layer\n",
      "0b6fc99b1680: Pulling fs layer\n",
      "6cee73498bf6: Pulling fs layer\n",
      "69633d945bde: Waiting\n",
      "8b4cfd996372: Pulling fs layer\n",
      "030b5d54675a: Waiting\n",
      "b090c9ed50ff: Pulling fs layer\n",
      "0a8adb3dcf22: Pulling fs layer\n",
      "282a25844090: Waiting\n",
      "6e88a7f6f15d: Pulling fs layer\n",
      "00f5ed83fb5b: Pulling fs layer\n",
      "e518f4c1fb4d: Pulling fs layer\n",
      "8b6afde98e21: Pulling fs layer\n",
      "dcdde75d70d4: Pulling fs layer\n",
      "c131c32a822c: Pulling fs layer\n",
      "7ef98a0bb031: Pulling fs layer\n",
      "7b2bf50cb482: Pulling fs layer\n",
      "54de03fa67ca: Waiting\n",
      "9e8473502f86: Waiting\n",
      "62cbcf4cbe13: Waiting\n",
      "7dd53e86e25e: Pulling fs layer\n",
      "228e876a170d: Pulling fs layer\n",
      "9054d495b912: Waiting\n",
      "363a6cd12433: Waiting\n",
      "2b8ce232b0f5: Pulling fs layer\n",
      "6acab9f9b6c9: Pulling fs layer\n",
      "49738298fc0b: Pulling fs layer\n",
      "450e691b8934: Pulling fs layer\n",
      "c11b1ed1ee46: Pulling fs layer\n",
      "67e193f6923c: Pulling fs layer\n",
      "51caa25c21ff: Waiting\n",
      "f3a3e4335302: Pulling fs layer\n",
      "4f4fb700ef54: Waiting\n",
      "77bc21e94f24: Waiting\n",
      "31514946c388: Pulling fs layer\n",
      "4045938dbcf0: Pulling fs layer\n",
      "ec3ab3614609: Pulling fs layer\n",
      "52321cba559e: Waiting\n",
      "d59ff5de4fe3: Pulling fs layer\n",
      "d0067f8e4744: Waiting\n",
      "c5e6195fcb5a: Pulling fs layer\n",
      "b64eb74ea0f6: Pulling fs layer\n",
      "fde2f9173aa1: Pulling fs layer\n",
      "05c48a632d89: Pulling fs layer\n",
      "45643bc35dd6: Pulling fs layer\n",
      "dac08d81643b: Pulling fs layer\n",
      "753e545517ec: Pulling fs layer\n",
      "59fa5f6703c7: Pulling fs layer\n",
      "5251fc0e145f: Pulling fs layer\n",
      "bdd807f81520: Pulling fs layer\n",
      "49738298fc0b: Waiting\n",
      "b090c9ed50ff: Waiting\n",
      "cda9dd232241: Pulling fs layer\n",
      "c1c0778801ed: Pulling fs layer\n",
      "d2ac586dac26: Pulling fs layer\n",
      "c82a34ea3155: Pulling fs layer\n",
      "07391a0f51be: Pulling fs layer\n",
      "31514946c388: Waiting\n",
      "0a8adb3dcf22: Waiting\n",
      "6e88a7f6f15d: Waiting\n",
      "8b6afde98e21: Waiting\n",
      "00f5ed83fb5b: Waiting\n",
      "67e193f6923c: Waiting\n",
      "4045938dbcf0: Waiting\n",
      "ec3ab3614609: Waiting\n",
      "d59ff5de4fe3: Waiting\n",
      "c11b1ed1ee46: Waiting\n",
      "753e545517ec: Waiting\n",
      "59fa5f6703c7: Waiting\n",
      "5251fc0e145f: Waiting\n",
      "8b4cfd996372: Waiting\n",
      "bdd807f81520: Waiting\n",
      "b64eb74ea0f6: Waiting\n",
      "fde2f9173aa1: Waiting\n",
      "05c48a632d89: Waiting\n",
      "6acab9f9b6c9: Waiting\n",
      "45643bc35dd6: Waiting\n",
      "f3a3e4335302: Waiting\n",
      "7e85ce497f3c: Pulling fs layer\n",
      "c5e6195fcb5a: Waiting\n",
      "c1c0778801ed: Waiting\n",
      "732c433b96b6: Pulling fs layer\n",
      "de409ef6b855: Pulling fs layer\n",
      "36a7c7813429: Pulling fs layer\n",
      "cc688c082c64: Pulling fs layer\n",
      "e518f4c1fb4d: Waiting\n",
      "7ef98a0bb031: Waiting\n",
      "92681808abed: Pulling fs layer\n",
      "8b7d3b755746: Pulling fs layer\n",
      "07973a04f293: Pulling fs layer\n",
      "2b8ce232b0f5: Waiting\n",
      "228e876a170d: Waiting\n",
      "af00054fc370: Waiting\n",
      "450e691b8934: Waiting\n",
      "07391a0f51be: Waiting\n",
      "6cee73498bf6: Waiting\n",
      "0b6fc99b1680: Waiting\n",
      "92681808abed: Waiting\n",
      "7b2bf50cb482: Waiting\n",
      "cda9dd232241: Waiting\n",
      "07973a04f293: Waiting\n",
      "84b159fc1d60: Pulling fs layer\n",
      "161c50cb1ab8: Pulling fs layer\n",
      "84b159fc1d60: Waiting\n",
      "3adfd786ac79: Pulling fs layer\n",
      "b687b9abe298: Pulling fs layer\n",
      "4d55d43d5c97: Pulling fs layer\n",
      "3adfd786ac79: Waiting\n",
      "2bda520db73d: Pulling fs layer\n",
      "4d55d43d5c97: Waiting\n",
      "b687b9abe298: Waiting\n",
      "f006e76f83a3: Pulling fs layer\n",
      "7b8995abb34f: Pulling fs layer\n",
      "7e85ce497f3c: Waiting\n",
      "8b7d3b755746: Waiting\n",
      "c131c32a822c: Waiting\n",
      "7dd53e86e25e: Waiting\n",
      "70651e98f007: Pulling fs layer\n",
      "7e50d9a71c79: Pulling fs layer\n",
      "161c50cb1ab8: Waiting\n",
      "70651e98f007: Waiting\n",
      "fe9e4cebdbb5: Pulling fs layer\n",
      "0147547644c1: Pulling fs layer\n",
      "daffe4a11a5d: Pulling fs layer\n",
      "daffe4a11a5d: Waiting\n",
      "fe9e4cebdbb5: Waiting\n",
      "0147547644c1: Waiting\n",
      "eaead16dc43b: Verifying Checksum\n",
      "eaead16dc43b: Download complete\n",
      "54de03fa67ca: Verifying Checksum\n",
      "54de03fa67ca: Download complete\n",
      "eaead16dc43b: Pull complete\n",
      "d86e6ecee9ab: Verifying Checksum\n",
      "d86e6ecee9ab: Download complete\n",
      "9e8473502f86: Download complete\n",
      "62cbcf4cbe13: Verifying Checksum\n",
      "62cbcf4cbe13: Download complete\n",
      "6b08e0981273: Verifying Checksum\n",
      "6b08e0981273: Download complete\n",
      "9054d495b912: Verifying Checksum\n",
      "9054d495b912: Download complete\n",
      "363a6cd12433: Verifying Checksum\n",
      "363a6cd12433: Download complete\n",
      "69633d945bde: Verifying Checksum\n",
      "69633d945bde: Download complete\n",
      "282a25844090: Verifying Checksum\n",
      "282a25844090: Download complete\n",
      "030b5d54675a: Verifying Checksum\n",
      "030b5d54675a: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "d86e6ecee9ab: Pull complete\n",
      "77bc21e94f24: Verifying Checksum\n",
      "77bc21e94f24: Download complete\n",
      "784e1b6fd612: Verifying Checksum\n",
      "784e1b6fd612: Download complete\n",
      "af1ee39433f4: Verifying Checksum\n",
      "af1ee39433f4: Download complete\n",
      "6b08e0981273: Pull complete\n",
      "54de03fa67ca: Pull complete\n",
      "51caa25c21ff: Verifying Checksum\n",
      "51caa25c21ff: Download complete\n",
      "af00054fc370: Verifying Checksum\n",
      "af00054fc370: Download complete\n",
      "0b6fc99b1680: Verifying Checksum\n",
      "0b6fc99b1680: Download complete\n",
      "6cee73498bf6: Verifying Checksum\n",
      "6cee73498bf6: Download complete\n",
      "52321cba559e: Verifying Checksum\n",
      "52321cba559e: Download complete\n",
      "b090c9ed50ff: Verifying Checksum\n",
      "b090c9ed50ff: Download complete\n",
      "0a8adb3dcf22: Verifying Checksum\n",
      "0a8adb3dcf22: Download complete\n",
      "6e88a7f6f15d: Download complete\n",
      "00f5ed83fb5b: Verifying Checksum\n",
      "00f5ed83fb5b: Download complete\n",
      "e518f4c1fb4d: Verifying Checksum\n",
      "e518f4c1fb4d: Download complete\n",
      "8b6afde98e21: Download complete\n",
      "dcdde75d70d4: Verifying Checksum\n",
      "dcdde75d70d4: Download complete\n",
      "c131c32a822c: Download complete\n",
      "7ef98a0bb031: Verifying Checksum\n",
      "7ef98a0bb031: Download complete\n",
      "7b2bf50cb482: Verifying Checksum\n",
      "7b2bf50cb482: Download complete\n",
      "7dd53e86e25e: Verifying Checksum\n",
      "7dd53e86e25e: Download complete\n",
      "228e876a170d: Verifying Checksum\n",
      "228e876a170d: Download complete\n",
      "d0067f8e4744: Verifying Checksum\n",
      "d0067f8e4744: Download complete\n",
      "2b8ce232b0f5: Verifying Checksum\n",
      "2b8ce232b0f5: Download complete\n",
      "6acab9f9b6c9: Verifying Checksum\n",
      "6acab9f9b6c9: Download complete\n",
      "49738298fc0b: Verifying Checksum\n",
      "49738298fc0b: Download complete\n",
      "450e691b8934: Verifying Checksum\n",
      "450e691b8934: Download complete\n",
      "c11b1ed1ee46: Verifying Checksum\n",
      "c11b1ed1ee46: Download complete\n",
      "f3a3e4335302: Verifying Checksum\n",
      "f3a3e4335302: Download complete\n",
      "31514946c388: Verifying Checksum\n",
      "31514946c388: Download complete\n",
      "4045938dbcf0: Verifying Checksum\n",
      "4045938dbcf0: Download complete\n",
      "ec3ab3614609: Verifying Checksum\n",
      "ec3ab3614609: Download complete\n",
      "d59ff5de4fe3: Download complete\n",
      "67e193f6923c: Verifying Checksum\n",
      "67e193f6923c: Download complete\n",
      "c5e6195fcb5a: Verifying Checksum\n",
      "c5e6195fcb5a: Download complete\n",
      "b64eb74ea0f6: Verifying Checksum\n",
      "b64eb74ea0f6: Download complete\n",
      "fde2f9173aa1: Verifying Checksum\n",
      "fde2f9173aa1: Download complete\n",
      "05c48a632d89: Verifying Checksum\n",
      "05c48a632d89: Download complete\n",
      "dac08d81643b: Verifying Checksum\n",
      "dac08d81643b: Download complete\n",
      "45643bc35dd6: Verifying Checksum\n",
      "45643bc35dd6: Download complete\n",
      "59fa5f6703c7: Verifying Checksum\n",
      "59fa5f6703c7: Download complete\n",
      "753e545517ec: Verifying Checksum\n",
      "753e545517ec: Download complete\n",
      "5251fc0e145f: Download complete\n",
      "bdd807f81520: Download complete\n",
      "c1c0778801ed: Download complete\n",
      "cda9dd232241: Verifying Checksum\n",
      "cda9dd232241: Download complete\n",
      "c82a34ea3155: Verifying Checksum\n",
      "c82a34ea3155: Download complete\n",
      "d2ac586dac26: Verifying Checksum\n",
      "d2ac586dac26: Download complete\n",
      "8b4cfd996372: Verifying Checksum\n",
      "8b4cfd996372: Download complete\n",
      "7e85ce497f3c: Download complete\n",
      "732c433b96b6: Verifying Checksum\n",
      "732c433b96b6: Download complete\n",
      "de409ef6b855: Verifying Checksum\n",
      "de409ef6b855: Download complete\n",
      "36a7c7813429: Verifying Checksum\n",
      "36a7c7813429: Download complete\n",
      "07391a0f51be: Verifying Checksum\n",
      "07391a0f51be: Download complete\n",
      "cc688c082c64: Verifying Checksum\n",
      "cc688c082c64: Download complete\n",
      "92681808abed: Verifying Checksum\n",
      "92681808abed: Download complete\n",
      "84b159fc1d60: Verifying Checksum\n",
      "84b159fc1d60: Download complete\n",
      "161c50cb1ab8: Verifying Checksum\n",
      "161c50cb1ab8: Download complete\n",
      "3adfd786ac79: Download complete\n",
      "b687b9abe298: Verifying Checksum\n",
      "b687b9abe298: Download complete\n",
      "8b7d3b755746: Verifying Checksum\n",
      "8b7d3b755746: Download complete\n",
      "4d55d43d5c97: Verifying Checksum\n",
      "4d55d43d5c97: Download complete\n",
      "f006e76f83a3: Verifying Checksum\n",
      "f006e76f83a3: Download complete\n",
      "7b8995abb34f: Verifying Checksum\n",
      "7b8995abb34f: Download complete\n",
      "2bda520db73d: Verifying Checksum\n",
      "2bda520db73d: Download complete\n",
      "70651e98f007: Verifying Checksum\n",
      "70651e98f007: Download complete\n",
      "7e50d9a71c79: Verifying Checksum\n",
      "7e50d9a71c79: Download complete\n",
      "fe9e4cebdbb5: Verifying Checksum\n",
      "fe9e4cebdbb5: Download complete\n",
      "07973a04f293: Verifying Checksum\n",
      "07973a04f293: Download complete\n",
      "daffe4a11a5d: Verifying Checksum\n",
      "daffe4a11a5d: Download complete\n",
      "d0067f8e4744: Pull complete\n",
      "9e8473502f86: Pull complete\n",
      "62cbcf4cbe13: Pull complete\n",
      "9054d495b912: Pull complete\n",
      "363a6cd12433: Pull complete\n",
      "69633d945bde: Pull complete\n",
      "030b5d54675a: Pull complete\n",
      "282a25844090: Pull complete\n",
      "51caa25c21ff: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "0147547644c1: Download complete\n",
      "77bc21e94f24: Pull complete\n",
      "784e1b6fd612: Pull complete\n",
      "af1ee39433f4: Pull complete\n",
      "52321cba559e: Pull complete\n",
      "af00054fc370: Pull complete\n",
      "0b6fc99b1680: Pull complete\n",
      "6cee73498bf6: Pull complete\n",
      "8b4cfd996372: Pull complete\n",
      "b090c9ed50ff: Pull complete\n",
      "0a8adb3dcf22: Pull complete\n",
      "6e88a7f6f15d: Pull complete\n",
      "00f5ed83fb5b: Pull complete\n",
      "e518f4c1fb4d: Pull complete\n",
      "8b6afde98e21: Pull complete\n",
      "dcdde75d70d4: Pull complete\n",
      "c131c32a822c: Pull complete\n",
      "7ef98a0bb031: Pull complete\n",
      "7b2bf50cb482: Pull complete\n",
      "7dd53e86e25e: Pull complete\n",
      "228e876a170d: Pull complete\n",
      "2b8ce232b0f5: Pull complete\n",
      "6acab9f9b6c9: Pull complete\n",
      "49738298fc0b: Pull complete\n",
      "450e691b8934: Pull complete\n",
      "c11b1ed1ee46: Pull complete\n",
      "67e193f6923c: Pull complete\n",
      "f3a3e4335302: Pull complete\n",
      "31514946c388: Pull complete\n",
      "4045938dbcf0: Pull complete\n",
      "ec3ab3614609: Pull complete\n",
      "d59ff5de4fe3: Pull complete\n",
      "c5e6195fcb5a: Pull complete\n",
      "b64eb74ea0f6: Pull complete\n",
      "fde2f9173aa1: Pull complete\n",
      "05c48a632d89: Pull complete\n",
      "45643bc35dd6: Pull complete\n",
      "dac08d81643b: Pull complete\n",
      "753e545517ec: Pull complete\n",
      "59fa5f6703c7: Pull complete\n",
      "5251fc0e145f: Pull complete\n",
      "bdd807f81520: Pull complete\n",
      "cda9dd232241: Pull complete\n",
      "c1c0778801ed: Pull complete\n",
      "d2ac586dac26: Pull complete\n",
      "c82a34ea3155: Pull complete\n",
      "07391a0f51be: Pull complete\n",
      "7e85ce497f3c: Pull complete\n",
      "732c433b96b6: Pull complete\n",
      "de409ef6b855: Pull complete\n",
      "36a7c7813429: Pull complete\n",
      "cc688c082c64: Pull complete\n",
      "92681808abed: Pull complete\n",
      "8b7d3b755746: Pull complete\n",
      "07973a04f293: Pull complete\n",
      "84b159fc1d60: Pull complete\n",
      "161c50cb1ab8: Pull complete\n",
      "3adfd786ac79: Pull complete\n",
      "b687b9abe298: Pull complete\n",
      "4d55d43d5c97: Pull complete\n",
      "2bda520db73d: Pull complete\n",
      "f006e76f83a3: Pull complete\n",
      "7b8995abb34f: Pull complete\n",
      "70651e98f007: Pull complete\n",
      "7e50d9a71c79: Pull complete\n",
      "fe9e4cebdbb5: Pull complete\n",
      "0147547644c1: Pull complete\n",
      "daffe4a11a5d: Pull complete\n",
      "Digest: sha256:f1714efb94467b1b8f2dfadb27f4b6703568b616ceba85dec4df36dc581c339e\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-tensorflow:22.12\n",
      " ---> 58c045557af1\n",
      "Step 2/5 : WORKDIR /src\n",
      " ---> Running in 1ce899b65bc2\n",
      "Removing intermediate container 1ce899b65bc2\n",
      " ---> b399864598b7\n",
      "Step 3/5 : COPY trainer/* trainer/\n",
      " ---> 6ded7efe6fec\n",
      "Step 4/5 : RUN pip install -r trainer/requirements.txt\n",
      " ---> Running in 71b3c572ba4d\n",
      "Requirement already satisfied: merlin-models in /usr/local/lib/python3.8/dist-packages (from -r trainer/requirements.txt (line 1)) (0.11.0)\n",
      "Collecting gcsfs\n",
      "  Downloading gcsfs-2023.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-cloud-aiplatform>=1.21.0\n",
      "  Downloading google_cloud_aiplatform-1.23.0-py2.py3-none-any.whl (2.5 MB)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
      "Collecting tensorboard-plugin-profile==2.11.1\n",
      "  Downloading tensorboard_plugin_profile-2.11.1-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: merlin-dataloader>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from merlin-models->-r trainer/requirements.txt (line 1)) (0.0.4)\n",
      "Requirement already satisfied: merlin-core>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-models->-r trainer/requirements.txt (line 1)) (0.10.0)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs->-r trainer/requirements.txt (line 2)) (5.1.1)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.7.0-py2.py3-none-any.whl (110 kB)\n",
      "Collecting fsspec==2023.3.0\n",
      "  Downloading fsspec-2023.3.0-py3-none-any.whl (145 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gcsfs->-r trainer/requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs->-r trainer/requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from gcsfs->-r trainer/requirements.txt (line 2)) (3.8.3)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs->-r trainer/requirements.txt (line 2)) (2.15.0)\n",
      "Collecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform>=1.21.0->-r trainer/requirements.txt (line 3)) (3.19.6)\n",
      "Collecting shapely<2.0.0\n",
      "  Downloading Shapely-1.8.5.post1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl (276 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "  Downloading proto_plus-1.22.2-py3-none-any.whl (47 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "  Downloading google_cloud_bigquery-3.7.0-py2.py3-none-any.whl (215 kB)\n",
      "Collecting starlette<0.27.0,>=0.26.1\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
      "  Downloading pydantic-1.10.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard-plugin-profile==2.11.1->-r trainer/requirements.txt (line 5)) (45.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorboard-plugin-profile==2.11.1->-r trainer/requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard-plugin-profile==2.11.1->-r trainer/requirements.txt (line 5)) (2.2.2)\n",
      "Collecting gviz-api>=1.9.0\n",
      "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: dask>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: betterproto<2.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.2.5)\n",
      "Requirement already satisfied: distributed>=2022.3.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (4.64.1)\n",
      "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (0.56.4)\n",
      "Requirement already satisfied: tensorflow-metadata>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: pandas<1.4.0dev0,>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (8.0.0)\n",
      "Collecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.4.1-py2.py3-none-any.whl (77 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r trainer/requirements.txt (line 2)) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gcsfs->-r trainer/requirements.txt (line 2)) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->gcsfs->-r trainer/requirements.txt (line 2)) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->gcsfs->-r trainer/requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs->-r trainer/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->-r trainer/requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs->-r trainer/requirements.txt (line 2)) (5.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs->-r trainer/requirements.txt (line 2)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs->-r trainer/requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform>=1.21.0->-r trainer/requirements.txt (line 3)) (3.0.9)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "  Downloading grpc_google_iam_v1-0.12.6-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r trainer/requirements.txt (line 3)) (1.57.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2; extra == \"grpc\"\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2; extra == \"grpc\" in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform>=1.21.0->-r trainer/requirements.txt (line 3)) (1.42.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform>=1.21.0->-r trainer/requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi->-r trainer/requirements.txt (line 4)) (3.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi->-r trainer/requirements.txt (line 4)) (4.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=0.11.15->tensorboard-plugin-profile==2.11.1->-r trainer/requirements.txt (line 5)) (2.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: stringcase in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: grpclib in /usr/local/lib/python3.8/dist-packages (from betterproto<2.0.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (5.9.4)\n",
      "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: tornado<6.2,>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (6.1)\n",
      "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (0.39.1)\n",
      "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.9\" in /usr/local/lib/python3.8/dist-packages (from numba>=0.54->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (5.2.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow-metadata>=1.2.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas<1.4.0dev0,>=1.2.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (2022.7)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->-r trainer/requirements.txt (line 2)) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->-r trainer/requirements.txt (line 2)) (0.4.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi->-r trainer/requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (4.1.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=2022.3.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata; python_version < \"3.9\"->numba>=0.54->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (3.11.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.8/dist-packages (from h2<5,>=3.1.0->grpclib->betterproto<2.0.0->merlin-core>=0.2.0->merlin-models->-r trainer/requirements.txt (line 1)) (4.0.0)\n",
      "\u001b[91mERROR: dask-cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty requires cupy-cuda118<12,>=9.5.0, which is not installed.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement flatbuffers>=2.0, but you'll have flatbuffers 1.12 which is incompatible.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement tensorboard<2.11,>=2.10, but you'll have tensorboard 2.9.1 which is incompatible.\n",
      "ERROR: tensorflow 2.10.0+nv22.11 has requirement tensorflow-estimator<2.11,>=2.10.0, but you'll have tensorflow-estimator 2.9.0 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: merlin-core 0.10.0 has requirement fsspec==2022.5.0, but you'll have fsspec 2023.3.0 which is incompatible.\n",
      "ERROR: jupyter-server 2.0.6 has requirement tornado>=6.2.0, but you'll have tornado 6.1 which is incompatible.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement cuda-python<11.7.1,>=11.5, but you'll have cuda-python 11.8.1 which is incompatible.\n",
      "ERROR: cudf 22.8.0a0+304.g6ca81bbc78.dirty has requirement protobuf<3.21.0a0,>=3.20.1, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "ERROR: grpcio-status 1.51.3 has requirement grpcio>=1.51.3, but you'll have grpcio 1.42.0 which is incompatible.\n",
      "ERROR: grpcio-status 1.51.3 has requirement protobuf>=4.21.6, but you'll have protobuf 3.19.6 which is incompatible.\n",
      "\u001b[0m\u001b[91mERROR: grpc-google-iam-v1 0.12.6 has requirement grpcio<2.0.0dev,>=1.44.0, but you'll have grpcio 1.42.0 which is incompatible.\n",
      "ERROR: google-cloud-bigquery 3.7.0 has requirement grpcio<2.0dev,>=1.47.0, but you'll have grpcio 1.42.0 which is incompatible.\n",
      "\u001b[0mInstalling collected packages: google-crc32c, google-resumable-media, grpcio-status, google-api-core, google-cloud-core, google-cloud-storage, fsspec, gcsfs, packaging, shapely, grpc-google-iam-v1, proto-plus, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform, starlette, pydantic, fastapi, gviz-api, tensorboard-plugin-profile\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.5.0\n",
      "    Uninstalling fsspec-2022.5.0:\n",
      "      Successfully uninstalled fsspec-2022.5.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 22.0\n",
      "    Uninstalling packaging-22.0:\n",
      "      Successfully uninstalled packaging-22.0\n",
      "Successfully installed fastapi-0.95.0 fsspec-2023.3.0 gcsfs-2023.3.0 google-api-core-2.11.0 google-cloud-aiplatform-1.23.0 google-cloud-bigquery-3.7.0 google-cloud-core-2.3.2 google-cloud-resource-manager-1.9.0 google-cloud-storage-2.7.0 google-crc32c-1.5.0 google-resumable-media-2.4.1 grpc-google-iam-v1-0.12.6 grpcio-status-1.51.3 gviz-api-1.10.0 packaging-21.3 proto-plus-1.22.2 pydantic-1.10.6 shapely-1.8.5.post1 starlette-0.26.1 tensorboard-plugin-profile-2.11.1\n",
      "Removing intermediate container 71b3c572ba4d\n",
      " ---> c0c8f8f1d6eb\n",
      "Step 5/5 : RUN apt update && apt -y install nvtop\n",
      " ---> Running in d455c836dcfc\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mGet:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal InRelease [265 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [923 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Get:6 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1998 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/restricted amd64 Packages [33.4 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 Packages [177 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 Packages [11.3 MB]\n",
      "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1017 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2544 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [28.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 Packages [1275 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1313 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2141 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3024 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [28.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [55.2 kB]\n",
      "Fetched 26.5 MB in 3s (8359 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "95 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-530\n",
      "The following NEW packages will be installed:\n",
      "  libnvidia-compute-418 libnvidia-compute-430 libnvidia-compute-530 nvtop\n",
      "0 upgraded, 4 newly installed, 0 to remove and 95 not upgraded.\n",
      "Need to get 50.7 MB of archives.\n",
      "After this operation, 231 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-530 530.30.02-0ubuntu1 [50.6 MB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/restricted amd64 libnvidia-compute-418 amd64 430.50-0ubuntu3 [6936 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/multiverse amd64 nvtop amd64 1.0.0-1ubuntu2 [26.8 kB]\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvidia-compute-430 530.30.02-0ubuntu1 [6900 B]\n",
      "Fetched 50.7 MB in 1s (50.8 MB/s)\n",
      "Selecting previously unselected package libnvidia-compute-530:amd64.\n",
      "(Reading database ... 41590 files and directories currently installed.)\n",
      "Preparing to unpack .../libnvidia-compute-530_530.30.02-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-530:amd64 (530.30.02-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-430:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-430_530.30.02-0ubuntu1_amd64.deb ...\n",
      "Unpacking libnvidia-compute-430:amd64 (530.30.02-0ubuntu1) ...\n",
      "Selecting previously unselected package libnvidia-compute-418:amd64.\n",
      "Preparing to unpack .../libnvidia-compute-418_430.50-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Selecting previously unselected package nvtop.\n",
      "Preparing to unpack .../nvtop_1.0.0-1ubuntu2_amd64.deb ...\n",
      "Unpacking nvtop (1.0.0-1ubuntu2) ...\n",
      "Setting up libnvidia-compute-530:amd64 (530.30.02-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-430:amd64 (530.30.02-0ubuntu1) ...\n",
      "Setting up libnvidia-compute-418:amd64 (430.50-0ubuntu3) ...\n",
      "Setting up nvtop (1.0.0-1ubuntu2) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "/sbin/ldconfig.real: /lib/libarrow_python.so.800 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /lib/libarrow_dataset.so.800 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /lib/libarrow.so.800 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /lib/libparquet.so.800 is not a symbolic link\n",
      "Removing intermediate container d455c836dcfc\n",
      " ---> 2e80234f5d0b\n",
      "Successfully built 2e80234f5d0b\n",
      "Successfully tagged gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34:latest\n",
      "PUSH\n",
      "Pushing gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34\n",
      "The push refers to repository [gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34]\n",
      "c4cd2be54ab1: Preparing\n",
      "bd119885b9fa: Preparing\n",
      "4dca9ba7bbfb: Preparing\n",
      "02ea5dc665f2: Preparing\n",
      "2f4c2e747ad7: Preparing\n",
      "f5b846caf956: Preparing\n",
      "12eec4f4a731: Preparing\n",
      "67516f8ac904: Preparing\n",
      "4c0655970ccc: Preparing\n",
      "73101e82e910: Preparing\n",
      "f4d87c23e98f: Preparing\n",
      "d07270ded861: Preparing\n",
      "8187da794091: Preparing\n",
      "d7ba493f5d44: Preparing\n",
      "ebd7caa9de82: Preparing\n",
      "242c03ad2a31: Preparing\n",
      "df5d5e1b9e48: Preparing\n",
      "3951f4addbfc: Preparing\n",
      "2e5281e7ae87: Preparing\n",
      "42ed3d5ddcb4: Preparing\n",
      "383fa992b719: Preparing\n",
      "e47df10e020a: Preparing\n",
      "317e7f70fff9: Preparing\n",
      "12eec4f4a731: Waiting\n",
      "b45c2562fda1: Preparing\n",
      "67516f8ac904: Waiting\n",
      "0511decbb6f6: Preparing\n",
      "ac6f1cea2b76: Preparing\n",
      "d5f7fcaf77a9: Preparing\n",
      "4c0655970ccc: Waiting\n",
      "73101e82e910: Waiting\n",
      "bf10e31ca3c7: Preparing\n",
      "f1b1f58ddb35: Preparing\n",
      "f4d87c23e98f: Waiting\n",
      "71fe61786022: Preparing\n",
      "f5b846caf956: Waiting\n",
      "20d51f7b3d75: Preparing\n",
      "4357e5c17b0b: Preparing\n",
      "d07270ded861: Waiting\n",
      "38dd15f32335: Preparing\n",
      "1014fb54d10a: Preparing\n",
      "e47df10e020a: Waiting\n",
      "f4c090be1950: Preparing\n",
      "8187da794091: Waiting\n",
      "bfc784fa095a: Preparing\n",
      "d7ba493f5d44: Waiting\n",
      "1f09cab8959e: Preparing\n",
      "ea787d764727: Preparing\n",
      "317e7f70fff9: Waiting\n",
      "04b7d4d05916: Preparing\n",
      "3ace34553782: Preparing\n",
      "b45c2562fda1: Waiting\n",
      "182ee86689d1: Preparing\n",
      "2e5281e7ae87: Waiting\n",
      "ebd7caa9de82: Waiting\n",
      "42ed3d5ddcb4: Waiting\n",
      "0511decbb6f6: Waiting\n",
      "383fa992b719: Waiting\n",
      "242c03ad2a31: Waiting\n",
      "3951f4addbfc: Waiting\n",
      "df5d5e1b9e48: Waiting\n",
      "bbdb6055545b: Preparing\n",
      "9a161f9c0e0a: Preparing\n",
      "ac6f1cea2b76: Waiting\n",
      "f4c090be1950: Waiting\n",
      "b7ad75a3e25e: Preparing\n",
      "1f09cab8959e: Waiting\n",
      "bfc784fa095a: Waiting\n",
      "1014fb54d10a: Waiting\n",
      "7752ee8ae666: Preparing\n",
      "d5f7fcaf77a9: Waiting\n",
      "ea787d764727: Waiting\n",
      "13f264ca1ed9: Preparing\n",
      "4357e5c17b0b: Waiting\n",
      "20d51f7b3d75: Waiting\n",
      "cd870b4929c1: Preparing\n",
      "bf10e31ca3c7: Waiting\n",
      "d092d49243b5: Preparing\n",
      "04b7d4d05916: Waiting\n",
      "38dd15f32335: Waiting\n",
      "aafb34ceba88: Preparing\n",
      "f1b1f58ddb35: Waiting\n",
      "32710f0c03c7: Preparing\n",
      "3ace34553782: Waiting\n",
      "71fe61786022: Waiting\n",
      "bbdb6055545b: Waiting\n",
      "6a55125f046a: Preparing\n",
      "13f264ca1ed9: Waiting\n",
      "2af1bd207a0a: Preparing\n",
      "9a161f9c0e0a: Waiting\n",
      "cd870b4929c1: Waiting\n",
      "ef25bc892765: Preparing\n",
      "b7ad75a3e25e: Waiting\n",
      "7752ee8ae666: Waiting\n",
      "2af1bd207a0a: Waiting\n",
      "e5171eae2464: Preparing\n",
      "6a55125f046a: Waiting\n",
      "d092d49243b5: Waiting\n",
      "f7bcdf2e75c1: Preparing\n",
      "ef25bc892765: Waiting\n",
      "aafb34ceba88: Waiting\n",
      "ef257ce50d4c: Preparing\n",
      "d7d019fd6cda: Preparing\n",
      "bf08ecf416c0: Preparing\n",
      "f7bcdf2e75c1: Waiting\n",
      "ef257ce50d4c: Waiting\n",
      "19be279686b3: Preparing\n",
      "d7d019fd6cda: Waiting\n",
      "bf08ecf416c0: Waiting\n",
      "a42b9d3ddcc4: Preparing\n",
      "19be279686b3: Waiting\n",
      "8a1a337e69bc: Preparing\n",
      "ca6dd165751d: Preparing\n",
      "ac02b4761a81: Preparing\n",
      "8a1a337e69bc: Waiting\n",
      "4be672edea27: Preparing\n",
      "ca6dd165751d: Waiting\n",
      "3041a215c94e: Preparing\n",
      "ac02b4761a81: Waiting\n",
      "db85811a91ed: Preparing\n",
      "4be672edea27: Waiting\n",
      "6eb00409a05c: Preparing\n",
      "fb3ac13a8afd: Preparing\n",
      "3041a215c94e: Waiting\n",
      "e614e8f11c1c: Preparing\n",
      "70d54afe53fd: Preparing\n",
      "cbbf130153a1: Preparing\n",
      "fb3ac13a8afd: Waiting\n",
      "5f70bf18a086: Preparing\n",
      "7173bbe6139c: Preparing\n",
      "23076d2c79f0: Preparing\n",
      "70d54afe53fd: Waiting\n",
      "cbbf130153a1: Waiting\n",
      "ed1a31ee03b7: Preparing\n",
      "fe17ea05bcc3: Preparing\n",
      "5f70bf18a086: Waiting\n",
      "3204b933fb0b: Preparing\n",
      "4616162d4e6a: Preparing\n",
      "9a03c5ba8f95: Preparing\n",
      "7173bbe6139c: Waiting\n",
      "764cbe4d1ae6: Preparing\n",
      "03a82e76641b: Preparing\n",
      "f874925be13d: Preparing\n",
      "23076d2c79f0: Waiting\n",
      "fe17ea05bcc3: Waiting\n",
      "2c4e7d9e38c0: Preparing\n",
      "67abb95254ee: Preparing\n",
      "f4462d5b2da2: Preparing\n",
      "03a82e76641b: Waiting\n",
      "764cbe4d1ae6: Waiting\n",
      "3204b933fb0b: Waiting\n",
      "2c4e7d9e38c0: Waiting\n",
      "4616162d4e6a: Waiting\n",
      "67abb95254ee: Waiting\n",
      "f874925be13d: Waiting\n",
      "9a03c5ba8f95: Waiting\n",
      "2f4c2e747ad7: Layer already exists\n",
      "f5b846caf956: Layer already exists\n",
      "12eec4f4a731: Layer already exists\n",
      "67516f8ac904: Layer already exists\n",
      "4c0655970ccc: Layer already exists\n",
      "73101e82e910: Layer already exists\n",
      "f4d87c23e98f: Layer already exists\n",
      "02ea5dc665f2: Pushed\n",
      "4dca9ba7bbfb: Pushed\n",
      "d07270ded861: Layer already exists\n",
      "8187da794091: Layer already exists\n",
      "d7ba493f5d44: Layer already exists\n",
      "ebd7caa9de82: Layer already exists\n",
      "242c03ad2a31: Layer already exists\n",
      "df5d5e1b9e48: Layer already exists\n",
      "2e5281e7ae87: Layer already exists\n",
      "3951f4addbfc: Layer already exists\n",
      "42ed3d5ddcb4: Layer already exists\n",
      "383fa992b719: Layer already exists\n",
      "e47df10e020a: Layer already exists\n",
      "317e7f70fff9: Layer already exists\n",
      "0511decbb6f6: Layer already exists\n",
      "b45c2562fda1: Layer already exists\n",
      "ac6f1cea2b76: Layer already exists\n",
      "d5f7fcaf77a9: Layer already exists\n",
      "bf10e31ca3c7: Layer already exists\n",
      "71fe61786022: Layer already exists\n",
      "f1b1f58ddb35: Layer already exists\n",
      "20d51f7b3d75: Layer already exists\n",
      "4357e5c17b0b: Layer already exists\n",
      "38dd15f32335: Layer already exists\n",
      "1014fb54d10a: Layer already exists\n",
      "bfc784fa095a: Layer already exists\n",
      "f4c090be1950: Layer already exists\n",
      "1f09cab8959e: Layer already exists\n",
      "ea787d764727: Layer already exists\n",
      "04b7d4d05916: Layer already exists\n",
      "3ace34553782: Layer already exists\n",
      "182ee86689d1: Layer already exists\n",
      "bbdb6055545b: Layer already exists\n",
      "9a161f9c0e0a: Layer already exists\n",
      "b7ad75a3e25e: Layer already exists\n",
      "13f264ca1ed9: Layer already exists\n",
      "7752ee8ae666: Layer already exists\n",
      "cd870b4929c1: Layer already exists\n",
      "d092d49243b5: Layer already exists\n",
      "aafb34ceba88: Layer already exists\n",
      "32710f0c03c7: Layer already exists\n",
      "6a55125f046a: Layer already exists\n",
      "2af1bd207a0a: Layer already exists\n",
      "ef25bc892765: Layer already exists\n",
      "e5171eae2464: Layer already exists\n",
      "f7bcdf2e75c1: Layer already exists\n",
      "ef257ce50d4c: Layer already exists\n",
      "19be279686b3: Layer already exists\n",
      "d7d019fd6cda: Layer already exists\n",
      "bf08ecf416c0: Layer already exists\n",
      "8a1a337e69bc: Layer already exists\n",
      "a42b9d3ddcc4: Layer already exists\n",
      "ca6dd165751d: Layer already exists\n",
      "4be672edea27: Layer already exists\n",
      "ac02b4761a81: Layer already exists\n",
      "db85811a91ed: Layer already exists\n",
      "3041a215c94e: Layer already exists\n",
      "6eb00409a05c: Layer already exists\n",
      "e614e8f11c1c: Layer already exists\n",
      "fb3ac13a8afd: Layer already exists\n",
      "70d54afe53fd: Layer already exists\n",
      "cbbf130153a1: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "7173bbe6139c: Layer already exists\n",
      "ed1a31ee03b7: Layer already exists\n",
      "23076d2c79f0: Layer already exists\n",
      "fe17ea05bcc3: Layer already exists\n",
      "4616162d4e6a: Layer already exists\n",
      "3204b933fb0b: Layer already exists\n",
      "9a03c5ba8f95: Layer already exists\n",
      "03a82e76641b: Layer already exists\n",
      "764cbe4d1ae6: Layer already exists\n",
      "f874925be13d: Layer already exists\n",
      "2c4e7d9e38c0: Layer already exists\n",
      "67abb95254ee: Layer already exists\n",
      "f4462d5b2da2: Layer already exists\n",
      "bd119885b9fa: Pushed\n",
      "c4cd2be54ab1: Pushed\n",
      "latest: digest: sha256:3e5fc84a8482267339c531c959ea18481d7df47fe883ab985977a262e3aa606d size: 18203\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                                                      STATUS\n",
      "39d033a4-a70e-4e33-bcbb-62f1c0f93bff  2023-03-21T04:09:11+00:00  5M33S     gs://hybrid-vertex_cloudbuild/source/1679371751.278726-2ff2c36e11104e49aa61599ad2cc964f.tgz  gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "! gcloud builds submit --config src/cloudbuild.yaml \\\n",
    "    --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION \\\n",
    "    --timeout=2h \\\n",
    "    --machine-type=$MACHINE_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1473cd1-af0d-4c63-aff0-d1286f096170",
   "metadata": {},
   "source": [
    "# Vertex Train Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267cdca6-d053-401d-8f95-5f24d3192d7a",
   "metadata": {},
   "source": [
    "### Prepare `worker_pool_specs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c6d7e-959a-4354-85d5-36f7a9a662ed",
   "metadata": {},
   "source": [
    "### Acclerators and Device Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81d7050b-e6a8-4314-b3df-27e7abf0d39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ====================================================\n",
    "# Single | Single machine, single GPU\n",
    "# ====================================================\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "REPLICA_COUNT = 1\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "REDUCTION_SERVER_COUNT = 0                                                      \n",
    "REDUCTION_SERVER_MACHINE_TYPE = \"n1-highcpu-16\"\n",
    "DISTRIBUTE_STRATEGY = 'single'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a158e910-f435-40fe-8150-6529191cab5d",
   "metadata": {},
   "source": [
    "## Train Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0383-d688-4fb9-b252-152a4639572e",
   "metadata": {},
   "source": [
    "### Previously defined Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3b9fcc0-f78a-4c43-9f71-aade96b55d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT: hybrid-vertex\n",
      "VERSION: jtv34\n",
      "IMAGE_URI: gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34\n",
      "MODEL_NAME: 2tower\n",
      "FRAMEWORK: merlin-tf\n",
      "MODEL_DISPLAY_NAME: vertex-merlin-tf-2tower-jtv34\n",
      "WORKSPACE: gs://jt-merlin-scaling/vertex-merlin-tf-2tower-jtv34\n"
     ]
    }
   ],
   "source": [
    "print(f\"PROJECT: {PROJECT_ID}\")\n",
    "print(f\"VERSION: {VERSION}\")\n",
    "print(f\"IMAGE_URI: {IMAGE_URI}\")\n",
    "print(f\"MODEL_NAME: {MODEL_NAME}\")\n",
    "print(f\"FRAMEWORK: {FRAMEWORK}\")\n",
    "print(f\"MODEL_DISPLAY_NAME: {MODEL_DISPLAY_NAME}\")\n",
    "print(f\"WORKSPACE: {WORKSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74892c-4a1d-4faf-9d10-d724af181704",
   "metadata": {},
   "source": [
    "### Vertex Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c21d5946-d679-4802-9e16-e45796defb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT_NAME: mm-vertex-tf-2tower-jtv34\n",
      "RUN_NAME_PREFIX: run-20230321-041554\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "EXPERIMENT_PREFIX = 'mm-vertex'\n",
    "EXPERIMENT_NAME = f'{EXPERIMENT_PREFIX}-tf-{MODEL_NAME}-{VERSION}'\n",
    "RUN_NAME_PREFIX = f'run-{TIMESTAMP}' # timestamp assigned during job\n",
    "\n",
    "print(f\"EXPERIMENT_NAME: {EXPERIMENT_NAME}\")\n",
    "print(f\"RUN_NAME_PREFIX: {RUN_NAME_PREFIX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c242ad-e3df-4ea3-9092-bd6a606fa6d4",
   "metadata": {},
   "source": [
    "### Data dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "487df262-9e3d-4dd9-b7c5-94ca8da2c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train\n",
      "VALID_DATA: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid\n",
      "WORKFLOW_DIR: gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow\n"
     ]
    }
   ],
   "source": [
    "# data and schema from nvtabular pipes\n",
    "# DATA_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed'\n",
    "DATA_DIR = '/gcs/jt-merlin-scaling/nvt-last5-latest-12/nvt-processed'\n",
    "TRAIN_DATA = f'{DATA_DIR}/train'\n",
    "VALID_DATA = f'{DATA_DIR}/valid' \n",
    "\n",
    "# WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-analyzed'\n",
    "# WORKFLOW_DIR = 'gs://spotify-beam-v3/merlin-processed/workflow/2t-spotify-workflow'\n",
    "# WORKFLOW_DIR = 'gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow'\n",
    "WORKFLOW_DIR = '/gcs/jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow'\n",
    "\n",
    "print(f\"TRAIN_DATA: {TRAIN_DATA}\")\n",
    "print(f\"VALID_DATA: {VALID_DATA}\")\n",
    "print(f\"WORKFLOW_DIR: {WORKFLOW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded705c-b752-4459-a8a3-4cab97f94360",
   "metadata": {},
   "source": [
    "### Managed TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a76672e0-ca2f-4c2c-9028-9d388bfa2498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB_RESOURCE_NAME: projects/934903580331/locations/us-central1/tensorboards/1229174835016368128\n",
      "TB display name: mm-vertex-tf-2tower-jtv34-v1\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Managed Tensorboard\n",
    "# ====================================================\n",
    "\n",
    "# use existing TB instance\n",
    "# TB_RESOURCE_NAME = 'projects/934903580331/locations/us-central1/tensorboards/6924469145035603968'\n",
    "\n",
    "# # create new TB instance\n",
    "TENSORBOARD_DISPLAY_NAME=f\"{EXPERIMENT_NAME}-v1\"\n",
    "tensorboard = vertex_ai.Tensorboard.create(display_name=TENSORBOARD_DISPLAY_NAME, project=PROJECT_ID, location=LOCATION)\n",
    "TB_RESOURCE_NAME = tensorboard.resource_name\n",
    "\n",
    "\n",
    "print(f\"TB_RESOURCE_NAME: {TB_RESOURCE_NAME}\")\n",
    "print(f\"TB display name: {tensorboard.display_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b58659-7506-4275-b26d-f37bfd3c9c79",
   "metadata": {},
   "source": [
    "### Worker args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "811a40ec-4829-4013-8820-2ffbfb5de09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'command': ['sh',\n",
      "                                 '-euc',\n",
      "                                 'pip freeze && python -m '\n",
      "                                 'trainer.train_task     '\n",
      "                                 '--per_gpu_batch_size=16384     '\n",
      "                                 '--train_output_bucket=jt-merlin-scaling     '\n",
      "                                 '--train_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/train     '\n",
      "                                 '--valid_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/valid     '\n",
      "                                 '--workflow_dir=gs://jt-merlin-scaling/nvt-last5-latest-12/nvt-processed/workflow     '\n",
      "                                 '--num_epochs=4     --learning_rate=0.001     '\n",
      "                                 '--distribute=single     '\n",
      "                                 '--experiment_name=mm-vertex-tf-2tower-jtv34     '\n",
      "                                 '--experiment_run=run-20230321-041554     '\n",
      "                                 '--project=hybrid-vertex     '\n",
      "                                 '--location=us-central1     '\n",
      "                                 '--valid_frequency=20     '\n",
      "                                 '--epoch_steps=500     --valid_steps=5     '\n",
      "                                 \"--layer_sizes='[512, 256, 128]'     \"\n",
      "                                 '--chkpt_freq=epoch     '\n",
      "                                 '--write_embeddings     --profiler'],\n",
      "                     'image_uri': 'gcr.io/hybrid-vertex/train-2212v16-vertex-merlin-tf-2tower-jtv34'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                   'machine_type': 'a2-highgpu-1g'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "from utils import train_utils\n",
    "\n",
    "# gcs bucket\n",
    "OUTPUT_BUCKET = 'jt-merlin-scaling'\n",
    "\n",
    "# data size\n",
    "train_sample_cnt = 8_205_265 # 8_205_265\n",
    "valid_samples_cnt = 82_959\n",
    "\n",
    "# train config\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 4096*4 \n",
    "LEARNING_RATE = 0.001\n",
    "VALID_FREQUENCY = 20\n",
    "VALID_STEPS = valid_samples_cnt // BATCH_SIZE\n",
    "EPOCH_STEPS = train_sample_cnt // BATCH_SIZE\n",
    "CHECKPOINT_FREQ='epoch'\n",
    "\n",
    "# model\n",
    "LAYERS = \"[512, 256, 128]\"\n",
    "\n",
    "    \n",
    "WORKER_CMD = [\n",
    "    'sh',\n",
    "    '-euc',\n",
    "    f\"\"\"pip freeze && python -m trainer.train_task \\\n",
    "    --per_gpu_batch_size={BATCH_SIZE} \\\n",
    "    --train_output_bucket={OUTPUT_BUCKET} \\\n",
    "    --train_dir={TRAIN_DATA} \\\n",
    "    --valid_dir={VALID_DATA} \\\n",
    "    --workflow_dir={WORKFLOW_DIR} \\\n",
    "    --num_epochs={NUM_EPOCHS} \\\n",
    "    --learning_rate={LEARNING_RATE} \\\n",
    "    --distribute={DISTRIBUTE_STRATEGY} \\\n",
    "    --experiment_name={EXPERIMENT_NAME} \\\n",
    "    --experiment_run={RUN_NAME_PREFIX} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --location={LOCATION} \\\n",
    "    --valid_frequency={VALID_FREQUENCY} \\\n",
    "    --epoch_steps={EPOCH_STEPS} \\\n",
    "    --valid_steps={VALID_STEPS} \\\n",
    "    --layer_sizes=\\'{LAYERS}\\' \\\n",
    "    --chkpt_freq={CHECKPOINT_FREQ} \\\n",
    "    --write_embeddings \\\n",
    "    --profiler\"\"\"\n",
    "    # --write_embeddings\n",
    "    # '''\n",
    "    # --tb_name={TB_RESOURCE_NAME} \\\n",
    "]\n",
    "\n",
    "# ====================================================\n",
    "# Worker pool specs\n",
    "# ====================================================\n",
    "    \n",
    "WORKER_POOL_SPECS = train_utils.prepare_worker_pool_specs(\n",
    "    image_uri=IMAGE_URI,\n",
    "    # args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(WORKER_POOL_SPECS)\n",
    "# jt-merlin-scaling/nvt-last5-latest-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a990b1e-107e-428c-9591-cddd62c00b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKER_POOL_SPECS[0]['container_spec']['command']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659f5b0-22ab-451d-ba49-09cc445d053f",
   "metadata": {},
   "source": [
    "## Submit train job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bddd12da-d6ef-4ffe-917f-408b7bade0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_OUTPUT_DIR : gs://jt-merlin-scaling/mm-vertex-tf-2tower-jtv34/run-20230321-041554\n",
      "JOB_NAME : train-vertex-merlin-tf-2tower-jtv34\n",
      "\n",
      "gpu_type : nvidia_tesla_a100\n",
      "gpu_per_replica : 1\n",
      "replica_cnt : 1\n"
     ]
    }
   ],
   "source": [
    "BASE_OUTPUT_DIR = f'gs://{OUTPUT_BUCKET}/{EXPERIMENT_NAME}/{RUN_NAME_PREFIX}'\n",
    "\n",
    "# initialize vertex sdk\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    # experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "JOB_NAME = f'train-{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# labels for train job\n",
    "gpu_type = ACCELERATOR_TYPE.lower()\n",
    "gpu_per_replica = PER_MACHINE_ACCELERATOR_COUNT\n",
    "replica_cnt = REPLICA_COUNT\n",
    "\n",
    "print(f'BASE_OUTPUT_DIR : {BASE_OUTPUT_DIR}')\n",
    "print(f'JOB_NAME : {JOB_NAME}\\n')\n",
    "print(f'gpu_type : {gpu_type}')\n",
    "print(f'gpu_per_replica : {gpu_per_replica}')\n",
    "print(f'replica_cnt : {replica_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "041217fa-0cf5-477a-b1a8-e5cfb58712bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomJob(\n",
    "    display_name=JOB_NAME,\n",
    "    worker_pool_specs=WORKER_POOL_SPECS,\n",
    "    base_output_dir=BASE_OUTPUT_DIR,\n",
    "    staging_bucket=f'{BASE_OUTPUT_DIR}/staging',\n",
    "    labels={\n",
    "        # 'mm_image' : 'nightly',\n",
    "        'gpu' : f'{gpu_type}',\n",
    "        'gpu_per_replica' : f'{gpu_per_replica}',\n",
    "        'replica_cnt' : f'{replica_cnt}',\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run(\n",
    "    tensorboard=TB_RESOURCE_NAME,\n",
    "    service_account=VERTEX_SA,\n",
    "    restart_job_on_worker_restart=False,\n",
    "    enable_web_access=True,\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709b978-1a64-40e8-b8c4-149a7a40bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FileNotFoundError: [Errno 2] No such file or directory: 'gs:/jt-merlin-scaling/latest-2tower-merlin-tf-jtv16/run-20230223-221213/model-dir/query-tower/.merlin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe04d7a8-2ba9-42ec-95ea-002adedbb4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TB_LOGS_PATH = f'{BASE_OUTPUT_DIR}/logs' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5629edd9-b8ff-4246-bf2c-ce3820b74438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "# %reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20e0b96c-65aa-424a-805d-4fe512865fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3c4fa2277c73f7f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3c4fa2277c73f7f0\");\n",
       "          const url = new URL(\"/proxy/6006/\", window.location);\n",
       "          const port = 0;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=$TB_LOGS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcf9e5-5bbc-43f5-ae2f-209ef9540395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-9.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-9:m100"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
